{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 2. Setup & Load Data\n",
        "\n",
        "## 2.1 Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m‚úÖ All packages installed successfully!\n",
            "‚ö†Ô∏è  If you encounter 'protobuf' errors, please RESTART THE KERNEL and run cells again.\n"
          ]
        }
      ],
      "source": [
        "# Core dependencies (protobuf required for T5 tokenizers)\n",
        "!pip install -q protobuf sentencepiece\n",
        "\n",
        "# Core libraries\n",
        "!pip install -q transformers datasets torch\n",
        "\n",
        "# Evaluation and metrics\n",
        "!pip install -q rouge-score py-rouge evaluate scikit-learn sacrebleu tabulate\n",
        "\n",
        "# Vietnamese NLP and graph algorithms\n",
        "!pip install -q underthesea networkx\n",
        "\n",
        "# Visualization\n",
        "!pip install -q matplotlib seaborn pandas numpy\n",
        "\n",
        "# Progress bars\n",
        "!pip install -q tqdm\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")\n",
        "print(\"‚ö†Ô∏è  If you encounter 'protobuf' errors, please RESTART THE KERNEL and run cells again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Import Libraries and Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SYSTEM INFORMATION\n",
            "============================================================\n",
            "‚úì CUDA is available\n",
            "‚úì GPU: NVIDIA GeForce RTX 3090\n",
            "‚úì Total VRAM: 23.6 GB\n",
            "‚úì PyTorch version: 2.9.1+cu128\n",
            "\n",
            "‚úÖ All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Standard libraries\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# HuggingFace\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModel, \n",
        "    AutoModelForSeq2SeqLM\n",
        ")\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Evaluation\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Graph algorithms for TextRank\n",
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Check CUDA availability\n",
        "print(\"=\"*60)\n",
        "print(\"SYSTEM INFORMATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"‚úì CUDA is available\")\n",
        "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚úì Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"‚ö† CUDA not available, using CPU\")\n",
        "    print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
        "\n",
        "print(\"\\n‚úÖ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Load Data from CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "LOADING VIETNAMESE TEXT SUMMARIZATION DATASET\n",
            "============================================================\n",
            "\n",
            "üìä Dataset loaded successfully!\n",
            "  Train: 15,620 samples\n",
            "  Validation: 1,952 samples\n",
            "  Test: 1,953 samples\n",
            "  Total: 19,525 samples\n",
            "\n",
            "üìã Columns: ['document', 'summary', 'keywords']\n",
            "\n",
            "‚úì After removing NaN: Train=15,620, Val=1,952, Test=1,953\n",
            "\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['document', 'summary'],\n",
            "        num_rows: 15620\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['document', 'summary'],\n",
            "        num_rows: 1952\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['document', 'summary'],\n",
            "        num_rows: 1953\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"LOADING VIETNAMESE TEXT SUMMARIZATION DATASET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load CSV files\n",
        "data_path = \"data\"\n",
        "train_df = pd.read_csv(f\"{data_path}/train.csv\")\n",
        "val_df = pd.read_csv(f\"{data_path}/validation.csv\")\n",
        "test_df = pd.read_csv(f\"{data_path}/test.csv\")\n",
        "\n",
        "print(f\"\\nüìä Dataset loaded successfully!\")\n",
        "print(f\"  Train: {len(train_df):,} samples\")\n",
        "print(f\"  Validation: {len(val_df):,} samples\")\n",
        "print(f\"  Test: {len(test_df):,} samples\")\n",
        "print(f\"  Total: {len(train_df) + len(val_df) + len(test_df):,} samples\")\n",
        "\n",
        "# Check columns\n",
        "print(f\"\\nüìã Columns: {list(train_df.columns)}\")\n",
        "\n",
        "# Keep only document and summary columns\n",
        "train_df = train_df[['document', 'summary']].dropna()\n",
        "val_df = val_df[['document', 'summary']].dropna()\n",
        "test_df = test_df[['document', 'summary']].dropna()\n",
        "\n",
        "print(f\"\\n‚úì After removing NaN: Train={len(train_df):,}, Val={len(val_df):,}, Test={len(test_df):,}\")\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "dataset = DatasetDict({\n",
        "    'train': Dataset.from_pandas(train_df, preserve_index=False),\n",
        "    'validation': Dataset.from_pandas(val_df, preserve_index=False),\n",
        "    'test': Dataset.from_pandas(test_df, preserve_index=False)\n",
        "})\n",
        "\n",
        "print(f\"\\n{dataset}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 TextRank Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ TextRank Summarizer class defined!\n"
          ]
        }
      ],
      "source": [
        "class TextRankSummarizer:\n",
        "    \"\"\"\n",
        "    TextRank algorithm for extractive summarization using PhoBERT embeddings\n",
        "    \n",
        "    Args:\n",
        "        top_n (int): Number of sentences to extract\n",
        "        damping (float): Damping factor for PageRank (default: 0.85)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, top_n=3, damping=0.85):\n",
        "        self.top_n = top_n\n",
        "        self.damping = damping\n",
        "        \n",
        "        print(\"Loading PhoBERT model for Vietnamese sentence embeddings...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base')\n",
        "        self.model = AutoModel.from_pretrained('vinai/phobert-base')\n",
        "        self.device = device\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        print(\"‚úì PhoBERT loaded successfully!\")\n",
        "    \n",
        "    def get_sentence_embedding(self, sentence):\n",
        "        \"\"\"\n",
        "        Get PhoBERT embedding for a sentence\n",
        "        \n",
        "        Args:\n",
        "            sentence (str): Input sentence\n",
        "            \n",
        "        Returns:\n",
        "            np.ndarray: Sentence embedding vector\n",
        "        \"\"\"\n",
        "        inputs = self.tokenizer(\n",
        "            sentence, \n",
        "            return_tensors='pt', \n",
        "            truncation=True, \n",
        "            max_length=256\n",
        "        ).to(self.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            # Use CLS token embedding\n",
        "            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "        \n",
        "        return embedding[0]\n",
        "    \n",
        "    def build_similarity_matrix(self, sentences):\n",
        "        \"\"\"\n",
        "        Build similarity matrix between sentences using cosine similarity\n",
        "        \n",
        "        Args:\n",
        "            sentences (list): List of sentences\n",
        "            \n",
        "        Returns:\n",
        "            np.ndarray: Similarity matrix\n",
        "        \"\"\"\n",
        "        embeddings = []\n",
        "        \n",
        "        for sent in tqdm(sentences, desc=\"Computing sentence embeddings\"):\n",
        "            emb = self.get_sentence_embedding(sent)\n",
        "            embeddings.append(emb)\n",
        "        \n",
        "        embeddings = np.array(embeddings)\n",
        "        similarity_matrix = cosine_similarity(embeddings)\n",
        "        \n",
        "        return similarity_matrix\n",
        "    \n",
        "    def textrank(self, similarity_matrix):\n",
        "        \"\"\"\n",
        "        Run TextRank algorithm (PageRank on sentence graph)\n",
        "        \n",
        "        Args:\n",
        "            similarity_matrix (np.ndarray): Sentence similarity matrix\n",
        "            \n",
        "        Returns:\n",
        "            np.ndarray: PageRank scores for each sentence\n",
        "        \"\"\"\n",
        "        # Create graph from similarity matrix\n",
        "        nx_graph = nx.from_numpy_array(similarity_matrix)\n",
        "        \n",
        "        # Compute PageRank scores\n",
        "        scores = nx.pagerank(nx_graph, alpha=self.damping)\n",
        "        \n",
        "        return np.array(list(scores.values()))\n",
        "    \n",
        "    def summarize(self, document, num_sentences=None):\n",
        "        \"\"\"\n",
        "        Generate extractive summary using TextRank\n",
        "        \n",
        "        Args:\n",
        "            document (str): Input document\n",
        "            num_sentences (int): Number of sentences to extract (default: self.top_n)\n",
        "            \n",
        "        Returns:\n",
        "            str: Extractive summary\n",
        "        \"\"\"\n",
        "        if num_sentences is None:\n",
        "            num_sentences = self.top_n\n",
        "        \n",
        "        # Split into sentences\n",
        "        sentences = sent_tokenize(document)\n",
        "        \n",
        "        if len(sentences) <= num_sentences:\n",
        "            return document\n",
        "        \n",
        "        # Build similarity matrix\n",
        "        similarity_matrix = self.build_similarity_matrix(sentences)\n",
        "        \n",
        "        # Run TextRank\n",
        "        scores = self.textrank(similarity_matrix)\n",
        "        \n",
        "        # Select top sentences\n",
        "        ranked_indices = np.argsort(scores)[::-1][:num_sentences]\n",
        "        \n",
        "        # Sort by original order to maintain coherence\n",
        "        ranked_indices = sorted(ranked_indices)\n",
        "        \n",
        "        # Extract summary\n",
        "        summary_sentences = [sentences[i] for i in ranked_indices]\n",
        "        summary = ' '.join(summary_sentences)\n",
        "        \n",
        "        return summary\n",
        "\n",
        "print(\"‚úÖ TextRank Summarizer class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Initialize TextRank Summarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "INITIALIZING TEXTRANK SUMMARIZER\n",
            "============================================================\n",
            "Loading PhoBERT model for Vietnamese sentence embeddings...\n",
            "‚úì PhoBERT loaded successfully!\n",
            "\n",
            "‚úÖ TextRank Summarizer initialized!\n"
          ]
        }
      ],
      "source": [
        "# Initialize TextRank summarizer\n",
        "print(\"=\"*60)\n",
        "print(\"INITIALIZING TEXTRANK SUMMARIZER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "textrank = TextRankSummarizer(top_n=3, damping=0.85)\n",
        "\n",
        "print(\"\\n‚úÖ TextRank Summarizer initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.4 Test Extractive Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "EXTRACTIVE SUMMARIZATION EXAMPLES\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EXAMPLE 1\n",
            "============================================================\n",
            "\n",
            "üìÑ Original Document (869 words):\n",
            "Nguy√™n nh√¢n\n",
            "Zona kh√¥ng ph·∫£i l√† m·ªôt b·ªánh nhi·ªÖm tr√πng, m√† n√≥ l√† s·ª± t√°i ph√°t c·ªßa virut g√¢y b·ªánh th·ªßy ƒë·∫≠u (Virus Varicella).ƒê·ªëi v·ªõi ng∆∞·ªùi ƒë√£ t·ª´ng m·∫Øc b·ªánh th·ªßy ƒë·∫≠u, sau khi kh·ªèi, virut v·∫´n ch∆∞a b·ªã ti√™u di·ªát ho√†n to√†n m√† ·∫©n trong c√°c t·∫ø b√†o th·∫ßn kinh d∆∞·ªõi d·∫°ng kh√¥ng ho·∫°t ƒë·ªông. Ch√∫ng b·ªã ki·ªÅm ch·∫ø b·ªüi h·ªá mi...\n",
            "\n",
            "ü§ñ Extractive Summary (TextRank):\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "545dd00791f446f184195ac50e9854d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Computing sentence embeddings:   0%|          | 0/33 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ch·∫©n ƒëo√°n\n",
            "Zona g√¢y ra do virut di chuy·ªÉn d·ªçc theo d√¢y th·∫ßn kinh, do ƒë√≥ bi·ªÉu hi·ªán t·ªïn th∆∞∆°ng da th∆∞·ªùng ch·ªâ x·∫£y ra v√† lan ·ªü m·ªôt b√™n c∆° th·ªÉ, v√≠ d·ª• nh∆∞ ch·ªâ m·ªôt b√™n ng·ª±c, m·ªôt b√™n l∆∞ng, m·ªôt b√™n m·∫Øt. N·∫øu ph√°t hi·ªán c√°c v·∫øt m·ª•n n∆∞·ªõc c√≥ d·ªãch ƒë·ª•c th√¨ k·∫øt h·ª£p d√πng kh√°ng sinh d·ª± ph√≤ng nhi·ªÖm khu·∫©n, n√™n d√πng c√°c lo·∫°i kh√°ng sinh th·∫ø h·ªá ƒë·∫ßu cho hi·ªáu qu·∫£ d·ª± ph√≤ng t·ªët. Vaccine VZV, c√≤n ƒë∆∞·ª£c bi·∫øt ƒë·∫øn l√† vaccine ng·ª´a th·ªßy ƒë·∫≠u, c√≥ th·ªÉ l√†m gi·∫£m nguy c∆° m·∫Øc b·ªánh Zona do l√†m tƒÉng s·ª©c ƒë·ªÅ kh√°ng c·ªßa c∆° th·ªÉ ƒë·ªÉ ch·ªëng l·∫°i VZV ho·∫∑c gi·ªØ ch√∫ng trong tr·∫°ng th√°i b·∫•t ho·∫°t.\n",
            "\n",
            "üìù Reference Summary:\n",
            "Zona l√† b·ªánh do s·ª± t√°i ph√°t c·ªßa virut Varicella (g√¢y b·ªánh th·ªßy ƒë·∫≠u). B·ªánh xu·∫•t hi·ªán khi h·ªá mi·ªÖn d·ªãch suy y·∫øu, t·∫°o ƒëi·ªÅu ki·ªán cho virut ·∫©n n√°u t√°i ho·∫°t ƒë·ªông, g√¢y t·ªïn th∆∞∆°ng d·ªçc theo d√¢y th·∫ßn kinh v√† bi·ªÉu hi·ªán tr√™n da. Tri·ªáu ch·ª©ng bao g·ªìm c·∫£m gi√°c ƒëau r√°t tr∆∞·ªõc khi n·ªïi m·ª•n n∆∞·ªõc, xu·∫•t hi·ªán ·ªü m·ªôt b√™n c∆° th·ªÉ. Vi·ªác ch·∫©n ƒëo√°n d·ª±a tr√™n v·ªã tr√≠ v√† h√¨nh d·∫°ng t·ªïn th∆∞∆°ng, v√† ƒëi·ªÅu tr·ªã t·∫≠p trung v√†o vi·ªác d√πng thu·ªëc kh√°ng virus, kh√°ng sinh, gi·∫£m ƒëau v√† thu·ªëc b√¥i. M·∫∑c d√π kh√¥ng c√≥ c√°ch ph√≤ng ng·ª´a tr·ª±c ti·∫øp, nh∆∞ng vi·ªác duy tr√¨ h·ªá mi·ªÖn d·ªãch kh·ªèe m·∫°nh v√† ti√™m vaccine th·ªßy ƒë·∫≠u c√≥ th·ªÉ l√†m gi·∫£m nguy c∆° m·∫Øc b·ªánh.\n",
            "\n",
            "üìä Statistics:\n",
            "  Original: 869 words\n",
            "  Extractive: 121 words\n",
            "  Compression: 13.9%\n",
            "\n",
            "============================================================\n",
            "EXAMPLE 2\n",
            "============================================================\n",
            "\n",
            "üìÑ Original Document (585 words):\n",
            "C√°c b√© N g√°i N th∆∞·ªùng c√≥ V kinh nguy·ªát l·∫ßn N ƒë·∫ßu N v√†o kho·∫£ng N 2 nƒÉm N sau N khi N xu·∫•t hi·ªán V c√°c d·∫•u hi·ªáu N ƒë·∫ßu ti√™n A c·ªßa th·ªùi k·ª≥ N d·∫≠y V th√¨ , th∆∞·ªùng l√† V nh√∫ N ng·ª±c N ( n√∫m v√∫ V h∆°i s∆∞ng A v√† nh√∫ V l√™n V ch·ª© ch∆∞a th·ª±c s·ª± A c√≥ V ng·ª±c N ) , v√† v√†i th√°ng N sau N l√† d·∫•u hi·ªáu N m·ªçc V l√¥ng N n√°ch A ...\n",
            "\n",
            "ü§ñ Extractive Summary (TextRank):\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d63895d5fffc43358c75672d99965b8b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Computing sentence embeddings:   0%|          | 0/13 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chu k·ª≥ V kinh nguy·ªát N c·ªßa m·ªói ph·ª• n·ªØ N c√≥ V kh√°c A nhau N ch√∫t √≠t V , nh∆∞ng d·∫ßn d·∫ßn th√¨ h·∫ßu h·∫øt m·ªçi ng∆∞·ªùi N ƒë·ªÅu h·ªçc V ƒë∆∞·ª£c c√°ch V nh·∫≠n bi·∫øt V chu k·ª≥ N c·ªßa m√¨nh ƒë·ªÉ c√≥ th·ªÉ chu·∫©n b·ªã V tr∆∞·ªõc khi N ƒë·∫øn V th√°ng N . H√†ng V th√°ng N , c∆° th·ªÉ N c·ªßa ph·ª• n·ªØ N trong ƒë·ªô N tu·ªïi N sinh s·∫£n V s·∫Ω chu·∫©n b·ªã V ƒë·ªÉ mang V thai N . Khi V c∆° th·ªÉ N b·∫°n N chu·∫©n b·ªã V cho k·ª≥ N kinh nguy·ªát V , b·∫°n N c√≥ th·ªÉ tr·∫£i V qua c√°c tri·ªáu ch·ª©ng N th∆∞·ªùng g·∫∑p V , ƒë∆∞·ª£c V bi·∫øt V v·ªõi t√™n N g·ªçi V h·ªôi ch·ª©ng V ti·ªÅn N kinh nguy·ªát V .\n",
            "\n",
            "üìù Reference Summary:\n",
            "Kinh nguy·ªát l√† m·ªôt hi·ªán t∆∞·ª£ng sinh l√Ω b√¨nh th∆∞·ªùng c·ªßa ph·ª• n·ªØ, th∆∞·ªùng b·∫Øt ƒë·∫ßu v√†o kho·∫£ng 12 tu·ªïi v√† k√©o d√†i ƒë·∫øn th·ªùi k·ª≥ m√£n kinh. D·∫•u hi·ªáu ƒë·∫ßu ti√™n c·ªßa d·∫≠y th√¨ ·ªü b√© g√°i th∆∞·ªùng l√† s·ª± ph√°t tri·ªÉn c·ªßa nh√∫ ng·ª±c, sau ƒë√≥ l√† m·ªçc l√¥ng n√°ch v√† l√¥ng mu. K·ª≥ kinh nguy·ªát ƒë·∫ßu ti√™n c√≥ th·ªÉ ƒëi k√®m v·ªõi c√°c tri·ªáu ch·ª©ng nh∆∞ ƒëau v√∫ v√† thay ƒë·ªïi t√¢m tr·∫°ng, nh∆∞ng ƒë√¢y l√† ƒëi·ªÅu b√¨nh th∆∞·ªùng. Chu k·ª≥ kinh nguy·ªát, th∆∞·ªùng k√©o d√†i 28 ng√†y, l√† m·ªôt ph·∫ßn c·ªßa chu k·ª≥ sinh s·∫£n v√† c√≥ th·ªÉ ƒëi k√®m v·ªõi h·ªôi ch·ª©ng ti·ªÅn kinh nguy·ªát.\n",
            "\n",
            "üìä Statistics:\n",
            "  Original: 585 words\n",
            "  Extractive: 138 words\n",
            "  Compression: 23.6%\n",
            "\n",
            "============================================================\n",
            "EXAMPLE 3\n",
            "============================================================\n",
            "\n",
            "üìÑ Original Document (379 words):\n",
            "Tweet th√¥ng b√°o v·ªÅ tr∆∞·ªùng h·ª£p s·∫£n ph·ª• sinh con t·∫°i nh√† ·ªü Miami do b√£o Irma .M·ªôt ph·ª• n·ªØ ·ªü ·ªü Little Haiti , th√†nh ph·ªë Miami , bang Florida , M·ªπ v·ªõi s·ª± h·ªó tr·ª£ c·ªßa b√°c sƒ© qua ƒëi·ªán tho·∫°i ƒë√£ t·ª± sinh con t·∫°i nh√† s√°ng ng√†y 10/9 , theo ABC News .Th√†nh ph·ªë Miami t·ªëi 10/9 tweet r·∫±ng S·ªü Ph√≤ng ch√°y Ch·ªØa ch√°y kh√¥...\n",
            "\n",
            "ü§ñ Extractive Summary (TextRank):\n",
            "Tweet th√¥ng b√°o v·ªÅ tr∆∞·ªùng h·ª£p s·∫£n ph·ª• sinh con t·∫°i nh√† ·ªü Miami do b√£o Irma .M·ªôt ph·ª• n·ªØ ·ªü ·ªü Little Haiti , th√†nh ph·ªë Miami , bang Florida , M·ªπ v·ªõi s·ª± h·ªó tr·ª£ c·ªßa b√°c sƒ© qua ƒëi·ªán tho·∫°i ƒë√£ t·ª± sinh con t·∫°i nh√† s√°ng ng√†y 10/9 , theo ABC News .Th√†nh ph·ªë Miami t·ªëi 10/9 tweet r·∫±ng S·ªü Ph√≤ng ch√°y Ch·ªØa ch√°y kh√¥ng th·ªÉ k·ªãp ti·∫øp c·∫≠n ƒë·ªÉ c·ª©u h·ªô s·∫£n ph·ª• do b√£o Irma .\" C√°c b√°c sƒ© t·∫°i b·ªánh vi·ªán Jackson do v·∫≠y ƒë√£ h∆∞·ªõng d·∫´n s·∫£n ph·ª• sinh con t·∫°i nh√† .M·ªôt b√© g√°i !\" , d√≤ng tweet cho bi·∫øt .C√°c b√°c sƒ© c≈©ng h∆∞·ªõng d·∫´n s·∫£n ph·ª• t·ª± c·∫Øt nhau thai .ƒê·ªôi c·ª©u h·ªô sau ƒë√≥ ƒë√£ ti·∫øp c·∫≠n ƒë∆∞·ª£c khu v·ª±c ƒë·ªÉ ƒë∆∞a s·∫£n ph·ª• v√† b√© s∆° sinh trong t√¨nh tr·∫°ng ·ªïn ƒë·ªãnh ƒë·∫øn b·ªánh vi·ªán Jackson .Th√¥ng tin v·ªÅ s·ª± vi·ªác nh·∫≠n ƒë∆∞·ª£c s·ª± t√°n d∆∞∆°ng tr√™n m·∫°ng x√£ h·ªôi .\" N∆∞·ªõc M·ªπ c√≥ nh·ªØng ng∆∞·ªùi c·ª©u h·ªô t·∫≠n t√¨nh tuy·ªát v·ªùi .G·ª≠i l·ªùi ch√∫c m·ª´ng ƒë·∫øn h·ªç , m·∫π v√† em b√© .H√£y gi·ªØ an to√†n ·ªü Florida \" , t√†i kho·∫£n Anne Oosty vi·∫øt .\" M·ªôt c√¢u chuy·ªán ·∫•n t∆∞·ª£ng ƒë·ªÉ m·∫π k·ªÉ v·ªõi b√© sau n√†y .Ch·∫Øc t√™n c√¥ b√© s·∫Ω c√≥ ch·ªØ Irma .M·ª´ng l√† c·∫£ hai ƒë·ªÅu kho·∫ª m·∫°nh .Xin ch√∫c m·ª´ng \" , t√†i kho·∫£n Sharron Taylor b√¨nh lu·∫≠n .Si√™u b√£o m·∫°nh nh·∫•t ƒê·∫°i T√¢y D∆∞∆°ng Irma c√≥ v·∫≠n t·ªëc gi√≥ ban ƒë·∫ßu g·∫ßn 300 km/h v√† t·∫°o ra nh·ªØng con s√≥ng cao t·ªõi 11 m . √çt nh·∫•t 28 ng∆∞·ªùi ch·∫øt khi c√†n qu√©t khu v·ª±c Caribe .B√£o g√¢y ·∫£nh h∆∞·ªüng ƒë·∫øn bang Florida t·ª´ s√°ng 10/9 , g√¢y ng·∫≠p l·ª•t , m·∫•t ƒëi·ªán tr√™n di·ªán r·ªông v√† l√†m √≠t nh·∫•t 5 ng∆∞·ªùi thi·ªát m·∫°ng .ƒê·ªÉ ·ª©ng ph√≥ v·ªõi Irma , ch√≠nh quy·ªÅn bang Florida ti·∫øn h√†nh m·ªôt trong nh·ªØng chi·∫øn d·ªãch s∆° t√°n l·ªõn nh·∫•t l·ªãch s·ª≠ M·ªπ khi di d·ªùi 6,5 tri·ªáu ng∆∞·ªùi kh·ªèi khu v·ª±c ph√≠a nam .B√£o Irma suy y·∫øu xu·ªëng c·∫•p 1 ƒëang di chuy·ªÉn ch·∫≠m v·ªÅ ph√≠a t√¢y b·∫Øc Florida , ƒëi qua khu v·ª±c gi·ªØa hai ƒë√¥ th·ªã l·ªõn l√† Orlando v√† Tampa v·ªõi s·ª©c gi√≥ t·ªëi ƒëa 135 km/h .V≈© Phong\n",
            "\n",
            "üìù Reference Summary:\n",
            "V√†o ng√†y 10/9, m·ªôt s·∫£n ph·ª• ·ªü Miami, bang Florida, M·ªπ ƒë√£ sinh con th√†nh c√¥ng t·∫°i nh√† trong b√£o Irma v·ªõi s·ª± h∆∞·ªõng d·∫´n qua ƒëi·ªán tho·∫°i c·ªßa b√°c sƒ© do ƒë·ªôi c·ª©u h·ªô kh√¥ng th·ªÉ ti·∫øp c·∫≠n k·ªãp th·ªùi. B√°c sƒ© ƒë√£ h∆∞·ªõng d·∫´n s·∫£n ph·ª• t·ª± sinh v√† c·∫Øt d√¢y r·ªën cho b√© g√°i s∆° sinh. Sau ƒë√≥, ƒë·ªôi c·ª©u h·ªô ƒë√£ ƒë∆∞a hai m·∫π con ƒë·∫øn b·ªánh vi·ªán trong t√¨nh tr·∫°ng ·ªïn ƒë·ªãnh. S·ª± vi·ªác ƒë∆∞·ª£c c√¥ng ch√∫ng ca ng·ª£i tr√™n m·∫°ng x√£ h·ªôi. B√£o Irma l√† m·ªôt trong nh·ªØng c∆°n b√£o m·∫°nh nh·∫•t l·ªãch s·ª≠, g√¢y thi·ªát h·∫°i n·∫∑ng n·ªÅ t·∫°i khu v·ª±c Caribe v√† Florida.\n",
            "\n",
            "üìä Statistics:\n",
            "  Original: 379 words\n",
            "  Extractive: 379 words\n",
            "  Compression: 100.0%\n",
            "\n",
            "‚úÖ Extractive summarization demo complete!\n"
          ]
        }
      ],
      "source": [
        "# Test on a few examples with evaluation metrics\n",
        "print(\"=\"*60)\n",
        "print(\"EXTRACTIVE SUMMARIZATION EXAMPLES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "num_examples = 3\n",
        "\n",
        "for i in range(num_examples):\n",
        "    test_doc = dataset['test'][i]['document']\n",
        "    test_ref = dataset['test'][i]['summary']\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EXAMPLE {i+1}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    print(f\"\\nüìÑ Original Document ({len(test_doc.split())} words):\")\n",
        "    print(test_doc[:300] + \"...\")\n",
        "    \n",
        "    print(f\"\\nü§ñ Extractive Summary (TextRank):\")\n",
        "    extractive_summary = textrank.summarize(test_doc, num_sentences=3)\n",
        "    print(extractive_summary)\n",
        "    \n",
        "    print(f\"\\nüìù Reference Summary:\")\n",
        "    print(test_ref)\n",
        "    \n",
        "    # Evaluate the extractive summary\n",
        "    metrics = evaluate_summary(extractive_summary, test_ref, test_doc)\n",
        "    display_evaluation_table(metrics, \"TextRank Extractive\")\n",
        "\n",
        "print(\"\\n‚úÖ Extractive summarization demo complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Load ViT5 Model from HuggingFace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "LOADING VIT5 MODEL\n",
            "============================================================\n",
            "\n",
            "Loading ViT5 model from HuggingFace (YangYang0203/vi5_summarize)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d44669e7cdc74198a6e5f5f7fb287800",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba6217dbbd064b00a1b79784559d7ab9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/820k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc400087054044a5ab9236e8f26f353e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cada0924c13749cba4e3ca0f5c6aa52f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8242751a087047139b9a0bd3c87f5157",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bed14d2281b8492fae1d94d9a8308e76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/904M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3db5128a82bb49e893b6657c3a429d9a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì ViT5 loaded on cuda\n",
            "‚úì Model: Vietnamese-specific T5 (YangYang0203/vi5_summarize)\n",
            "‚úì GPU memory: 3.27 GB\n",
            "\n",
            "‚úÖ Both models loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"LOADING VIT5 MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load ViT5 model from HuggingFace\n",
        "print(\"\\nLoading ViT5 model from HuggingFace (YangYang0203/vi5_summarize)...\")\n",
        "vit5_tokenizer = AutoTokenizer.from_pretrained(\"YangYang0203/vi5_summarize\")\n",
        "vit5_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    \"YangYang0203/vi5_summarize\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "vit5_model.to(device)\n",
        "vit5_model.eval()\n",
        "\n",
        "print(f\"‚úì ViT5 loaded on {device}\")\n",
        "print(f\"‚úì Model: Vietnamese-specific T5 (YangYang0203/vi5_summarize)\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úì GPU memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "\n",
        "print(f\"\\n‚úÖ Both models loaded successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4 Inference Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Inference functions defined!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def generate_summary_vit5(text, max_length=256, min_length=50, num_beams=4):\n",
        "    input_text = f\"t√≥m t·∫Øt: {text}\"\n",
        "    inputs = vit5_tokenizer(\n",
        "        input_text,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = vit5_model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            min_length=min_length,\n",
        "            num_beams=num_beams,\n",
        "            length_penalty=2.0,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=3\n",
        "        )\n",
        "\n",
        "    return vit5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"‚úÖ Inference functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.5 Test Abstractive Summarization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ABSTRACTIVE SUMMARIZATION EXAMPLES\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EXAMPLE 1\n",
            "============================================================\n",
            "\n",
            "üìÑ Original Document (869 words):\n",
            "Nguy√™n nh√¢n\n",
            "Zona kh√¥ng ph·∫£i l√† m·ªôt b·ªánh nhi·ªÖm tr√πng, m√† n√≥ l√† s·ª± t√°i ph√°t c·ªßa virut g√¢y b·ªánh th·ªßy ƒë·∫≠u (Virus Varicella).ƒê·ªëi v·ªõi ng∆∞·ªùi ƒë√£ t·ª´ng m·∫Øc b·ªánh th·ªßy ƒë·∫≠u, sau khi kh·ªèi, virut v·∫´n ch∆∞a b·ªã ti√™u di·ªát ho√†n to√†n m√† ·∫©n trong c√°c t·∫ø b√†o th·∫ßn kinh d∆∞·ªõi d·∫°ng kh√¥ng ho·∫°t ƒë·ªông. Ch√∫ng b·ªã ki·ªÅm ch·∫ø b·ªüi h·ªá mi...\n",
            "\n",
            "ü§ñ mT5-small Summary:\n",
            "<extra_id_0> c√≥ th·ªÉ x·∫£y ra?.. ... ... ................................................................................................. ................... ... ......\" ... ... \" ...\n",
            "\n",
            "ü§ñ ViT5 Summary:\n",
            "B√†i vi·∫øt n√†y t√≥m t·∫Øt v·ªÅ b·ªánh Zona, m·ªôt b·ªánh nhi·ªÖm tr√πng do virus Varicella g√¢y ra. B·ªánh kh√¥ng ph·∫£i l√† b·ªánh truy·ªÅn nhi·ªÖm m√† l√† s·ª± t√°i ph√°t c·ªßa virus th·ªßy ƒë·∫≠u (Virus Varicella), g√¢y ra b·ªüi h·ªá mi·ªÖn d·ªãch t·ª± nhi√™n. C√°c tri·ªáu ch·ª©ng l√¢m s√†ng bao g·ªìm c√°c m·∫£ng ƒë·ªè, n·ªÅ nh·∫π, v√† c√°c t·ªïn th∆∞∆°ng c∆° b·∫£n. Vi·ªác ƒëi·ªÅu tr·ªã bao g·ªìm thu·ªëc kh√°ng virus, thu·ªëc kh√°ng histamin, v√† thu·ªëc gi·∫£m ƒëau th·∫ßn kinh. C√°c bi·ªán ph√°p ph√≤ng ng·ª´a bao g·ªìm v·ªá sinh c√° nh√¢n, s·ª≠ d·ª•ng qu·∫ßn √°o che ch·∫Øn v√† ngh·ªâ ng∆°i h·ª£p l√Ω.\n",
            "\n",
            "üìù Reference Summary:\n",
            "Zona l√† b·ªánh do s·ª± t√°i ph√°t c·ªßa virut Varicella (g√¢y b·ªánh th·ªßy ƒë·∫≠u). B·ªánh xu·∫•t hi·ªán khi h·ªá mi·ªÖn d·ªãch suy y·∫øu, t·∫°o ƒëi·ªÅu ki·ªán cho virut ·∫©n n√°u t√°i ho·∫°t ƒë·ªông, g√¢y t·ªïn th∆∞∆°ng d·ªçc theo d√¢y th·∫ßn kinh v√† bi·ªÉu hi·ªán tr√™n da. Tri·ªáu ch·ª©ng bao g·ªìm c·∫£m gi√°c ƒëau r√°t tr∆∞·ªõc khi n·ªïi m·ª•n n∆∞·ªõc, xu·∫•t hi·ªán ·ªü m·ªôt b√™n c∆° th·ªÉ. Vi·ªác ch·∫©n ƒëo√°n d·ª±a tr√™n v·ªã tr√≠ v√† h√¨nh d·∫°ng t·ªïn th∆∞∆°ng, v√† ƒëi·ªÅu tr·ªã t·∫≠p trung v√†o vi·ªác d√πng thu·ªëc kh√°ng virus, kh√°ng sinh, gi·∫£m ƒëau v√† thu·ªëc b√¥i. M·∫∑c d√π kh√¥ng c√≥ c√°ch ph√≤ng ng·ª´a tr·ª±c ti·∫øp, nh∆∞ng vi·ªác duy tr√¨ h·ªá mi·ªÖn d·ªãch kh·ªèe m·∫°nh v√† ti√™m vaccine th·ªßy ƒë·∫≠u c√≥ th·ªÉ l√†m gi·∫£m nguy c∆° m·∫Øc b·ªánh.\n",
            "\n",
            "üìä Statistics:\n",
            "  Original: 869 words\n",
            "  mT5: 15 words\n",
            "  ViT5: 100 words\n",
            "  Reference: 127 words\n",
            "\n",
            "============================================================\n",
            "EXAMPLE 2\n",
            "============================================================\n",
            "\n",
            "üìÑ Original Document (585 words):\n",
            "C√°c b√© N g√°i N th∆∞·ªùng c√≥ V kinh nguy·ªát l·∫ßn N ƒë·∫ßu N v√†o kho·∫£ng N 2 nƒÉm N sau N khi N xu·∫•t hi·ªán V c√°c d·∫•u hi·ªáu N ƒë·∫ßu ti√™n A c·ªßa th·ªùi k·ª≥ N d·∫≠y V th√¨ , th∆∞·ªùng l√† V nh√∫ N ng·ª±c N ( n√∫m v√∫ V h∆°i s∆∞ng A v√† nh√∫ V l√™n V ch·ª© ch∆∞a th·ª±c s·ª± A c√≥ V ng·ª±c N ) , v√† v√†i th√°ng N sau N l√† d·∫•u hi·ªáu N m·ªçc V l√¥ng N n√°ch A ...\n",
            "\n",
            "ü§ñ mT5-small Summary:\n",
            "<extra_id_0> , n·∫øu V c√≥ V . Kinh <extra_id_1> N kinh nguy·ªát N , b·∫°n N c√≥ th·ªÉ :\n",
            "\n",
            "ü§ñ ViT5 Summary:\n",
            "B√†i vi·∫øt n√†y t·∫≠p trung v√†o chu k·ª≥ kinh nguy·ªát, m·ªôt hi·ªán t∆∞·ª£ng sinh l√Ω b√¨nh th∆∞·ªùng c·ªßa con ng∆∞·ªùi, th∆∞·ªùng b·∫Øt ƒë·∫ßu v√†o kho·∫£ng 12 tu·ªïi v√† tr·∫£i qua 39 nƒÉm v·ªõi kinh nguy·ªát. Chu k·ª≥ n√†y th∆∞·ªùng xu·∫•t hi·ªán h√†ng th√°ng, v·ªõi c√°c tri·ªáu ch·ª©ng nh∆∞ ƒëau v√∫, thay ƒë·ªïi t√¢m tr·∫°ng v√† ƒëau b·ª•ng kinh. Kinh nguy·ªát l√† hi·ªán t∆∞·ª£ng b√¨nh th∆∞·ªùng, th∆∞·ªùng k√©o d√†i 28 ng√†y v√† xu·∫•t hi·ªán m·ªói th√°ng m·ªôt l·∫ßn. H·ªôi ch·ª©ng ti·ªÅn kinh nguy·ªát l√† m·ªôt trong nh·ªØng hi·ªán t∆∞·ª£ng ph·ªï bi·∫øn c·ªßa ph·ª• n·ªØ, c√≥ th·ªÉ g√¢y ra c√°c v·∫•n ƒë·ªÅ v·ªÅ s·ª©c kh·ªèe v√† t√¢m l√Ω, ƒë·∫∑c bi·ªát l√† ·ªü ph·ª• n·ªØ trong ƒë·ªô\n",
            "\n",
            "üìù Reference Summary:\n",
            "Kinh nguy·ªát l√† m·ªôt hi·ªán t∆∞·ª£ng sinh l√Ω b√¨nh th∆∞·ªùng c·ªßa ph·ª• n·ªØ, th∆∞·ªùng b·∫Øt ƒë·∫ßu v√†o kho·∫£ng 12 tu·ªïi v√† k√©o d√†i ƒë·∫øn th·ªùi k·ª≥ m√£n kinh. D·∫•u hi·ªáu ƒë·∫ßu ti√™n c·ªßa d·∫≠y th√¨ ·ªü b√© g√°i th∆∞·ªùng l√† s·ª± ph√°t tri·ªÉn c·ªßa nh√∫ ng·ª±c, sau ƒë√≥ l√† m·ªçc l√¥ng n√°ch v√† l√¥ng mu. K·ª≥ kinh nguy·ªát ƒë·∫ßu ti√™n c√≥ th·ªÉ ƒëi k√®m v·ªõi c√°c tri·ªáu ch·ª©ng nh∆∞ ƒëau v√∫ v√† thay ƒë·ªïi t√¢m tr·∫°ng, nh∆∞ng ƒë√¢y l√† ƒëi·ªÅu b√¨nh th∆∞·ªùng. Chu k·ª≥ kinh nguy·ªát, th∆∞·ªùng k√©o d√†i 28 ng√†y, l√† m·ªôt ph·∫ßn c·ªßa chu k·ª≥ sinh s·∫£n v√† c√≥ th·ªÉ ƒëi k√®m v·ªõi h·ªôi ch·ª©ng ti·ªÅn kinh nguy·ªát.\n",
            "\n",
            "üìä Statistics:\n",
            "  Original: 585 words\n",
            "  mT5: 19 words\n",
            "  ViT5: 114 words\n",
            "  Reference: 110 words\n",
            "\n",
            "============================================================\n",
            "EXAMPLE 3\n",
            "============================================================\n",
            "\n",
            "üìÑ Original Document (379 words):\n",
            "Tweet th√¥ng b√°o v·ªÅ tr∆∞·ªùng h·ª£p s·∫£n ph·ª• sinh con t·∫°i nh√† ·ªü Miami do b√£o Irma .M·ªôt ph·ª• n·ªØ ·ªü ·ªü Little Haiti , th√†nh ph·ªë Miami , bang Florida , M·ªπ v·ªõi s·ª± h·ªó tr·ª£ c·ªßa b√°c sƒ© qua ƒëi·ªán tho·∫°i ƒë√£ t·ª± sinh con t·∫°i nh√† s√°ng ng√†y 10/9 , theo ABC News .Th√†nh ph·ªë Miami t·ªëi 10/9 tweet r·∫±ng S·ªü Ph√≤ng ch√°y Ch·ªØa ch√°y kh√¥...\n",
            "\n",
            "ü§ñ mT5-small Summary:\n",
            "<extra_id_0> .M·ªôt b√© g√°i . <extra_id_1> .\" , d√≤ng tweet cho bi·∫øt .Si√™u m·ªõi nh·∫•t:\n",
            "\n",
            "ü§ñ ViT5 Summary:\n",
            "V√†o ng√†y 10/9, m·ªôt s·∫£n ph·ª• ·ªü Miami, Florida, M·ªπ ƒë√£ t·ª± sinh con t·∫°i nh√† do b√£o Irma, g√¢y ra h·∫≠u qu·∫£ nghi√™m tr·ªçng. C√°c b√°c sƒ© t·∫°i b·ªánh vi·ªán Jackson ƒë√£ nhanh ch√≥ng ti·∫øp c·∫≠n v√† h∆∞·ªõng d·∫´n s·∫£n ph·ª• t·ª± c·∫Øt nhau thai. S·ª± vi·ªác nh·∫≠n ƒë∆∞·ª£c s·ª± t√°n d∆∞∆°ng tr√™n m·∫°ng x√£ h·ªôi, th·ªÉ hi·ªán s·ª± quan t√¢m ƒë·∫øn s·ª± an to√†n c·ªßa c·∫£ m·∫π v√† b√©. B√£o Irma l√† c∆°n b√£o m·∫°nh nh·∫•t ƒê·∫°i T√¢y D∆∞∆°ng, g√¢y ng·∫≠p l·ª•t v√† m·∫•t ƒëi·ªán nghi√™m tr·ªçng, g√¢y thi·ªát h·∫°i l·ªõn cho Florida. ƒê·ªÉ ·ª©ng ph√≥, ch√≠nh quy·ªÅn bang Florida ƒë√£ ti·∫øn h√†nh s∆° t√°n l·ªõn, di d·ªùi\n",
            "\n",
            "üìù Reference Summary:\n",
            "V√†o ng√†y 10/9, m·ªôt s·∫£n ph·ª• ·ªü Miami, bang Florida, M·ªπ ƒë√£ sinh con th√†nh c√¥ng t·∫°i nh√† trong b√£o Irma v·ªõi s·ª± h∆∞·ªõng d·∫´n qua ƒëi·ªán tho·∫°i c·ªßa b√°c sƒ© do ƒë·ªôi c·ª©u h·ªô kh√¥ng th·ªÉ ti·∫øp c·∫≠n k·ªãp th·ªùi. B√°c sƒ© ƒë√£ h∆∞·ªõng d·∫´n s·∫£n ph·ª• t·ª± sinh v√† c·∫Øt d√¢y r·ªën cho b√© g√°i s∆° sinh. Sau ƒë√≥, ƒë·ªôi c·ª©u h·ªô ƒë√£ ƒë∆∞a hai m·∫π con ƒë·∫øn b·ªánh vi·ªán trong t√¨nh tr·∫°ng ·ªïn ƒë·ªãnh. S·ª± vi·ªác ƒë∆∞·ª£c c√¥ng ch√∫ng ca ng·ª£i tr√™n m·∫°ng x√£ h·ªôi. B√£o Irma l√† m·ªôt trong nh·ªØng c∆°n b√£o m·∫°nh nh·∫•t l·ªãch s·ª≠, g√¢y thi·ªát h·∫°i n·∫∑ng n·ªÅ t·∫°i khu v·ª±c Caribe v√† Florida.\n",
            "\n",
            "üìä Statistics:\n",
            "  Original: 379 words\n",
            "  mT5: 15 words\n",
            "  ViT5: 110 words\n",
            "  Reference: 111 words\n",
            "\n",
            "‚úÖ Abstractive summarization demo complete!\n"
          ]
        }
      ],
      "source": [
        "# Test both models on examples with evaluation\n",
        "print(\"=\"*60)\n",
        "print(\"ABSTRACTIVE SUMMARIZATION EXAMPLES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "num_examples = 3\n",
        "\n",
        "for i in range(num_examples):\n",
        "    test_doc = dataset['test'][i]['document']\n",
        "    test_ref = dataset['test'][i]['summary']\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EXAMPLE {i+1}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    print(f\"\\nüìÑ Original Document ({len(test_doc.split())} words):\")\n",
        "    print(test_doc[:300] + \"...\")\n",
        "\n",
        "    print(f\"\\nü§ñ mT5-small Summary:\")\n",
        "    mt5_summary = generate_summary_mt5(test_doc)\n",
        "    print(mt5_summary)\n",
        "\n",
        "    print(f\"\\nü§ñ ViT5 Summary:\")\n",
        "    vit5_summary = generate_summary_vit5(test_doc)\n",
        "    print(vit5_summary)\n",
        "\n",
        "    print(f\"\\nüìù Reference Summary:\")\n",
        "    print(test_ref)\n",
        "    \n",
        "    # Evaluate both models\n",
        "    mt5_metrics = evaluate_summary(mt5_summary, test_ref, test_doc)\n",
        "    vit5_metrics = evaluate_summary(vit5_summary, test_ref, test_doc)\n",
        "    \n",
        "    # Display comparison\n",
        "    compare_models([mt5_metrics, vit5_metrics], ['mT5-small', 'ViT5'])\n",
        "\n",
        "print(\"\\n‚úÖ Abstractive summarization demo complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.0 Evaluation Helper Functions\n",
        "\n",
        "These helper functions compute comprehensive evaluation metrics:\n",
        "\n",
        "- **ROUGE scores**: Precision, Recall, F1 for ROUGE-1, ROUGE-2, ROUGE-L\n",
        "- **BLEU score**: Machine translation quality metric\n",
        "- **Statistics**: Length comparison and compression ratio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU\n",
        "from tabulate import tabulate\n",
        "\n",
        "def evaluate_summary(prediction, reference, original_doc):\n",
        "    \"\"\"\n",
        "    Compute comprehensive evaluation metrics for a summary\n",
        "    \n",
        "    Args:\n",
        "        prediction (str): Generated summary\n",
        "        reference (str): Reference/gold summary\n",
        "        original_doc (str): Original document\n",
        "    \n",
        "    Returns:\n",
        "        dict: Dictionary containing all evaluation metrics\n",
        "    \"\"\"\n",
        "    # Initialize ROUGE scorer\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
        "    rouge_scores = scorer.score(reference, prediction)\n",
        "    \n",
        "    # Initialize BLEU scorer\n",
        "    bleu = BLEU()\n",
        "    bleu_score = bleu.sentence_score(prediction, [reference])\n",
        "    \n",
        "    # Calculate statistics\n",
        "    doc_words = len(original_doc.split())\n",
        "    pred_words = len(prediction.split())\n",
        "    ref_words = len(reference.split())\n",
        "    compression = (pred_words / doc_words * 100) if doc_words > 0 else 0\n",
        "    \n",
        "    return {\n",
        "        'rouge1_f1': rouge_scores['rouge1'].fmeasure,\n",
        "        'rouge1_p': rouge_scores['rouge1'].precision,\n",
        "        'rouge1_r': rouge_scores['rouge1'].recall,\n",
        "        'rouge2_f1': rouge_scores['rouge2'].fmeasure,\n",
        "        'rouge2_p': rouge_scores['rouge2'].precision,\n",
        "        'rouge2_r': rouge_scores['rouge2'].recall,\n",
        "        'rougeL_f1': rouge_scores['rougeL'].fmeasure,\n",
        "        'rougeL_p': rouge_scores['rougeL'].precision,\n",
        "        'rougeL_r': rouge_scores['rougeL'].recall,\n",
        "        'bleu': bleu_score.score,\n",
        "        'doc_words': doc_words,\n",
        "        'pred_words': pred_words,\n",
        "        'ref_words': ref_words,\n",
        "        'compression': compression\n",
        "    }\n",
        "\n",
        "def display_evaluation_table(metrics, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Display evaluation metrics in a formatted table\n",
        "    \n",
        "    Args:\n",
        "        metrics (dict): Evaluation metrics from evaluate_summary()\n",
        "        model_name (str): Name of the model for display\n",
        "    \"\"\"\n",
        "    print(f\"\\nüìä Evaluation Metrics for {model_name}\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # ROUGE scores table\n",
        "    rouge_table = [\n",
        "        ['ROUGE-1', f\"{metrics['rouge1_p']:.4f}\", f\"{metrics['rouge1_r']:.4f}\", f\"{metrics['rouge1_f1']:.4f}\"],\n",
        "        ['ROUGE-2', f\"{metrics['rouge2_p']:.4f}\", f\"{metrics['rouge2_r']:.4f}\", f\"{metrics['rouge2_f1']:.4f}\"],\n",
        "        ['ROUGE-L', f\"{metrics['rougeL_p']:.4f}\", f\"{metrics['rougeL_r']:.4f}\", f\"{metrics['rougeL_f1']:.4f}\"]\n",
        "    ]\n",
        "    print(\"\\nROUGE Scores:\")\n",
        "    print(tabulate(rouge_table, headers=['Metric', 'Precision', 'Recall', 'F1-Score'], tablefmt='grid'))\n",
        "    \n",
        "    # BLEU and statistics\n",
        "    stats_table = [\n",
        "        ['BLEU Score', f\"{metrics['bleu']:.2f}\"],\n",
        "        ['Original Length', f\"{metrics['doc_words']} words\"],\n",
        "        ['Prediction Length', f\"{metrics['pred_words']} words\"],\n",
        "        ['Reference Length', f\"{metrics['ref_words']} words\"],\n",
        "        ['Compression Ratio', f\"{metrics['compression']:.1f}%\"]\n",
        "    ]\n",
        "    print(\"\\nAdditional Metrics:\")\n",
        "    print(tabulate(stats_table, headers=['Metric', 'Value'], tablefmt='grid'))\n",
        "\n",
        "def compare_models(metrics_list, model_names):\n",
        "    \"\"\"\n",
        "    Compare multiple models side by side\n",
        "    \n",
        "    Args:\n",
        "        metrics_list (list): List of metrics dictionaries\n",
        "        model_names (list): List of model names\n",
        "    \"\"\"\n",
        "    print(\"\\nüìä Model Comparison\")\n",
        "    print(\"=\" * 100)\n",
        "    \n",
        "    comparison_table = [\n",
        "        ['ROUGE-1 F1'] + [f\"{m['rouge1_f1']:.4f}\" for m in metrics_list],\n",
        "        ['ROUGE-2 F1'] + [f\"{m['rouge2_f1']:.4f}\" for m in metrics_list],\n",
        "        ['ROUGE-L F1'] + [f\"{m['rougeL_f1']:.4f}\" for m in metrics_list],\n",
        "        ['BLEU'] + [f\"{m['bleu']:.2f}\" for m in metrics_list],\n",
        "        ['Length (words)'] + [f\"{m['pred_words']}\" for m in metrics_list],\n",
        "        ['Compression'] + [f\"{m['compression']:.1f}%\" for m in metrics_list]\n",
        "    ]\n",
        "    \n",
        "    print(tabulate(comparison_table, headers=['Metric'] + model_names, tablefmt='grid'))\n",
        "    \n",
        "    # Highlight best scores\n",
        "    print(\"\\nüèÜ Best Scores:\")\n",
        "    best_rouge1 = max(range(len(metrics_list)), key=lambda i: metrics_list[i]['rouge1_f1'])\n",
        "    best_rouge2 = max(range(len(metrics_list)), key=lambda i: metrics_list[i]['rouge2_f1'])\n",
        "    best_rougeL = max(range(len(metrics_list)), key=lambda i: metrics_list[i]['rougeL_f1'])\n",
        "    best_bleu = max(range(len(metrics_list)), key=lambda i: metrics_list[i]['bleu'])\n",
        "    \n",
        "    print(f\"  ‚Ä¢ ROUGE-1: {model_names[best_rouge1]} ({metrics_list[best_rouge1]['rouge1_f1']:.4f})\")\n",
        "    print(f\"  ‚Ä¢ ROUGE-2: {model_names[best_rouge2]} ({metrics_list[best_rouge2]['rouge2_f1']:.4f})\")\n",
        "    print(f\"  ‚Ä¢ ROUGE-L: {model_names[best_rougeL]} ({metrics_list[best_rougeL]['rougeL_f1']:.4f})\")\n",
        "    print(f\"  ‚Ä¢ BLEU: {model_names[best_bleu]} ({metrics_list[best_bleu]['bleu']:.2f})\")\n",
        "\n",
        "print(\"‚úÖ Evaluation helper functions loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 5. Evaluation & Comparison\n",
        "\n",
        "## 5.1 ROUGE Metrics Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ROUGE computation function defined!\n"
          ]
        }
      ],
      "source": [
        "def compute_rouge_scores(predictions, references):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
        "\n",
        "    scores = {\n",
        "        'rouge1': {'precision': [], 'recall': [], 'fmeasure': []},\n",
        "        'rouge2': {'precision': [], 'recall': [], 'fmeasure': []},\n",
        "        'rougeL': {'precision': [], 'recall': [], 'fmeasure': []}\n",
        "    }\n",
        "\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        result = scorer.score(ref, pred)\n",
        "        for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
        "            scores[metric]['precision'].append(result[metric].precision)\n",
        "            scores[metric]['recall'].append(result[metric].recall)\n",
        "            scores[metric]['fmeasure'].append(result[metric].fmeasure)\n",
        "\n",
        "    return scores\n",
        "\n",
        "print(\"‚úÖ ROUGE computation function defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2 Generate Predictions on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"GENERATING PREDICTIONS ON TEST SET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Use subset for faster execution (adjust as needed)\n",
        "sample_size = 500\n",
        "print(f\"\\nUsing {sample_size} samples from test set\")\n",
        "print(\"This will take approximately 10-15 minutes...\")\n",
        "\n",
        "test_docs_sample = dataset['test']['document'][:sample_size]\n",
        "test_refs_sample = dataset['test']['summary'][:sample_size]\n",
        "\n",
        "# Initialize lists\n",
        "mt5_predictions = []\n",
        "vit5_predictions = []\n",
        "extractive_predictions = []\n",
        "\n",
        "# Generate predictions with progress bar\n",
        "print(\"\\nGenerating predictions...\")\n",
        "\n",
        "for i, doc in enumerate(tqdm(test_docs_sample, desc=\"Processing\")):\n",
        "    # ViT5 predictions\n",
        "    vit5_pred = generate_summary_vit5(doc)\n",
        "    vit5_predictions.append(vit5_pred)\n",
        "\n",
        "    # Extractive predictions\n",
        "    extractive_pred = textrank.summarize(doc, num_sentences=3)\n",
        "    extractive_predictions.append(extractive_pred)\n",
        "\n",
        "    if (i + 1) % 50 == 0:\n",
        "        print(f\"  Processed {i + 1}/{sample_size} samples...\")\n",
        "\n",
        "print(f\"\\n‚úÖ All {sample_size} predictions generated!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3 Compute ROUGE Scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "COMPUTING ROUGE SCORES\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS\n",
            "============================================================\n",
            "\n",
            "mT5-small:\n",
            "  ROUGE-1 F1: 0.1269\n",
            "  ROUGE-2 F1: 0.0571\n",
            "  ROUGE-L F1: 0.1074\n",
            "\n",
            "ViT5:\n",
            "  ROUGE-1 F1: 0.7781\n",
            "  ROUGE-2 F1: 0.4963\n",
            "  ROUGE-L F1: 0.4915\n",
            "\n",
            "TextRank (Extractive):\n",
            "  ROUGE-1 F1: 0.5924\n",
            "  ROUGE-2 F1: 0.3267\n",
            "  ROUGE-L F1: 0.3587\n",
            "\n",
            "‚úÖ ROUGE evaluation complete!\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"COMPUTING ROUGE SCORES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Compute ROUGE scores for all models\n",
        "vit5_scores = compute_rouge_scores(vit5_predictions, test_refs_sample)\n",
        "extractive_scores = compute_rouge_scores(extractive_predictions, test_refs_sample)\n",
        "\n",
        "# Create models dictionary\n",
        "models = {\n",
        "    'ViT5': vit5_scores,\n",
        "    'TextRank (Extractive)': extractive_scores\n",
        "}\n",
        "\n",
        "# Print results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for model_name, scores in models.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  ROUGE-1 F1: {np.mean(scores['rouge1']['fmeasure']):.4f}\")\n",
        "    print(f\"  ROUGE-2 F1: {np.mean(scores['rouge2']['fmeasure']):.4f}\")\n",
        "    print(f\"  ROUGE-L F1: {np.mean(scores['rougeL']['fmeasure']):.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ ROUGE evaluation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.4 Detailed Comparison Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "DETAILED MODEL COMPARISON\n",
            "================================================================================\n",
            "                Model         ROUGE-1         ROUGE-2         ROUGE-L      Avg\n",
            "                 ViT5 0.7781 ¬± 0.0457 0.4963 ¬± 0.0897 0.4915 ¬± 0.0912 0.588634\n",
            "TextRank (Extractive) 0.5924 ¬± 0.1170 0.3267 ¬± 0.0874 0.3587 ¬± 0.0689 0.425942\n",
            "            mT5-small 0.1269 ¬± 0.0544 0.0571 ¬± 0.0344 0.1074 ¬± 0.0410 0.097129\n",
            "\n",
            "‚úÖ Comparison table created!\n"
          ]
        }
      ],
      "source": [
        "# Create detailed comparison table\n",
        "comparison_data = []\n",
        "\n",
        "for model_name, scores in models.items():\n",
        "    comparison_data.append({\n",
        "        'Model': model_name,\n",
        "        'ROUGE-1': f\"{np.mean(scores['rouge1']['fmeasure']):.4f} ¬± {np.std(scores['rouge1']['fmeasure']):.4f}\",\n",
        "        'ROUGE-2': f\"{np.mean(scores['rouge2']['fmeasure']):.4f} ¬± {np.std(scores['rouge2']['fmeasure']):.4f}\",\n",
        "        'ROUGE-L': f\"{np.mean(scores['rougeL']['fmeasure']):.4f} ¬± {np.std(scores['rougeL']['fmeasure']):.4f}\",\n",
        "        'Avg': np.mean([\n",
        "            np.mean(scores['rouge1']['fmeasure']),\n",
        "            np.mean(scores['rouge2']['fmeasure']),\n",
        "            np.mean(scores['rougeL']['fmeasure'])\n",
        "        ])\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.sort_values('Avg', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DETAILED MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"\\n‚úÖ Comparison table created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.5 Side-by-Side Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SIDE-BY-SIDE COMPARISON EXAMPLES\n",
            "============================================================\n",
            "\n",
            "================================================================================\n",
            "EXAMPLE 1\n",
            "================================================================================\n",
            "\n",
            "üìÑ Original Document (869 words):\n",
            "Nguy√™n nh√¢n\n",
            "Zona kh√¥ng ph·∫£i l√† m·ªôt b·ªánh nhi·ªÖm tr√πng, m√† n√≥ l√† s·ª± t√°i ph√°t c·ªßa virut g√¢y b·ªánh th·ªßy ƒë·∫≠u (Virus Varicella).ƒê·ªëi v·ªõi ng∆∞·ªùi ƒë√£ t·ª´ng m·∫Øc b·ªánh th·ªßy ƒë·∫≠u, sau khi kh·ªèi, virut v·∫´n ch∆∞a b·ªã ti√™u di...\n",
            "\n",
            "ü§ñ mT5-small:\n",
            "<extra_id_0> c√≥ th·ªÉ x·∫£y ra?.. ... ... ................................................................................................. ................... ... ......\" ... ... \" ...\n",
            "\n",
            "ü§ñ ViT5:\n",
            "B√†i vi·∫øt n√†y t√≥m t·∫Øt v·ªÅ b·ªánh Zona, m·ªôt b·ªánh nhi·ªÖm tr√πng do virus Varicella g√¢y ra. B·ªánh kh√¥ng ph·∫£i l√† b·ªánh truy·ªÅn nhi·ªÖm m√† l√† s·ª± t√°i ph√°t c·ªßa virus th·ªßy ƒë·∫≠u (Virus Varicella), g√¢y ra b·ªüi h·ªá mi·ªÖn d·ªãch t·ª± nhi√™n. C√°c tri·ªáu ch·ª©ng l√¢m s√†ng bao g·ªìm c√°c m·∫£ng ƒë·ªè, n·ªÅ nh·∫π, v√† c√°c t·ªïn th∆∞∆°ng c∆° b·∫£n. Vi·ªác ƒëi·ªÅu tr·ªã bao g·ªìm thu·ªëc kh√°ng virus, thu·ªëc kh√°ng histamin, v√† thu·ªëc gi·∫£m ƒëau th·∫ßn kinh. C√°c bi·ªán ph√°p ph√≤ng ng·ª´a bao g·ªìm v·ªá sinh c√° nh√¢n, s·ª≠ d·ª•ng qu·∫ßn √°o che ch·∫Øn v√† ngh·ªâ ng∆°i h·ª£p l√Ω.\n",
            "\n",
            "ü§ñ TextRank (Extractive):\n",
            "Ch·∫©n ƒëo√°n\n",
            "Zona g√¢y ra do virut di chuy·ªÉn d·ªçc theo d√¢y th·∫ßn kinh, do ƒë√≥ bi·ªÉu hi·ªán t·ªïn th∆∞∆°ng da th∆∞·ªùng ch·ªâ x·∫£y ra v√† lan ·ªü m·ªôt b√™n c∆° th·ªÉ, v√≠ d·ª• nh∆∞ ch·ªâ m·ªôt b√™n ng·ª±c, m·ªôt b√™n l∆∞ng, m·ªôt b√™n m·∫Øt. N·∫øu ph√°\n",
            "\n",
            "üìù Reference:\n",
            "Zona l√† b·ªánh do s·ª± t√°i ph√°t c·ªßa virut Varicella (g√¢y b·ªánh th·ªßy ƒë·∫≠u). B·ªánh xu·∫•t hi·ªán khi h·ªá mi·ªÖn d·ªãch suy y·∫øu, t·∫°o ƒëi·ªÅu ki·ªán cho virut ·∫©n n√°u t√°i ho·∫°t ƒë·ªông, g√¢y t·ªïn th∆∞∆°ng d·ªçc theo d√¢y th·∫ßn kinh v√† bi·ªÉu hi·ªán tr√™n da. Tri·ªáu ch·ª©ng bao g·ªìm c·∫£m gi√°c ƒëau r√°t tr∆∞·ªõc khi n·ªïi m·ª•n n∆∞·ªõc, xu·∫•t hi·ªán ·ªü m·ªôt b√™n c∆° th·ªÉ. Vi·ªác ch·∫©n ƒëo√°n d·ª±a tr√™n v·ªã tr√≠ v√† h√¨nh d·∫°ng t·ªïn th∆∞∆°ng, v√† ƒëi·ªÅu tr·ªã t·∫≠p trung v√†o vi·ªác d√πng thu·ªëc kh√°ng virus, kh√°ng sinh, gi·∫£m ƒëau v√† thu·ªëc b√¥i. M·∫∑c d√π kh√¥ng c√≥ c√°ch ph√≤ng ng·ª´a tr·ª±c ti·∫øp, nh∆∞ng vi·ªác duy tr√¨ h·ªá mi·ªÖn d·ªãch kh·ªèe m·∫°nh v√† ti√™m vaccine th·ªßy ƒë·∫≠u c√≥ th·ªÉ l√†m gi·∫£m nguy c∆° m·∫Øc b·ªánh.\n",
            "\n",
            "================================================================================\n",
            "EXAMPLE 2\n",
            "================================================================================\n",
            "\n",
            "üìÑ Original Document (585 words):\n",
            "C√°c b√© N g√°i N th∆∞·ªùng c√≥ V kinh nguy·ªát l·∫ßn N ƒë·∫ßu N v√†o kho·∫£ng N 2 nƒÉm N sau N khi N xu·∫•t hi·ªán V c√°c d·∫•u hi·ªáu N ƒë·∫ßu ti√™n A c·ªßa th·ªùi k·ª≥ N d·∫≠y V th√¨ , th∆∞·ªùng l√† V nh√∫ N ng·ª±c N ( n√∫m v√∫ V h∆°i s∆∞ng A v√† nh...\n",
            "\n",
            "ü§ñ mT5-small:\n",
            "<extra_id_0> , n·∫øu V c√≥ V . Kinh <extra_id_1> N kinh nguy·ªát N , b·∫°n N c√≥ th·ªÉ :\n",
            "\n",
            "ü§ñ ViT5:\n",
            "B√†i vi·∫øt n√†y t·∫≠p trung v√†o chu k·ª≥ kinh nguy·ªát, m·ªôt hi·ªán t∆∞·ª£ng sinh l√Ω b√¨nh th∆∞·ªùng c·ªßa con ng∆∞·ªùi, th∆∞·ªùng b·∫Øt ƒë·∫ßu v√†o kho·∫£ng 12 tu·ªïi v√† tr·∫£i qua 39 nƒÉm v·ªõi kinh nguy·ªát. Chu k·ª≥ n√†y th∆∞·ªùng xu·∫•t hi·ªán h√†ng th√°ng, v·ªõi c√°c tri·ªáu ch·ª©ng nh∆∞ ƒëau v√∫, thay ƒë·ªïi t√¢m tr·∫°ng v√† ƒëau b·ª•ng kinh. Kinh nguy·ªát l√† hi·ªán t∆∞·ª£ng b√¨nh th∆∞·ªùng, th∆∞·ªùng k√©o d√†i 28 ng√†y v√† xu·∫•t hi·ªán m·ªói th√°ng m·ªôt l·∫ßn. H·ªôi ch·ª©ng ti·ªÅn kinh nguy·ªát l√† m·ªôt trong nh·ªØng hi·ªán t∆∞·ª£ng ph·ªï bi·∫øn c·ªßa ph·ª• n·ªØ, c√≥ th·ªÉ g√¢y ra c√°c v·∫•n ƒë·ªÅ v·ªÅ s·ª©c kh·ªèe v√† t√¢m l√Ω, ƒë·∫∑c bi·ªát l√† ·ªü ph·ª• n·ªØ trong ƒë·ªô\n",
            "\n",
            "ü§ñ TextRank (Extractive):\n",
            "Chu k·ª≥ V kinh nguy·ªát N c·ªßa m·ªói ph·ª• n·ªØ N c√≥ V kh√°c A nhau N ch√∫t √≠t V , nh∆∞ng d·∫ßn d·∫ßn th√¨ h·∫ßu h·∫øt m·ªçi ng∆∞·ªùi N ƒë·ªÅu h·ªçc V ƒë∆∞·ª£c c√°ch V nh·∫≠n bi·∫øt V chu k·ª≥ N c·ªßa m√¨nh ƒë·ªÉ c√≥ th·ªÉ chu·∫©n b·ªã V tr∆∞·ªõc khi N ƒë·∫øn V \n",
            "\n",
            "üìù Reference:\n",
            "Kinh nguy·ªát l√† m·ªôt hi·ªán t∆∞·ª£ng sinh l√Ω b√¨nh th∆∞·ªùng c·ªßa ph·ª• n·ªØ, th∆∞·ªùng b·∫Øt ƒë·∫ßu v√†o kho·∫£ng 12 tu·ªïi v√† k√©o d√†i ƒë·∫øn th·ªùi k·ª≥ m√£n kinh. D·∫•u hi·ªáu ƒë·∫ßu ti√™n c·ªßa d·∫≠y th√¨ ·ªü b√© g√°i th∆∞·ªùng l√† s·ª± ph√°t tri·ªÉn c·ªßa nh√∫ ng·ª±c, sau ƒë√≥ l√† m·ªçc l√¥ng n√°ch v√† l√¥ng mu. K·ª≥ kinh nguy·ªát ƒë·∫ßu ti√™n c√≥ th·ªÉ ƒëi k√®m v·ªõi c√°c tri·ªáu ch·ª©ng nh∆∞ ƒëau v√∫ v√† thay ƒë·ªïi t√¢m tr·∫°ng, nh∆∞ng ƒë√¢y l√† ƒëi·ªÅu b√¨nh th∆∞·ªùng. Chu k·ª≥ kinh nguy·ªát, th∆∞·ªùng k√©o d√†i 28 ng√†y, l√† m·ªôt ph·∫ßn c·ªßa chu k·ª≥ sinh s·∫£n v√† c√≥ th·ªÉ ƒëi k√®m v·ªõi h·ªôi ch·ª©ng ti·ªÅn kinh nguy·ªát.\n",
            "\n",
            "================================================================================\n",
            "EXAMPLE 3\n",
            "================================================================================\n",
            "\n",
            "üìÑ Original Document (379 words):\n",
            "Tweet th√¥ng b√°o v·ªÅ tr∆∞·ªùng h·ª£p s·∫£n ph·ª• sinh con t·∫°i nh√† ·ªü Miami do b√£o Irma .M·ªôt ph·ª• n·ªØ ·ªü ·ªü Little Haiti , th√†nh ph·ªë Miami , bang Florida , M·ªπ v·ªõi s·ª± h·ªó tr·ª£ c·ªßa b√°c sƒ© qua ƒëi·ªán tho·∫°i ƒë√£ t·ª± sinh con t·∫°i...\n",
            "\n",
            "ü§ñ mT5-small:\n",
            "<extra_id_0> .M·ªôt b√© g√°i . <extra_id_1> .\" , d√≤ng tweet cho bi·∫øt .Si√™u m·ªõi nh·∫•t:\n",
            "\n",
            "ü§ñ ViT5:\n",
            "V√†o ng√†y 10/9, m·ªôt s·∫£n ph·ª• ·ªü Miami, Florida, M·ªπ ƒë√£ t·ª± sinh con t·∫°i nh√† do b√£o Irma, g√¢y ra h·∫≠u qu·∫£ nghi√™m tr·ªçng. C√°c b√°c sƒ© t·∫°i b·ªánh vi·ªán Jackson ƒë√£ nhanh ch√≥ng ti·∫øp c·∫≠n v√† h∆∞·ªõng d·∫´n s·∫£n ph·ª• t·ª± c·∫Øt nhau thai. S·ª± vi·ªác nh·∫≠n ƒë∆∞·ª£c s·ª± t√°n d∆∞∆°ng tr√™n m·∫°ng x√£ h·ªôi, th·ªÉ hi·ªán s·ª± quan t√¢m ƒë·∫øn s·ª± an to√†n c·ªßa c·∫£ m·∫π v√† b√©. B√£o Irma l√† c∆°n b√£o m·∫°nh nh·∫•t ƒê·∫°i T√¢y D∆∞∆°ng, g√¢y ng·∫≠p l·ª•t v√† m·∫•t ƒëi·ªán nghi√™m tr·ªçng, g√¢y thi·ªát h·∫°i l·ªõn cho Florida. ƒê·ªÉ ·ª©ng ph√≥, ch√≠nh quy·ªÅn bang Florida ƒë√£ ti·∫øn h√†nh s∆° t√°n l·ªõn, di d·ªùi\n",
            "\n",
            "ü§ñ TextRank (Extractive):\n",
            "Tweet th√¥ng b√°o v·ªÅ tr∆∞·ªùng h·ª£p s·∫£n ph·ª• sinh con t·∫°i nh√† ·ªü Miami do b√£o Irma .M·ªôt ph·ª• n·ªØ ·ªü ·ªü Little Haiti , th√†nh ph·ªë Miami , bang Florida , M·ªπ v·ªõi s·ª± h·ªó tr·ª£ c·ªßa b√°c sƒ© qua ƒëi·ªán tho·∫°i ƒë√£ t·ª± sinh con t·∫°i\n",
            "\n",
            "üìù Reference:\n",
            "V√†o ng√†y 10/9, m·ªôt s·∫£n ph·ª• ·ªü Miami, bang Florida, M·ªπ ƒë√£ sinh con th√†nh c√¥ng t·∫°i nh√† trong b√£o Irma v·ªõi s·ª± h∆∞·ªõng d·∫´n qua ƒëi·ªán tho·∫°i c·ªßa b√°c sƒ© do ƒë·ªôi c·ª©u h·ªô kh√¥ng th·ªÉ ti·∫øp c·∫≠n k·ªãp th·ªùi. B√°c sƒ© ƒë√£ h∆∞·ªõng d·∫´n s·∫£n ph·ª• t·ª± sinh v√† c·∫Øt d√¢y r·ªën cho b√© g√°i s∆° sinh. Sau ƒë√≥, ƒë·ªôi c·ª©u h·ªô ƒë√£ ƒë∆∞a hai m·∫π con ƒë·∫øn b·ªánh vi·ªán trong t√¨nh tr·∫°ng ·ªïn ƒë·ªãnh. S·ª± vi·ªác ƒë∆∞·ª£c c√¥ng ch√∫ng ca ng·ª£i tr√™n m·∫°ng x√£ h·ªôi. B√£o Irma l√† m·ªôt trong nh·ªØng c∆°n b√£o m·∫°nh nh·∫•t l·ªãch s·ª≠, g√¢y thi·ªát h·∫°i n·∫∑ng n·ªÅ t·∫°i khu v·ª±c Caribe v√† Florida.\n",
            "\n",
            "================================================================================\n",
            "EXAMPLE 4\n",
            "================================================================================\n",
            "\n",
            "üìÑ Original Document (408 words):\n",
            "Tr∆∞·ªõc khi u·ªëng th·∫£o d∆∞·ª£c v√† axit amin, b·∫°n c·∫ßn t√¨m hi·ªÉu t√¨nh tr·∫°ng thi·∫øu h·ª•t vitamin ho·∫∑c kho√°ng ch·∫•t. Nguy√™n nh√¢n c√≥ th·ªÉ l√† do ch·∫ø ƒë·ªô ƒÉn u·ªëng thi·∫øu dinh d∆∞·ª°ng (k√©m dinh d∆∞·ª°ng) g√¢y ·∫£nh h∆∞·ªüng ƒë·∫øn t√¢m l...\n",
            "\n",
            "ü§ñ mT5-small:\n",
            "<extra_id_0> t·ªïng h·ª£p c√°c ch·∫•t b·ªï s <extra_id_1>. Vitamin v√† axit amin. Vitamin. Vitamin B. Vitamin C.\n",
            "\n",
            "ü§ñ ViT5:\n",
            "B√†i vi·∫øt n√†y cung c·∫•p th√¥ng tin v·ªÅ vi·ªác s·ª≠ d·ª•ng th·∫£o d∆∞·ª£c v√† axit amin ƒë·ªÉ ƒëi·ªÅu tr·ªã tr·∫ßm c·∫£m. Tr∆∞·ªõc khi d√πng th·∫£o d∆∞·ª£c, c·∫ßn t√¨m hi·ªÉu t√¨nh tr·∫°ng thi·∫øu h·ª•t vitamin ho·∫∑c kho√°ng ch·∫•t, bao g·ªìm vitamin B, C, D, D v√† h·ªón h·ª£p vitamin. C√°c lo·∫°i th·∫£o d∆∞·ª£c nh∆∞ St John's v√† Tryptophan c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng, nh∆∞ng c·∫ßn tham kh·∫£o √Ω ki·∫øn b√°c sƒ© ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n v√† k√™ ƒë∆°n ph√π h·ª£p. Ngo√†i ra, b√†i vi·∫øt c√≤n ƒë·ªÅ c·∫≠p ƒë·∫øn c√°c th·∫£o d∆∞·ª£c b·ªï sung nh∆∞ St. Johns v√† h·∫°t lecithin, c≈©ng nh∆∞ c√°c lo·∫°i th·∫£o m·ªôc kh√°c\n",
            "\n",
            "ü§ñ TextRank (Extractive):\n",
            "Nhi·ªÅu ng∆∞·ªùi h·∫•p th·ª• Vitamin D c·∫ßn thi·∫øt th√¥ng qua √°nh s√°ng m·∫∑t tr·ªùi, gi√∫p c∆° th·ªÉ t·ªïng h·ª£p lo·∫°i vitamin n√†y. C√°c d∆∞·ª°ng ch·∫•t n√†y c√≥ th·ªÉ gi·∫£m cƒÉng th·∫≥ng v√† c·∫£i thi·ªán t√¢m tr·∫°ng. N·∫øu kh√¥ng mu·ªën u·ªëng trypto\n",
            "\n",
            "üìù Reference:\n",
            "ƒê·ªÉ c·∫£i thi·ªán t√¢m tr·∫°ng v√† gi·∫£m cƒÉng th·∫≥ng, vi·ªác t√¨m hi·ªÉu v·ªÅ t√¨nh tr·∫°ng thi·∫øu h·ª•t vitamin v√† kho√°ng ch·∫•t l√† r·∫•t quan tr·ªçng. Ch·∫ø ƒë·ªô ƒÉn u·ªëng thi·∫øu dinh d∆∞·ª°ng c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ƒë·∫øn t√¢m l√Ω do n√£o b·ªô thi·∫øu c√°c d∆∞·ª°ng ch·∫•t c·∫ßn thi·∫øt. Vitamin B, C, v√† D c√≥ vai tr√≤ quan tr·ªçng, ƒë·∫∑c bi·ªát l√† Vitamin D3 gi√∫p c·∫£i thi·ªán tinh th·∫ßn. C√°c lo·∫°i th·∫£o d∆∞·ª£c nh∆∞ St John's Wort v√† r·ªÖ c√¢y h·ªì ti√™u c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng, nh∆∞ng c·∫ßn c√≥ s·ª± t∆∞ v·∫•n. Tryptophan, m·ªôt axit amin quan tr·ªçng, c≈©ng c√≥ th·ªÉ c·∫£i thi·ªán t√¢m tr·∫°ng v√† gi·∫•c ng·ªß, c√≥ th·ªÉ ƒë∆∞·ª£c b·ªï sung qua th·ª±c ph·∫©m ho·∫∑c vi√™n u·ªëng.\n",
            "\n",
            "================================================================================\n",
            "EXAMPLE 5\n",
            "================================================================================\n",
            "\n",
            "üìÑ Original Document (431 words):\n",
            "N·∫•m n√†y c√≥ th·ªÉ l√¢y nhi·ªÖm ·ªü da, mi·ªáng, t√≥c, m√≥ng tr√™n c∆° th·ªÉ ng∆∞·ªùi. H·∫Øc l√†o r·∫•t d·ªÖ l√¢y. Ki·ªÉm tra c√°c d·∫•u hi·ªáu nh∆∞ ph·ªìng r·ªôp, bong tr√≥c ho·∫∑c n·ª©t n·∫ª da. N·∫•m da ch√¢n th∆∞·ªùng k√®m theo c·∫£m gi√°c b·ªèng r√°t. N·∫•m...\n",
            "\n",
            "ü§ñ mT5-small:\n",
            "<extra_id_0> c√≥ th·ªÉ g√¢y b·ªánh n√†y ·ªü ng∆∞·ªùi kh√°c: <extra_id_1>. N·∫•m da ch√¢n <extra_id_2>.. <extra_id_3>..\n",
            "\n",
            "ü§ñ ViT5:\n",
            "B√†i vi·∫øt n√†y cung c·∫•p th√¥ng tin v·ªÅ c√°c lo·∫°i n·∫•m ph·ªï bi·∫øn, bao g·ªìm da, mi·ªáng, t√≥c v√† m√≥ng. C√°c lo·∫°i n·∫•m n√†y c√≥ th·ªÉ l√¢y lan qua nhi·ªÅu con ƒë∆∞·ªùng kh√°c nhau nh∆∞ h·∫Øc l√†o, da ch√¢n, m√≥ng. Ngo√†i ra, b√†i vi·∫øt c√≤n ƒë·ªÅ c·∫≠p ƒë·∫øn c√°c ph∆∞∆°ng ph√°p ƒëi·ªÅu tr·ªã kh√°c nh∆∞ s·ª≠ d·ª•ng gi·∫•m, d·∫ßu tr√†m tr√†, t·ªèi s·ªëng v√† h·ªón h·ª£p t·ªèi. Cu·ªëi c√πng, vi·ªác s·ª≠ d·ª•ng t·ªèi nghi·ªÅn ƒë·ªÉ ƒëi·ªÅu tr·ªã ph√°t ban do n·∫•m l√† c·∫ßn thi·∫øt ƒë·ªÉ h·ªó tr·ª£ qu√° tr√¨nh h·ªìi ph·ª•c.\n",
            "\n",
            "ü§ñ TextRank (Extractive):\n",
            "Thoa d·∫ßu tr√†m tr√† l√™n v√πng da b·ªã nhi·ªÖm n·∫•m 2-3 l·∫ßn m·ªói ng√†y. H·ª£p ch·∫•t n√†y gi√∫p ti√™u di·ªát n·∫•m tr√™n da v√† mau l√†nh v·∫øt th∆∞∆°ng. B·∫°n c√≥ th·ªÉ thoa t·ªèi nghi·ªÅn l√™n v√πng da b·ªã ·∫£nh h∆∞·ªüng 2 l·∫ßn m·ªói ng√†y.\n",
            "\n",
            "üìù Reference:\n",
            "B√†i vi·∫øt n√†y cung c·∫•p th√¥ng tin v·ªÅ b·ªánh nhi·ªÖm n·∫•m tr√™n da, m√≥ng, t√≥c v√† mi·ªáng. C√°c tri·ªáu ch·ª©ng bao g·ªìm ph·ªìng r·ªôp, bong tr√≥c da, v√† bi·∫øn m√†u da, c√πng v·ªõi c·∫£m gi√°c b·ªèng r√°t v√† ƒëau. ƒê·ªÉ ƒëi·ªÅu tr·ªã, c√≥ th·ªÉ s·ª≠ d·ª•ng x√† ph√≤ng s√°t khu·∫©n, d·∫ßu tr√†m tr√† (c·∫ßn th·∫≠n tr·ªçng v·ªõi nam gi·ªõi v·ªã th√†nh ni√™n), gi·∫•m v√† t·ªèi. C√°c bi·ªán ph√°p n√†y gi√∫p kh√°ng n·∫•m v√† s√°t tr√πng, h·ªó tr·ª£ qu√° tr√¨nh ph·ª•c h·ªìi da.\n",
            "\n",
            "‚úÖ Side-by-side comparison complete!\n"
          ]
        }
      ],
      "source": [
        "# Show side-by-side examples\n",
        "print(\"=\"*60)\n",
        "print(\"SIDE-BY-SIDE COMPARISON EXAMPLES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "num_examples = 5\n",
        "\n",
        "for i in range(num_examples):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"EXAMPLE {i+1}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    print(f\"\\nüìÑ Original Document ({len(test_docs_sample[i].split())} words):\")\n",
        "    print(test_docs_sample[i][:200] + \"...\")\n",
        "\n",
        "    print(f\"\\nü§ñ ViT5:\")\n",
        "    print(vit5_predictions[i])\n",
        "\n",
        "    print(f\"\\nü§ñ TextRank (Extractive):\")\n",
        "    print(extractive_predictions[i][:200] if len(extractive_predictions[i]) > 200 else extractive_predictions[i])\n",
        "\n",
        "    print(f\"\\nüìù Reference:\")\n",
        "    print(test_refs_sample[i])\n",
        "\n",
        "print(\"\\n‚úÖ Side-by-side comparison complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2 Application 1: News Article Summarization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "APPLICATION 1: NEWS ARTICLE SUMMARIZATION\n",
            "============================================================\n",
            "\n",
            "üì∞ Original News Article (405 words):\n",
            "B·∫°n h√£y l·∫•y l√Ω do n√†y ƒë·ªÉ th·ªânh tho·∫£ng t·∫Øm l√¢u m·ªôt ch√∫t d∆∞·ªõi v√≤i sen n∆∞·ªõc n√≥ng!  M·ªôt c√°ch kh√°c ƒë·ªÉ s·ª≠ d·ª•ng h∆°i n∆∞·ªõc n√≥ng l√† ƒëun s√¥i m·ªôt n·ªìi n∆∞·ªõc, nh·∫•c ra kh·ªèi b·∫øp, tr√πm khƒÉn t·∫Øm l√™n ƒë·∫ßu v√† h∆° m·∫∑t tr√™n n·ªìi n∆∞·ªõc b·ªëc h∆°i. Nh·ªõ ki·ªÉm tra tr∆∞·ªõc ƒë·ªÉ ƒë·∫£m b·∫£o h∆°i n∆∞·ªõc kh√¥ng qu√° n√≥ng. B·∫°n c≈©ng c√≥ th·ªÉ mua m√°y x√¥ng h∆°i lo·∫°i t∆∞∆°ng ƒë·ªëi r·∫ª ƒë·∫∑t trong ph√≤ng ho·∫∑c c·∫°nh gi∆∞·ªùng ng·ªß. M√°y x√¥ng h∆°i th∆∞·ªùng c√≥ hi·ªáu qu·∫£ h∆°n n∆∞·ªõc s√¥i. Mu·ªëi c√≥ t√°c d·ª•ng ti√™u di·ªát vi tr√πng trong mi·ªáng v√† h·ªçng, ƒë·ªìng th·ªùi gi√∫p gi·∫£m t√¨nh tr·∫°ng kh√¥ v√† k√≠ch ·ª©ng. S√∫c mi·ªáng n∆∞·ªõc mu·ªëi hai l·∫ßn m·ªói ng√†y s·∫Ω gi√∫p l√†m d·ªãu c·ªï h·ªçng kh√¥ r√°t. Pha 1 th√¨a c√† ph√™ mu·ªëi v·ªõi m·ªôt √≠t n∆∞·ªõc n√≥ng, ƒë·ªÉ ngu·ªôi m·ªôt ch√∫t r·ªìi th√™m n∆∞·ªõc m√°t. Nh·ªï n∆∞·ªõc mu·ªëi khi ƒë√£ s√∫c mi·ªáng xong, kh√¥ng nu·ªët. M·ªôt s·ªë ng∆∞·ªùi c≈©ng s√∫c mi·ªáng b·∫±ng dung d·ªãch gi·∫•m t√°o (1 th√¨a canh gi·∫•m t√°o pha v·ªõi m·ªôt c·ªëc n∆∞·ªõc). Tuy h∆∞∆°ng v·ªã c·ªßa gi·∫•m kh√¥ng d·ªÖ ch·ªãu cho l·∫Øm, nh∆∞ng c√°ch n√†y c√≥ th·ªÉ ƒëem l·∫°i hi·ªáu qu·∫£. √çt ra th√¨ m·∫≠t ong c≈©ng c√≥ v·ªã d·ªÖ ch·ªãu h∆°n n∆∞·ªõc mu·ªëi hay gi·∫•m t√°o! Ngo√†i kh·∫£ nƒÉng bao b·ªçc c·ªï h·ªçng v·ªõi k·∫øt c·∫•u ƒë·∫∑c s√°nh, m·∫≠t ong c√≤n c√≥ t√°c d·ª•ng kh√°ng khu·∫©n. Ch·∫≥ng l·∫° g√¨ m√† ong m·∫≠t l·∫°i th√≠ch m·∫≠t ong ƒë·∫øn th·∫ø! Nh·ªØng vi√™n ng·∫≠m ho·∫∑c k·∫πo c·ª©ng hay k·∫πo cao su s·∫Ω k√≠ch th√≠ch s·∫£n xu·∫•t n∆∞·ªõc b·ªçt, qua ƒë√≥ l√†m d·ªãu c·ªï h·ªçng kh√¥. Nh·ªõ ch·ªçn c√°c s·∫£n ph·∫©m kh√¥ng ch·ª©a ƒë∆∞·ªùng ‚Äì h·∫≥n l√† nha sƒ© s·∫Ω c·∫£m ∆°n b·∫°n v√¨ vi·ªác n√†y! H·∫ßu h·∫øt m·ªçi ng∆∞·ªùi ƒë·ªÅu nh·∫≠n th·∫•y r·∫±ng c√°c ch·∫•t l·ªèng ·∫•m ƒë·ªÅu c√≥ t√°c d·ª•ng xoa d·ªãu, v√¨ v·∫≠y c√°c lo·∫°i tr√† c√≥ h√†m l∆∞·ª£ng caffeine th·∫•p, (c√≥ l·∫Ω n√™n th√™m m·∫≠t ong v√† chanh), s·∫Ω l√† l·ª±a ch·ªçn t·ªët ƒë·ªÉ ch·ªØa c·ªï h·ªçng kh√¥. C√°c lo·∫°i tr√† th·∫£o m·ªôc ph·ªï bi·∫øn nh∆∞ tr√† c√∫c La M√£ c√≥ th·ªÉ gi√∫p l√†m d·ªãu c·ªï h·ªçng, nh∆∞ng nhi·ªÅu ng∆∞·ªùi c≈©ng kh·∫≥ng ƒë·ªãnh r·∫±ng c√°c lo·∫°i tr√† t·ª´ nguy√™n li·ªáu nh∆∞ b·∫°c h√† cay, g·ª´ng, ƒëinh h∆∞∆°ng, r·ªÖ cam th·∫£o, r·ªÖ th·ª•c qu·ª≥, c√∫c d·∫°i v√† c√¢y du tr∆°n c≈©ng r·∫•t t·ªët. B·∫°n c≈©ng c√≥ th·ªÉ c√¢n nh·∫Øc cho th√™m m·∫≠t ong ho·∫∑c qu·∫ø v√†o tr√†. .\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "ü§ñ mT5-small Summary:\n",
            "<extra_id_0> s·∫Ω gi√∫p b·∫°n kh√° gi·∫£m kh√° nhi·ªÅu b·ªánh. ƒê·ªçc nh√©!:):):):)\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "ü§ñ ViT5 Summary:\n",
            "B√†i vi·∫øt cung c·∫•p c√°c bi·ªán ph√°p kh·∫Øc ph·ª•c t√¨nh tr·∫°ng c·ªï h·ªçng kh√¥, bao g·ªìm t·∫Øm n∆∞·ªõc n√≥ng, s·ª≠ d·ª•ng m√°y x√¥ng h∆°i, s√∫c mi·ªáng b·∫±ng n∆∞·ªõc mu·ªëi, v√† s·ª≠ d·ª•ng c√°c s·∫£n ph·∫©m kh√¥ng ƒë∆∞·ªùng. Ngo√†i ra, vi·ªác s·ª≠ d·ª•ng mu·ªëi v√† c√°c lo·∫°i tr√† th·∫£o m·ªôc nh∆∞ c√∫c La M√£, b·∫°c h√† cay, g·ª´ng, ƒëinh h∆∞∆°ng, r·ªÖ cam th·∫£o, v√† c√¢y du tr∆°n c≈©ng l√† nh·ªØng l·ª±a ch·ªçn t·ªët. Cu·ªëi c√πng, b√†i vi·∫øt nh·∫•n m·∫°nh t·∫ßm quan tr·ªçng c·ªßa vi·ªác tham kh·∫£o √Ω ki·∫øn b√°c sƒ© tr∆∞·ªõc khi s·ª≠ d·ª•ng b·∫•t k·ª≥ ph∆∞∆°ng ph√°p ƒëi·ªÅu tr·ªã n√†o.\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "ü§ñ TextRank (Extractive) Summary:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccea4c95afb3463ca6afc8e69ed49378",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Computing sentence embeddings:   0%|          | 0/19 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M·ªôt c√°ch kh√°c ƒë·ªÉ s·ª≠ d·ª•ng h∆°i n∆∞·ªõc n√≥ng l√† ƒëun s√¥i m·ªôt n·ªìi n∆∞·ªõc, nh·∫•c ra kh·ªèi b·∫øp, tr√πm khƒÉn t·∫Øm l√™n ƒë·∫ßu v√† h∆° m·∫∑t tr√™n n·ªìi n∆∞·ªõc b·ªëc h∆°i. Mu·ªëi c√≥ t√°c d·ª•ng ti√™u di·ªát vi tr√πng trong mi·ªáng v√† h·ªçng, ƒë·ªìng th·ªùi gi√∫p gi·∫£m t√¨nh tr·∫°ng kh√¥ v√† k√≠ch ·ª©ng. Ngo√†i kh·∫£ nƒÉng bao b·ªçc c·ªï h·ªçng v·ªõi k·∫øt c·∫•u ƒë·∫∑c s√°nh, m·∫≠t ong c√≤n c√≥ t√°c d·ª•ng kh√°ng khu·∫©n.\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìù Reference Summary:\n",
            "B√†i vi·∫øt cung c·∫•p c√°c bi·ªán ph√°p kh·∫Øc ph·ª•c t√¨nh tr·∫°ng kh√¥ c·ªï h·ªçng. C√≥ nhi·ªÅu c√°ch ƒë·ªÉ gi·∫£m t√¨nh tr·∫°ng kh√¥ v√† k√≠ch ·ª©ng, bao g·ªìm vi·ªác s·ª≠ d·ª•ng h∆°i n∆∞·ªõc n√≥ng, s√∫c mi·ªáng b·∫±ng n∆∞·ªõc mu·ªëi ho·∫∑c gi·∫•m t√°o, v√† s·ª≠ d·ª•ng m·∫≠t ong. C√°c s·∫£n ph·∫©m ng·∫≠m ho·∫∑c k·∫πo c·ª©ng c≈©ng c√≥ th·ªÉ gi√∫p √≠ch, c√πng v·ªõi vi·ªác u·ªëng c√°c lo·∫°i tr√† th·∫£o m·ªôc ho·∫∑c tr√† ·∫•m. Vi·ªác k·∫øt h·ª£p c√°c ph∆∞∆°ng ph√°p n√†y c√≥ th·ªÉ gi√∫p l√†m d·ªãu v√† gi·∫£m b·ªõt kh√≥ ch·ªãu do c·ªï h·ªçng kh√¥ g√¢y ra.\n"
          ]
        }
      ],
      "source": [
        "# Example: News Article\n",
        "news_article = dataset['test'][10]['document']\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"APPLICATION 1: NEWS ARTICLE SUMMARIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüì∞ Original News Article ({len(news_article.split())} words):\")\n",
        "print(news_article)\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(\"ü§ñ ViT5 Summary:\")\n",
        "print(generate_summary_vit5(news_article))\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(\"ü§ñ TextRank (Extractive) Summary:\")\n",
        "print(textrank.summarize(news_article, num_sentences=3))\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(\"üìù Reference Summary:\")\n",
        "print(dataset['test'][10]['summary'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.3 Application 2: Long Document Summarization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "APPLICATION 2: LONG DOCUMENT SUMMARIZATION\n",
            "============================================================\n",
            "\n",
            "üìÑ Original Document (411 words):\n",
            "Vi·ªác hi·ªÉu v·ªÅ c√°c nguy√™n nh√¢n g√¢y m√≤n men rƒÉng s·∫Ω gi√∫p b·∫°n ngƒÉn ng·ª´a s√¢u rƒÉng. Ch·∫ø ƒë·ªô ƒÉn nhi·ªÅu tinh b·ªôt v√† ƒë∆∞·ªùng c≈©ng d·∫´n ƒë·∫øn t√¨nh tr·∫°ng m√≤n men rƒÉng. V·ªá sinh rƒÉng mi·ªáng k√©m c√≥ th·ªÉ g√¢y m√≤n men rƒÉng. ƒê√¢y l√† k·∫øt qu·∫£ c·ªßa vi·ªác ng√† rƒÉng l·ªô ra b√™n d∆∞·ªõi l·ªõp men rƒÉng ƒë√£ b·ªã m√≤n. V·∫øt ·ªë nh√¨n th·∫•y r√µ tr√™n b·ªÅ m·∫∑t rƒÉng. Fluoride gi√∫p rƒÉng ch·ªëng l·∫°i a-x√≠t v√† th·∫≠m ch√≠ c√≥ th·ªÉ gi√∫p ƒë·∫£o ng∆∞·ª£c hi·ªán t∆∞·ª£ng s√¢u rƒÉng ·ªü giai ƒëo·∫°n s·ªõm. Nha sƒ© c≈©ng c√≥ th·ªÉ k√™ toa nh·ªØng lo·∫°i kem ƒë√°nh rƒÉng fluoride m·∫°nh h∆°n lo·∫°i m√† b·∫°n v·∫´n mu...\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "ü§ñ mT5-small Summary:\n",
            "<extra_id_0> rƒÉng mi·ªáng. <extra_id_1> c√≥ th·ªÉ gi√∫p b·∫°n. M√£o - C√¥ng ngh·ªá v√† s·ª©c kho·∫ª\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "ü§ñ ViT5 Summary:\n",
            "ƒê·ªÉ ngƒÉn ng·ª´a s√¢u rƒÉng, vi·ªác hi·ªÉu r√µ nguy√™n nh√¢n g√¢y m√≤n men rƒÉng l√† r·∫•t quan tr·ªçng. C√°c nguy√™n nh√¢n bao g·ªìm ch·∫ø ƒë·ªô ƒÉn nhi·ªÅu tinh b·ªôt v√† ƒë∆∞·ªùng, v·ªá sinh rƒÉng mi·ªáng k√©m. Fluoride, kem ƒë√°nh rƒÉng fluoride, m·∫∑t d√°n s·ª©, v√† sealant nha khoa c√≥ th·ªÉ gi√∫p b·∫£o v·ªá men rƒÉng, ngƒÉn ng·ª´a h∆∞ h·∫°i v√† tƒÉng c∆∞·ªùng s·ª©c kh·ªèe rƒÉng mi·ªáng. Ngo√†i ra, vi·ªác b·∫£o v·ªá rƒÉng kh·ªèi axit v√† h∆∞ h·∫°i, c√πng v·ªõi vi·ªác tu√¢n th·ªß h∆∞·ªõng d·∫´n c·ªßa nha sƒ© trong qu√° tr√¨nh ƒëi·ªÅu tr·ªã v√† chƒÉm s√≥c rƒÉng mi·ªáng l√† c·∫ßn thi·∫øt.\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìù Reference Summary:\n",
            "B√†i vi·∫øt n√†y t·∫≠p trung v√†o c√°c bi·ªán ph√°p ngƒÉn ng·ª´a m√≤n men rƒÉng v√† s√¢u rƒÉng. Nguy√™n nh√¢n g√¢y m√≤n men rƒÉng bao g·ªìm ch·∫ø ƒë·ªô ƒÉn nhi·ªÅu ƒë∆∞·ªùng, tinh b·ªôt v√† v·ªá sinh rƒÉng mi·ªáng k√©m. C√°c ph∆∞∆°ng ph√°p ƒëi·ªÅu tr·ªã bao g·ªìm s·ª≠ d·ª•ng fluoride, n∆∞·ªõc s√∫c mi·ªáng theo toa, m√£o rƒÉng, m·∫∑t d√°n s·ª©, tr√°m rƒÉng v√† sealant nha khoa. Nh·ªØng li·ªáu ph√°p n√†y gi√∫p ph·ª•c h·ªìi men rƒÉng, ngƒÉn ng·ª´a s√¢u rƒÉng v√† b·∫£o v·ªá s·ª©c kh·ªèe rƒÉng mi·ªáng.\n"
          ]
        }
      ],
      "source": [
        "# Example: Long Document\n",
        "long_doc = dataset['test'][50]['document']\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"APPLICATION 2: LONG DOCUMENT SUMMARIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìÑ Original Document ({len(long_doc.split())} words):\")\n",
        "print(long_doc[:500] + \"...\")\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(\"ü§ñ ViT5 Summary:\")\n",
        "print(generate_summary_vit5(long_doc, max_length=200))\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(\"üìù Reference Summary:\")\n",
        "print(dataset['test'][50]['summary'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.6 Application 5: Quality Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "APPLICATION 5: QUALITY COMPARISON\n",
            "============================================================\n",
            "\n",
            "üìÑ Original Document:\n",
            "S√°ng ng√†y 3/8 , chia s·∫ª v·ªõi PV b√°o Ng∆∞·ªùi ƒê∆∞a Tin , ƒë·∫°o di·ªÖn Tr·∫ßn V≈© Thu·ª∑ - con r·ªÉ c·ªßa NS∆ØT B√πi C∆∞·ªùng ƒë√£ x√°c nh·∫≠n , ngh·ªá sƒ© B√πi C∆∞·ªùng ƒë√£ qua ƒë·ªùi v√†o l√∫c g·∫ßn 3h s√°ng ng√†y 3/8 t·∫°i b·ªánh vi·ªán Xanh - p√¥n , sau nhi·ªÅu ng√†y ch·ªëng ch·ªçi v·ªõi b·ªánh tai bi·∫øn .NS∆ØT B√πi C∆∞·ªùng sinh nƒÉm 1947 , √¥ng l√† di·ªÖn vi√™n ƒëi·ªán ·∫£n...\n",
            "\n",
            "ROUGE Scores:\n",
            "\n",
            "mT5-small (beam=4):\n",
            "  ROUGE-1: 0.1205, ROUGE-2: 0.0732, ROUGE-L: 0.1084\n",
            "  Summary: <extra_id_0> , .. . .... <extra_id_1> ƒë√£ qua ƒë·ªùi .ƒê∆∞·ª£c bi·∫øt , sau nhi·ªÅu nƒÉm , ....\n",
            "\n",
            "mT5-small (beam=8):\n",
            "  ROUGE-1: 0.1395, ROUGE-2: 0.0706, ROUGE-L: 0.1163\n",
            "  Summary: <extra_id_0> c≈©ng r·∫•t .. :D <extra_id_1> qua ƒë·ªùi: S√°ng ng√†y 3/8 , PV b√°o:\n",
            "\n",
            "ViT5:\n",
            "  ROUGE-1: 0.8544, ROUGE-2: 0.6775, ROUGE-L: 0.5307\n",
            "  Summary: NS∆ØT B√πi C∆∞·ªùng, m·ªôt ngh·ªá sƒ© t√†i nƒÉng, ƒë√£ qua ƒë·ªùi sau th·ªùi gian ch·ªëng ch·ªçi v·ªõi b·ªánh tai bi·∫øn. √îng l√† m·ªôt di·ªÖn vi√™n ƒëi·ªán ·∫£nh n·ªïi ti·∫øng, t·ª´ng ƒë√≥ng nhi·ªÅu vai di·ªÖn quan tr·ªçng, ƒë·∫∑c bi·ªát l√† vai Ch√≠ Ph√®o trong phim 'L√†ng V≈© ƒê·∫°i ng√†y ·∫•y'. S·ª± nghi·ªáp c·ªßa √¥ng kh√¥ng ch·ªâ d·ª´ng l·∫°i ·ªü s·ª± nghi·ªáp m√† c√≤n c√≥ m·ªôt cu·ªôc s·ªëng h√¥n nh√¢n h·∫°nh ph√∫c k√©o d√†i g·∫ßn 50 nƒÉm. B√™n c·∫°nh s·ª± nghi·ªáp di·ªÖn xu·∫•t, √¥ng c√≤n l√† ƒë·∫°o di·ªÖn cho h√†ng ch·ª•c b·ªô phim truy·ªÅn h√¨nh, trong ƒë√≥ c√≥ '√îng t∆∞·ªõng t√¨nh b√°o v√† hai b√† v·ª£'.\n",
            "\n",
            "TextRank:\n",
            "  ROUGE-1: 0.4441, ROUGE-2: 0.3528, ROUGE-L: 0.3577\n",
            "  Summary: S√°ng ng√†y 3/8 , chia s·∫ª v·ªõi PV b√°o Ng∆∞·ªùi ƒê∆∞a Tin , ƒë·∫°o di·ªÖn Tr·∫ßn V≈© Thu·ª∑ - con r·ªÉ c·ªßa NS∆ØT B√πi C∆∞·ªùng ƒë√£ x√°c nh·∫≠n , ngh·ªá sƒ© B√πi C∆∞·ªùng ƒë√£ qua ƒë·ªùi v√†o l√∫c g·∫ßn 3h s√°ng ng√†y 3/8 t·∫°i b·ªánh vi·ªán Xanh - p√¥n , sau nhi·ªÅu ng√†y ch·ªëng ch·ªçi v·ªõi b·ªánh tai bi·∫øn .NS∆ØT B√πi C∆∞·ªùng sinh nƒÉm 1947 , √¥ng l√† di·ªÖn vi√™n ƒëi·ªán ·∫£nh kho√° 2 c·ªßa tr∆∞·ªùng ƒë·∫°i h·ªçc S√¢n kh·∫•u ‚Äì ƒêi·ªán ·∫£nh , c√πng th·ªùi v·ªõi NSND B√πi B√†i B√¨nh , NSND Ph∆∞∆°ng Thanh , NSND Minh Ch√¢u ‚Ä¶Tr√™n con ƒë∆∞·ªùng s·ª± nghi·ªáp c·ªßa m√¨nh , √¥ng t·ª´ng ghi d·∫•u ·∫•n v·ªõi nhi·ªÅu vai di·ªÖn kh√°c nhau t·ª´ vai b·ªô ƒë·ªôi , bi·ªát ƒë·ªông , chi·∫øn sƒ© , .. nh∆∞ng trong s·ªë ƒë√≥ vai di·ªÖn Ch√≠ Ph√®o trong b·ªô phim L√†ng V≈© ƒê·∫°i ng√†y ·∫•y ƒë√£ ƒë·ªÉ l·∫°i trong l√≤ng kh√°n gi·∫£ ·∫•n t∆∞·ª£ng s√¢u s·∫Øc .Vai di·ªÖn n√†y c≈©ng mang l·∫°i cho √¥ng r·∫•t nhi·ªÅu gi·∫£i th∆∞·ªüng danh gi√° .B·ªô phim truy·ªán ƒë·∫ßu tay c·ªßa √¥ng l√† Ng∆∞·ªùi h√πng r√¢u qu·∫∑p s·∫£n xu·∫•t nƒÉm 1990 l√† m·ªôt trong nh·ªØng b·ªô phim ƒÉn kh√°ch trong d√≤ng phim th·ªã tr∆∞·ªùng v√†o l√∫c b·∫•y gi·ªù .Ti·∫øp sau ƒë√≥ kh√¥ng l√¢u nƒÉm 1996 , √¥ng ra m·∫Øt phim truy·ªán nh·ª±a ƒë·∫ßu tay c√≥ t√™n Ng∆∞·ªùi ƒë√†n b√† kh√¥ng con , m·ªôt b·ªô phim t√¢m l√Ω c≈©ng ƒë·ªÉ l·∫°i ·∫•n t∆∞·ª£ng s√¢u s·∫Øc cho kh√°n gi·∫£ truy·ªÅn h√¨nh .NS∆ØT B√πi C∆∞·ªùng ƒë√£ l√†m ƒë·∫°o di·ªÖn cho t·∫ßm 80 b·ªô phim truy·ªÅn h√¨nh , trong ƒë√≥ b·ªô phim √îng t∆∞·ªõng t√¨nh b√°o v√† hai b√† v·ª£ d√†i 29 t·∫≠p ƒë√£ ƒë∆∞·ª£c kh√°n gi·∫£ h√†o h·ª©ng theo d√µi v√† gi√†nh ƒë∆∞·ª£c Huy ch∆∞∆°ng V√†ng t·∫°i Li√™n hoan phim Truy·ªÅn h√¨nh to√†n qu·ªëc .Kh√¥ng ch·ªâ c√≥ m·ªôt s·ª± nghi·ªáp v·∫ª vang , NS∆ØT B√πi C∆∞·ªùng c√≤n khi·∫øn nhi·ªÅu ng∆∞·ªùi ao ∆∞·ªõc khi c√≥ m·ªôt cu·ªôc s·ªëng h√¥n nh√¢n h·∫°nh ph√∫c .G·∫ßn 50 nƒÉm y√™u nhau , 40 nƒÉm n√™n nghƒ©a v·ª£ ch·ªìng , NS∆ØT B√πi C∆∞·ªùng may m·∫Øn v√¨ c√≥ m·ªôt gia ƒë√¨nh l√Ω t∆∞·ªüng , m·ªôt ng∆∞·ªùi v·ª£ ƒë·∫£m ƒëang th√°o v√°t , gi·ªèi kinh doanh v√† c≈©ng kh√¥ng k√©m ph·∫ßn xinh ƒë·∫πp .\n",
            "\n",
            "\n",
            "‚úÖ Quality comparison complete!\n"
          ]
        }
      ],
      "source": [
        "# Compare quality across different approaches\n",
        "comparison_doc = dataset['test'][150]['document']\n",
        "comparison_ref = dataset['test'][150]['summary']\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"APPLICATION 5: QUALITY COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìÑ Original Document:\")\n",
        "print(comparison_doc[:300] + \"...\\n\")\n",
        "\n",
        "# Generate summaries\n",
        "summaries = {\n",
        "    'ViT5': generate_summary_vit5(comparison_doc),\n",
        "    'TextRank': textrank.summarize(comparison_doc, num_sentences=3),\n",
        "    'Reference': comparison_ref\n",
        "}\n",
        "\n",
        "# Compute ROUGE for each\n",
        "print(\"ROUGE Scores:\\n\")\n",
        "for name, summary in summaries.items():\n",
        "    if name != 'Reference':\n",
        "        score = compute_rouge_scores([summary], [comparison_ref])\n",
        "        r1 = np.mean(score['rouge1']['fmeasure'])\n",
        "        r2 = np.mean(score['rouge2']['fmeasure'])\n",
        "        rL = np.mean(score['rougeL']['fmeasure'])\n",
        "        print(f\"{name}:\")\n",
        "        print(f\"  ROUGE-1: {r1:.4f}, ROUGE-2: {r2:.4f}, ROUGE-L: {rL:.4f}\")\n",
        "        print(f\"  Summary: {summary}\")\n",
        "        print()\n",
        "\n",
        "print(\"\\n‚úÖ Quality comparison complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.7 Conclusion\n",
        "\n",
        "### Summary of Findings:\n",
        "\n",
        "1. **Best Overall Performance**: The abstractive models (mT5-small and ViT5) generally outperform the extractive approach in ROUGE scores\n",
        "\n",
        "2. **Model Comparison**:\n",
        "   - **ViT5**: Best for Vietnamese-specific content, more natural summaries\n",
        "   - **mT5-small**: Good multilingual performance, fast inference\n",
        "   - **TextRank**: Fast, reliable, but less fluent summaries\n",
        "\n",
        "3. **Use Case Recommendations**:\n",
        "   - **News**: Use ViT5 or mT5 for natural, concise summaries\n",
        "   - **Technical Documents**: TextRank for factual accuracy\n",
        "   - **Long Documents**: mT5/ViT5 with adjusted length parameters\n",
        "   - **Real-time Applications**: TextRank for speed\n",
        "\n",
        "4. **Key Insights**:\n",
        "   - Beam search (4-8 beams) produces best quality\n",
        "   - Document length impacts performance\n",
        "   - Vietnamese-specific models (ViT5) better capture language nuances\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Fine-tune mT5/ViT5 on your specific domain data\n",
        "- Experiment with different generation parameters\n",
        "- Combine extractive and abstractive approaches\n",
        "- Deploy models with appropriate hardware for production\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Notebook Complete!\n",
        "\n",
        "This comprehensive notebook covered:\n",
        "1. ‚úÖ Theory of text summarization\n",
        "2. ‚úÖ Data loading and exploration\n",
        "3. ‚úÖ Extractive summarization (TextRank)\n",
        "4. ‚úÖ Abstractive summarization (mT5 + ViT5)\n",
        "5. ‚úÖ ROUGE evaluation and comparison\n",
        "6. ‚úÖ 8 comprehensive visualizations\n",
        "7. ‚úÖ Real-world applications\n",
        "\n",
        "Thank you for using this notebook! üéâ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}