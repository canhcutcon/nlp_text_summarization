{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üö® EMERGENCY FIX - MT5 Training Loss = 0\n",
    "\n",
    "## Fixes Applied:\n",
    "1. ‚úÖ Disable FP16 (numerical instability)\n",
    "2. ‚úÖ Explicit model unfreezing\n",
    "3. ‚úÖ Force labels to not be all -100\n",
    "4. ‚úÖ Test EVERY step before training\n",
    "5. ‚úÖ Higher learning rate\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è RUN THIS FIRST: Diagnostic Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31merror: externally-managed-environment\n",
      "\u001b[1;31m\n",
      "\u001b[1;31m√ó This environment is externally managed\n",
      "\u001b[1;31m‚ï∞‚îÄ> To install Python packages system-wide, try brew install\n",
      "\u001b[1;31m    xyz, where xyz is the package you are trying to\n",
      "\u001b[1;31m    install.\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    If you wish to install a Python library that isn't in Homebrew,\n",
      "\u001b[1;31m    use a virtual environment:\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    python3 -m venv path/to/venv\n",
      "\u001b[1;31m    source path/to/venv/bin/activate\n",
      "\u001b[1;31m    python3 -m pip install xyz\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    If you wish to install a Python application that isn't in Homebrew,\n",
      "\u001b[1;31m    it may be easiest to use 'pipx install xyz', which will manage a\n",
      "\u001b[1;31m    virtual environment for you. You can install pipx with\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    brew install pipx\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    You may restore the old behavior of pip by passing\n",
      "\u001b[1;31m    the '--break-system-packages' flag to pip, or by adding\n",
      "\u001b[1;31m    'break-system-packages = true' to your pip.conf file. The latter\n",
      "\u001b[1;31m    will permanently disable this error.\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    If you disable this error, we STRONGLY recommend that you additionally\n",
      "\u001b[1;31m    pass the '--user' flag to pip, or set 'user = true' in your pip.conf\n",
      "\u001b[1;31m    file. Failure to do this can result in a broken Homebrew installation.\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    Read more about this behavior here: <https://peps.python.org/pep-0668/>\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mnote: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;31mhint: See PEP 668 for the detailed specification. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# CH·∫†Y CELL N√ÄY TR∆Ø·ªöC ƒë·ªÉ verify m·ªçi th·ª© OK\n",
    "!python diagnostic_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install & Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets evaluate rouge-score sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# Clear GPU\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "val_df = pd.read_csv(\"data/validation.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "train_df = train_df[['document', 'summary']].dropna()\n",
    "val_df = val_df[['document', 'summary']].dropna()\n",
    "test_df = test_df[['document', 'summary']].dropna()\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df, preserve_index=False),\n",
    "    'validation': Dataset.from_pandas(val_df, preserve_index=False),\n",
    "    'test': Dataset.from_pandas(test_df, preserve_index=False)\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ Data loaded: {len(train_df)} train, {len(val_df)} val, {len(test_df)} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Model - WITH EXPLICIT CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/mt5-small\"\n",
    "print(f\"\\nLoading {MODEL_NAME}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"   Parameters: {model.num_parameters():,}\")\n",
    "print(f\"   Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "# üî• CRITICAL FIX 1: Explicitly unfreeze ALL parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "print(\"‚úÖ All parameters unfrozen\")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "print(f\"‚úÖ Model on {device}\")\n",
    "\n",
    "# üî• CRITICAL CHECK: Verify model can compute loss\n",
    "print(\"\\nüîç Testing forward pass...\")\n",
    "test_input = tokenizer(\"t√≥m t·∫Øt: Test sentence\", return_tensors=\"pt\").to(device)\n",
    "test_label = tokenizer(\"Test output\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_output = model(\n",
    "        input_ids=test_input['input_ids'],\n",
    "        labels=test_label['input_ids']\n",
    "    )\n",
    "    test_loss = test_output.loss.item()\n",
    "\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "\n",
    "if test_loss == 0.0:\n",
    "    print(\"‚ùå‚ùå‚ùå CRITICAL ERROR: Test loss is 0!\")\n",
    "    print(\"Model is broken. DO NOT CONTINUE.\")\n",
    "    raise RuntimeError(\"Model test loss is 0\")\n",
    "elif torch.isnan(torch.tensor(test_loss)):\n",
    "    print(\"‚ùå‚ùå‚ùå CRITICAL ERROR: Test loss is NaN!\")\n",
    "    raise RuntimeError(\"Model test loss is NaN\")\n",
    "else:\n",
    "    print(f\"‚úÖ Test loss is normal: {test_loss:.4f}\")\n",
    "\n",
    "# Test generation\n",
    "print(\"\\nüîç Testing generation...\")\n",
    "with torch.no_grad():\n",
    "    test_gen = model.generate(**test_input, max_length=20)\n",
    "    test_gen_text = tokenizer.decode(test_gen[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated: '{test_gen_text}'\")\n",
    "if len(test_gen_text.strip()) == 0:\n",
    "    print(\"‚ùå WARNING: Generated empty text\")\n",
    "elif '<' in test_gen_text and '>' in test_gen_text:\n",
    "    print(\"‚ùå WARNING: Generated sentinel tokens\")\n",
    "else:\n",
    "    print(\"‚úÖ Generation works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Tokenize - WITH VERIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize with explicit checks\"\"\"\n",
    "    inputs = [\"t√≥m t·∫Øt: \" + doc for doc in examples[\"document\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    # üî• CRITICAL: Use text_target for labels\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"summary\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "# üî• VERIFICATION: Check labels are NOT all -100\n",
    "sample = tokenized_datasets[\"train\"][0]\n",
    "print(f\"\\n‚úÖ Sample tokenized:\")\n",
    "print(f\"   Input length: {len(sample['input_ids'])}\")\n",
    "print(f\"   Label length: {len(sample['labels'])}\")\n",
    "print(f\"   Labels (first 20): {sample['labels'][:20]}\")\n",
    "\n",
    "if all(l == -100 for l in sample['labels']):\n",
    "    print(\"‚ùå‚ùå‚ùå CRITICAL: ALL LABELS ARE -100!\")\n",
    "    raise RuntimeError(\"All labels are -100\")\n",
    "else:\n",
    "    valid_count = sum(1 for l in sample['labels'] if l != -100)\n",
    "    print(f\"‚úÖ Labels OK: {valid_count}/{len(sample['labels'])} valid tokens\")\n",
    "    print(f\"   Decoded: {tokenizer.decode([l for l in sample['labels'][:30] if l != -100])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Handle 3D predictions (logits)\n",
    "    if len(predictions.shape) == 3:\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Debug print\n",
    "    print(f\"\\n[EVAL] Sample prediction: {decoded_preds[0][:100]}\")\n",
    "    print(f\"[EVAL] Sample reference: {decoded_labels[0][:100]}\")\n",
    "    \n",
    "    # Clean\n",
    "    decoded_preds = [\" \".join(pred.strip().split()) for pred in decoded_preds]\n",
    "    decoded_labels = [\" \".join(label.strip().split()) for label in decoded_labels]\n",
    "    \n",
    "    # Check empty\n",
    "    if all(len(p.strip()) == 0 for p in decoded_preds):\n",
    "        print(\"‚ö†Ô∏è  All predictions empty!\")\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0}\n",
    "    \n",
    "    try:\n",
    "        result = rouge.compute(\n",
    "            predictions=decoded_preds,\n",
    "            references=decoded_labels,\n",
    "            use_stemmer=False\n",
    "        )\n",
    "        return {\n",
    "            \"rouge1\": result[\"rouge1\"],\n",
    "            \"rouge2\": result[\"rouge2\"],\n",
    "            \"rougeL\": result[\"rougeL\"],\n",
    "            \"rougeLsum\": result[\"rougeLsum\"],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  ROUGE error: {e}\")\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0}\n",
    "\n",
    "print(\"‚úÖ Metrics defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training Setup - EMERGENCY FIXES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    padding=True,  # Explicit padding\n",
    ")\n",
    "\n",
    "# üî• TEST DATA COLLATOR\n",
    "print(\"\\nüîç Testing data collator...\")\n",
    "test_batch = [tokenized_datasets[\"train\"][i] for i in range(2)]\n",
    "collated = data_collator(test_batch)\n",
    "\n",
    "print(f\"Collated batch:\")\n",
    "print(f\"  Input IDs shape: {collated['input_ids'].shape}\")\n",
    "print(f\"  Labels shape: {collated['labels'].shape}\")\n",
    "\n",
    "# Check labels\n",
    "labels_check = collated['labels'][0]\n",
    "valid_labels = (labels_check != -100).sum().item()\n",
    "total_labels = len(labels_check)\n",
    "print(f\"  Valid labels: {valid_labels}/{total_labels} ({valid_labels/total_labels*100:.1f}%)\")\n",
    "\n",
    "if valid_labels == 0:\n",
    "    print(\"‚ùå‚ùå‚ùå CRITICAL: Data collator produces all -100 labels!\")\n",
    "    raise RuntimeError(\"Data collator broken\")\n",
    "else:\n",
    "    print(\"‚úÖ Data collator OK\")\n",
    "\n",
    "# Training arguments - EMERGENCY MODE\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mt5-emergency-fix\",\n",
    "    \n",
    "    # Batch size\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    \n",
    "    # üî• FIX 2: Higher learning rate\n",
    "    learning_rate=1e-4,  # 10x higher!\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=200,    # Shorter warmup\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Eval\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    \n",
    "    # Generation\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=4,\n",
    "    \n",
    "    # üî• FIX 3: DISABLE FP16 - can cause loss=0 issues\n",
    "    fp16=False,  # Disabled!\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,     # Log every 10 steps\n",
    "    logging_first_step=True,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    \n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Trainer created\")\n",
    "print(f\"\\n‚ö†Ô∏è  EMERGENCY MODE ENABLED:\")\n",
    "print(f\"  - FP16 disabled (avoid numerical issues)\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate} (10x normal)\")\n",
    "print(f\"  - Short warmup: {training_args.warmup_steps} steps\")\n",
    "print(f\"  - Frequent logging: every {training_args.logging_steps} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: FINAL CHECK Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç FINAL PRE-TRAINING CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get a real batch from the dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    batch_size=4,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "# Get first batch\n",
    "first_batch = next(iter(train_dataloader))\n",
    "first_batch = {k: v.to(device) for k, v in first_batch.items()}\n",
    "\n",
    "print(\"\\n1. Batch shape check:\")\n",
    "print(f\"   Input IDs: {first_batch['input_ids'].shape}\")\n",
    "print(f\"   Labels: {first_batch['labels'].shape}\")\n",
    "print(f\"   Attention mask: {first_batch['attention_mask'].shape}\")\n",
    "\n",
    "print(\"\\n2. Labels validity check:\")\n",
    "for i in range(min(2, first_batch['labels'].shape[0])):\n",
    "    labels = first_batch['labels'][i]\n",
    "    valid = (labels != -100).sum().item()\n",
    "    print(f\"   Sample {i}: {valid}/{len(labels)} valid tokens ({valid/len(labels)*100:.1f}%)\")\n",
    "    if valid == 0:\n",
    "        print(f\"      ‚ùå ALL -100!\")\n",
    "\n",
    "print(\"\\n3. Forward pass with real batch:\")\n",
    "model.train()  # Ensure training mode\n",
    "outputs = model(**first_batch)\n",
    "batch_loss = outputs.loss.item()\n",
    "\n",
    "print(f\"   Loss: {batch_loss:.4f}\")\n",
    "print(f\"   Loss requires_grad: {outputs.loss.requires_grad}\")\n",
    "\n",
    "if batch_loss == 0.0:\n",
    "    print(\"\\n‚ùå‚ùå‚ùå CRITICAL ERROR: Loss is 0!\")\n",
    "    print(\"DO NOT START TRAINING!\")\n",
    "    print(\"\\nPossible issues:\")\n",
    "    print(\"- All labels are -100\")\n",
    "    print(\"- Model parameters are frozen\")\n",
    "    print(\"- Incorrect loss computation\")\n",
    "    raise RuntimeError(\"Training loss is 0\")\n",
    "elif torch.isnan(outputs.loss):\n",
    "    print(\"\\n‚ùå‚ùå‚ùå CRITICAL ERROR: Loss is NaN!\")\n",
    "    raise RuntimeError(\"Training loss is NaN\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Loss is normal!\")\n",
    "\n",
    "print(\"\\n4. Backward pass test:\")\n",
    "outputs.loss.backward()\n",
    "print(\"   ‚úÖ Backward pass successful\")\n",
    "\n",
    "# Check gradients\n",
    "grad_norm = 0\n",
    "for p in model.parameters():\n",
    "    if p.grad is not None:\n",
    "        grad_norm += p.grad.norm().item() ** 2\n",
    "grad_norm = grad_norm ** 0.5\n",
    "\n",
    "print(f\"   Gradient norm: {grad_norm:.4f}\")\n",
    "if grad_norm == 0:\n",
    "    print(\"   ‚ùå No gradients!\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Gradients OK\")\n",
    "\n",
    "# Clear gradients\n",
    "model.zero_grad()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ ALL CHECKS PASSED - READY TO TRAIN\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚ö†Ô∏è  WATCH FOR:\")\n",
    "print(\"  - First step loss should be 2-8\")\n",
    "print(\"  - Loss should NOT be 0 or NaN\")\n",
    "print(\"  - Loss should decrease over time\")\n",
    "print(\"  - ROUGE should be > 0 after first eval\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: TRAIN üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Starting training...\")\n",
    "print(\"Expected time: ~1-1.5 hours\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST RESULTS\")\n",
    "print(\"=\"*50)\n",
    "for key, value in results.items():\n",
    "    if 'rouge' in key:\n",
    "        print(f\"{key.upper()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text, max_length=128, num_beams=4):\n",
    "    inputs = tokenizer(\"t√≥m t·∫Øt: \" + text, max_length=512, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_length=max_length, num_beams=num_beams)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test\n",
    "for i in range(3):\n",
    "    test_text = dataset['test'][i]['document']\n",
    "    ground_truth = dataset['test'][i]['summary']\n",
    "    \n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Document: {test_text[:200]}...\")\n",
    "    print(f\"\\nGenerated: {generate_summary(test_text)}\")\n",
    "    print(f\"\\nGround truth: {ground_truth}\")\n",
    "    print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
