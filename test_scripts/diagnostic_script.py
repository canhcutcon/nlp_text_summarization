"""
ğŸ” CHáº¨N ÄOÃN TOÃ€N DIá»†N - MT5 TRAINING BUG
Cháº¡y script nÃ y TRÆ¯á»šC KHI train Ä‘á»ƒ tÃ¬m lá»—i
"""

import torch
import numpy as np
import pandas as pd
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq,
)

print("="*60)
print("ğŸ” Báº®T Äáº¦U CHáº¨N ÄOÃN")
print("="*60)

# ============================================================================
# 1. KIá»‚M TRA THIáº¾T Láº¬P CÆ  Báº¢N
# ============================================================================
print("\nğŸ“‹ BÆ¯á»šC 1: Kiá»ƒm tra thiáº¿t láº­p cÆ¡ báº£n")
print("-"*60)

# Check GPU
if torch.cuda.is_available():
    print(f"âœ… GPU: {torch.cuda.get_device_name(0)}")
    print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
else:
    print("âŒ KHÃ”NG CÃ“ GPU!")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ============================================================================
# 2. LOAD VÃ€ KIá»‚M TRA DATA
# ============================================================================
print("\nğŸ“‹ BÆ¯á»šC 2: Load vÃ  kiá»ƒm tra data")
print("-"*60)

try:
    train_df = pd.read_csv("data/train.csv")
    print(f"âœ… Loaded train.csv: {len(train_df)} rows")
    
    # Check columns
    print(f"   Columns: {train_df.columns.tolist()}")
    
    # Check sample
    sample = train_df.iloc[0]
    print(f"\n   Sample document (first 100 chars):")
    print(f"   '{sample['document'][:100]}'")
    print(f"\n   Sample summary (first 100 chars):")
    print(f"   '{sample['summary'][:100]}'")
    
    # Check for NaN
    if train_df['document'].isna().any():
        print("   âŒ WARNING: NaN values in document column!")
    if train_df['summary'].isna().any():
        print("   âŒ WARNING: NaN values in summary column!")
    
except Exception as e:
    print(f"âŒ ERROR loading data: {e}")
    exit(1)

# ============================================================================
# 3. LOAD VÃ€ KIá»‚M TRA MODEL
# ============================================================================
print("\nğŸ“‹ BÆ¯á»šC 3: Load vÃ  kiá»ƒm tra model")
print("-"*60)

MODEL_NAME = "google/mt5-small"
print(f"Loading model: {MODEL_NAME}")

try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
    
    print(f"âœ… Model loaded successfully")
    print(f"   Model class: {model.__class__.__name__}")
    print(f"   Parameters: {model.num_parameters():,}")
    print(f"   Tokenizer class: {tokenizer.__class__.__name__}")
    print(f"   Vocab size: {tokenizer.vocab_size:,}")
    print(f"   Pad token ID: {tokenizer.pad_token_id}")
    print(f"   EOS token ID: {tokenizer.eos_token_id}")
    
    # Move to GPU
    model = model.to(device)
    print(f"   Model on device: {next(model.parameters()).device}")
    
except Exception as e:
    print(f"âŒ ERROR loading model: {e}")
    exit(1)

# ============================================================================
# 4. KIá»‚M TRA TOKENIZATION
# ============================================================================
print("\nğŸ“‹ BÆ¯á»šC 4: Kiá»ƒm tra tokenization")
print("-"*60)

test_doc = train_df.iloc[0]['document'][:200]
test_sum = train_df.iloc[0]['summary'][:100]

print(f"Test document: '{test_doc}'")
print(f"Test summary: '{test_sum}'")

# Tokenize input
input_encoding = tokenizer(
    "tÃ³m táº¯t: " + test_doc,
    max_length=512,
    truncation=True,
    return_tensors="pt"
)

print(f"\nâœ… Input tokenization:")
print(f"   Input IDs shape: {input_encoding['input_ids'].shape}")
print(f"   Input IDs (first 20): {input_encoding['input_ids'][0][:20].tolist()}")
print(f"   Decoded (first 100 chars): '{tokenizer.decode(input_encoding['input_ids'][0][:100])}'")

# Tokenize target
target_encoding = tokenizer(
    text_target=test_sum,
    max_length=128,
    truncation=True,
    return_tensors="pt"
)

print(f"\nâœ… Target tokenization:")
print(f"   Target IDs shape: {target_encoding['input_ids'].shape}")
print(f"   Target IDs (first 20): {target_encoding['input_ids'][0][:20].tolist()}")
print(f"   Decoded: '{tokenizer.decode(target_encoding['input_ids'][0])}'")

# ============================================================================
# 5. KIá»‚M TRA FORWARD PASS
# ============================================================================
print("\nğŸ“‹ BÆ¯á»šC 5: Kiá»ƒm tra forward pass")
print("-"*60)

# Test forward pass WITHOUT labels (generation mode)
print("Test 1: Forward pass WITHOUT labels (inference)")
with torch.no_grad():
    outputs_no_labels = model(
        input_ids=input_encoding['input_ids'].to(device),
        attention_mask=input_encoding['attention_mask'].to(device)
    )
    print(f"   Logits shape: {outputs_no_labels.logits.shape}")
    print(f"   Logits min/max: {outputs_no_labels.logits.min().item():.4f} / {outputs_no_labels.logits.max().item():.4f}")

# Test forward pass WITH labels (training mode)
print("\nTest 2: Forward pass WITH labels (training)")
with torch.no_grad():
    outputs_with_labels = model(
        input_ids=input_encoding['input_ids'].to(device),
        attention_mask=input_encoding['attention_mask'].to(device),
        labels=target_encoding['input_ids'].to(device)
    )
    
    loss = outputs_with_labels.loss.item()
    print(f"   Loss: {loss:.4f}")
    print(f"   Loss is finite: {torch.isfinite(outputs_with_labels.loss).item()}")
    
    if loss == 0.0:
        print("   âŒâŒâŒ CRITICAL ERROR: Loss is 0! This is WRONG!")
    elif torch.isnan(outputs_with_labels.loss):
        print("   âŒâŒâŒ CRITICAL ERROR: Loss is NaN! This is WRONG!")
    elif loss > 10:
        print("   âš ï¸  WARNING: Loss is very high (>10)")
    elif loss < 0.1:
        print("   âš ï¸  WARNING: Loss is suspiciously low (<0.1)")
    else:
        print("   âœ… Loss looks normal!")

# ============================================================================
# 6. KIá»‚M TRA GENERATION
# ============================================================================
print("\nğŸ“‹ BÆ¯á»šC 6: Kiá»ƒm tra generation")
print("-"*60)

test_input = "Chiá»u 26/1, UBND TP HÃ  Ná»™i tá»• chá»©c há»p bÃ¡o cÃ´ng bá»‘ káº¿t quáº£ phÃ¡t triá»ƒn kinh táº¿."
print(f"Test input: '{test_input}'")

inputs = tokenizer("tÃ³m táº¯t: " + test_input, return_tensors="pt").to(device)

with torch.no_grad():
    # Greedy generation (fast)
    outputs_greedy = model.generate(
        **inputs,
        max_length=50,
        num_beams=1,
        do_sample=False
    )
    
    generated_greedy = tokenizer.decode(outputs_greedy[0], skip_special_tokens=True)
    generated_with_special = tokenizer.decode(outputs_greedy[0], skip_special_tokens=False)
    
    print(f"\nâœ… Greedy generation:")
    print(f"   Output IDs: {outputs_greedy[0].tolist()[:30]}")
    print(f"   With special tokens: '{generated_with_special}'")
    print(f"   Without special tokens: '{generated_greedy}'")
    print(f"   Length: {len(generated_greedy)} chars")
    
    # Check for garbage output
    if len(generated_greedy.strip()) == 0:
        print("   âŒ CRITICAL: Generated text is EMPTY!")
    elif '<' in generated_greedy and '>' in generated_greedy:
        print("   âŒ CRITICAL: Generated text contains sentinel tokens!")
    elif any(c < ' ' for c in generated_greedy if c != '\n'):
        print("   âŒ CRITICAL: Generated text contains control characters!")
    elif all(ord(c) < 128 for c in generated_greedy):
        print("   âš ï¸  WARNING: Generated text is all ASCII (expected Vietnamese)")
    else:
        print("   âœ… Generated text looks reasonable!")

# ============================================================================
# 7. KIá»‚M TRA DATA COLLATOR
# ============================================================================
print("\nğŸ“‹ BÆ¯á»šC 7: Kiá»ƒm tra Data Collator")
print("-"*60)

data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    label_pad_token_id=-100
)

# Create mini dataset
mini_df = train_df.head(2)
mini_dataset = Dataset.from_pandas(mini_df[['document', 'summary']], preserve_index=False)

def preprocess_function(examples):
    inputs = ["tÃ³m táº¯t: " + doc for doc in examples["document"]]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=False)
    labels = tokenizer(text_target=examples["summary"], max_length=128, truncation=True, padding=False)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_mini = mini_dataset.map(preprocess_function, batched=True)

# Test collator
batch = [tokenized_mini[0], tokenized_mini[1]]
collated_batch = data_collator(batch)

print(f"âœ… Data collator output:")
print(f"   Input IDs shape: {collated_batch['input_ids'].shape}")
print(f"   Labels shape: {collated_batch['labels'].shape}")
print(f"   Attention mask shape: {collated_batch['attention_mask'].shape}")

# Check labels
labels_sample = collated_batch['labels'][0]
num_neg100 = (labels_sample == -100).sum().item()
num_valid = (labels_sample != -100).sum().item()

print(f"\n   Labels analysis:")
print(f"   Total tokens: {len(labels_sample)}")
print(f"   Padding (-100): {num_neg100}")
print(f"   Valid tokens: {num_valid}")
print(f"   Valid percentage: {num_valid/len(labels_sample)*100:.1f}%")

if num_valid == 0:
    print("   âŒâŒâŒ CRITICAL: ALL LABELS ARE -100!")
elif num_valid < 5:
    print("   âŒ WARNING: Very few valid labels!")
else:
    print("   âœ… Labels look normal!")

# Decode labels
valid_labels = labels_sample[labels_sample != -100]
if len(valid_labels) > 0:
    decoded_labels = tokenizer.decode(valid_labels)
    print(f"   Decoded labels: '{decoded_labels[:100]}'")

# ============================================================================
# 8. KIá»‚M TRA TRAINING STEP
# ============================================================================
print("\nğŸ“‹ BÆ¯á»šC 8: Kiá»ƒm tra má»™t training step")
print("-"*60)

# Set model to training mode
model.train()
print(f"Model in training mode: {model.training}")

# Move batch to device
collated_batch = {k: v.to(device) for k, v in collated_batch.items()}

# Forward pass
outputs = model(**collated_batch)
loss = outputs.loss

print(f"\nâœ… Training step:")
print(f"   Loss: {loss.item():.4f}")
print(f"   Loss requires_grad: {loss.requires_grad}")
print(f"   Loss is finite: {torch.isfinite(loss).item()}")

if loss.item() == 0.0:
    print("   âŒâŒâŒ CRITICAL: Training loss is 0!")
    
    # Debug why
    print("\n   Debugging zero loss:")
    print(f"   - Labels contain -100: {(collated_batch['labels'] == -100).any().item()}")
    print(f"   - All labels are -100: {(collated_batch['labels'] == -100).all().item()}")
    print(f"   - Input IDs shape: {collated_batch['input_ids'].shape}")
    print(f"   - Labels shape: {collated_batch['labels'].shape}")
    
elif torch.isnan(loss):
    print("   âŒâŒâŒ CRITICAL: Training loss is NaN!")
else:
    print("   âœ… Training loss looks normal!")

# Test backward
loss.backward()
print(f"\nâœ… Backward pass successful")

# Check gradients
has_grad = False
total_params = 0
params_with_grad = 0

for name, param in model.named_parameters():
    total_params += 1
    if param.grad is not None and param.grad.abs().sum() > 0:
        has_grad = True
        params_with_grad += 1

print(f"   Total parameters: {total_params}")
print(f"   Parameters with gradients: {params_with_grad}")

if not has_grad:
    print("   âŒ CRITICAL: NO GRADIENTS COMPUTED!")
elif params_with_grad < total_params * 0.5:
    print("   âš ï¸  WARNING: Less than 50% of parameters have gradients")
else:
    print("   âœ… Gradients look normal!")

# ============================================================================
# 9. FINAL DIAGNOSIS
# ============================================================================
print("\n" + "="*60)
print("ğŸ“Š FINAL DIAGNOSIS")
print("="*60)

issues_found = []

# Check each component
if loss.item() == 0.0:
    issues_found.append("âŒ Training loss is 0 - Model is not learning!")
    issues_found.append("   Possible causes:")
    issues_found.append("   - All labels are -100 (padding)")
    issues_found.append("   - Model weights are frozen")
    issues_found.append("   - Incorrect loss computation")

if torch.isnan(loss):
    issues_found.append("âŒ Training loss is NaN - Numerical instability!")
    issues_found.append("   Possible causes:")
    issues_found.append("   - FP16 precision issues")
    issues_found.append("   - Exploding gradients")
    issues_found.append("   - Invalid input data")

if not has_grad:
    issues_found.append("âŒ No gradients computed - Model cannot learn!")
    issues_found.append("   Possible causes:")
    issues_found.append("   - Model in eval mode")
    issues_found.append("   - Parameters frozen")
    issues_found.append("   - Loss computation error")

if '<' in generated_greedy and '>' in generated_greedy:
    issues_found.append("âŒ Model generates sentinel tokens - Wrong tokenizer/model combo!")
    issues_found.append("   Possible causes:")
    issues_found.append("   - Tokenizer doesn't match model")
    issues_found.append("   - Model not properly initialized")

if num_valid == 0:
    issues_found.append("âŒ All labels are -100 - Data collator error!")
    issues_found.append("   Possible causes:")
    issues_found.append("   - Wrong label_pad_token_id")
    issues_found.append("   - Preprocessing error")

if issues_found:
    print("\nğŸ”´ ISSUES FOUND:")
    for issue in issues_found:
        print(issue)
    print("\nğŸ’¡ Next steps:")
    print("1. Review the issues above")
    print("2. Check the corresponding sections for details")
    print("3. Fix issues before training")
else:
    print("\nâœ… NO CRITICAL ISSUES FOUND!")
    print("Model should train correctly.")
    print("\nğŸ’¡ Proceed with training and monitor:")
    print("- First step loss should be 2-8")
    print("- Loss should decrease over time")
    print("- ROUGE scores should be > 0 after first eval")

print("\n" + "="*60)
print("ğŸ DIAGNOSIS COMPLETE")
print("="*60)
