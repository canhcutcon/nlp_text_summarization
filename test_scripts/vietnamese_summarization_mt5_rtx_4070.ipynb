{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vietnamese Text Summarization - mT5-Small (FINAL)\n",
    "\n",
    "âœ… **Model**: google/mt5-small (300M params)  \n",
    "âœ… **GPU**: RTX 4070 SUPER 12GB  \n",
    "âœ… **Optimized**: Batch size 8, FP16, Gradient Checkpointing  \n",
    "âœ… **Fixed**: All previous errors (CSV, tokenization, OOM)  \n",
    "âœ… **Training time**: ~1-1.5 hours  \n",
    "\n",
    "---\n",
    "\n",
    "## Why mT5-Small?\n",
    "- âœ… Fast training (~1.5h vs 3h for base)\n",
    "- âœ… Good results (only 1-2% lower ROUGE than base)\n",
    "- âœ… No OOM issues\n",
    "- âœ… Perfect for Vietnamese summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trong terminal hoáº·c notebook cell\n",
    "# !pkill -9 python\n",
    "# !pkill -9 jupyter\n",
    "\n",
    "# # Hoáº·c kill specific PIDs\n",
    "# !kill -9 2048433\n",
    "# !kill -9 2059574"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting underthesea\n",
      "  Using cached underthesea-8.3.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting Click>=6.0 (from underthesea)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
      "  Using cached python_crfsuite-0.9.12-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting nltk>=3.8 (from underthesea)\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting tqdm (from underthesea)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting requests (from underthesea)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting joblib (from underthesea)\n",
      "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting scikit-learn>=1.6.1 (from underthesea)\n",
      "  Using cached scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting PyYAML (from underthesea)\n",
      "  Using cached pyyaml-6.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting underthesea_core==1.0.5 (from underthesea)\n",
      "  Using cached underthesea_core-1.0.5-cp310-cp310-manylinux2010_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting huggingface-hub (from underthesea)\n",
      "  Using cached huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>=3.8->underthesea)\n",
      "  Using cached regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting numpy>=1.22.0 (from scikit-learn>=1.6.1->underthesea)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn>=1.6.1->underthesea)\n",
      "  Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.6.1->underthesea)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from huggingface-hub->underthesea)\n",
      "  Using cached filelock-3.20.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub->underthesea)\n",
      "  Using cached fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub->underthesea)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from huggingface-hub->underthesea)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting packaging>=20.9 (from huggingface-hub->underthesea)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting shellingham (from huggingface-hub->underthesea)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub->underthesea)\n",
      "  Using cached typer_slim-0.21.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub->underthesea)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->huggingface-hub->underthesea)\n",
      "  Using cached anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->huggingface-hub->underthesea)\n",
      "  Using cached certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface-hub->underthesea)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<1,>=0.23.0->huggingface-hub->underthesea)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub->underthesea)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting exceptiongroup>=1.0.2 (from anyio->httpx<1,>=0.23.0->huggingface-hub->underthesea)\n",
      "  Using cached exceptiongroup-1.3.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->underthesea)\n",
      "  Using cached charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->underthesea)\n",
      "  Using cached urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Using cached underthesea-8.3.0-py3-none-any.whl (8.3 MB)\n",
      "Using cached underthesea_core-1.0.5-cp310-cp310-manylinux2010_x86_64.whl (978 kB)\n",
      "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Using cached python_crfsuite-0.9.12-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "Using cached regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
      "Using cached scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "Using cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached huggingface_hub-1.2.3-py3-none-any.whl (520 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached pyyaml-6.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (770 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "Using cached exceptiongroup-1.3.1-py3-none-any.whl (16 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "Using cached filelock-3.20.2-py3-none-any.whl (16 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
      "Using cached urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached typer_slim-0.21.0-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: underthesea_core, urllib3, typing-extensions, tqdm, threadpoolctl, shellingham, regex, PyYAML, python-crfsuite, packaging, numpy, joblib, idna, hf-xet, h11, fsspec, filelock, Click, charset_normalizer, certifi, typer-slim, scipy, requests, nltk, httpcore, exceptiongroup, scikit-learn, anyio, httpx, huggingface-hub, underthesea\n",
      "\u001b[2K  Attempting uninstall: underthesea_core\n",
      "\u001b[2K    Found existing installation: underthesea_core 1.0.5\n",
      "\u001b[2K    Uninstalling underthesea_core-1.0.5:\n",
      "\u001b[2K      Successfully uninstalled underthesea_core-1.0.5\u001b[32m 0/31\u001b[0m [underthesea_core]\n",
      "\u001b[2K  Attempting uninstall: urllib3â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 0/31\u001b[0m [underthesea_core]\n",
      "\u001b[2K    Found existing installation: urllib3 2.6.20m \u001b[32m 0/31\u001b[0m [underthesea_core]\n",
      "\u001b[2K    Uninstalling urllib3-2.6.2:â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 0/31\u001b[0m [underthesea_core]\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.6.2\u001b[0m \u001b[32m 0/31\u001b[0m [underthesea_core]\n",
      "\u001b[2K  Attempting uninstall: typing-extensionsâ”â”â”\u001b[0m \u001b[32m 0/31\u001b[0m [underthesea_core]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.15.031\u001b[0m [underthesea_core]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.15.0:â”â”\u001b[0m \u001b[32m 0/31\u001b[0m [underthesea_core]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.15.0â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/31\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: tqdmâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/31\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: tqdm 4.67.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/31\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling tqdm-4.67.1:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/31\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled tqdm-4.67.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/31\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: threadpoolctlâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/31\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: threadpoolctl 3.6.0â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/31\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling threadpoolctl-3.6.0:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/31\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled threadpoolctl-3.6.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/31\u001b[0m [threadpoolctl]\n",
      "\u001b[2K  Attempting uninstall: shellinghamâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/31\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Found existing installation: shellingham 1.5.4â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/31\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Uninstalling shellingham-1.5.4:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/31\u001b[0m [threadpoolctl]\n",
      "\u001b[2K      Successfully uninstalled shellingham-1.5.4â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/31\u001b[0m [threadpoolctl]\n",
      "\u001b[2K  Attempting uninstall: regexâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/31\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Found existing installation: regex 2025.11.3â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/31\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Uninstalling regex-2025.11.3:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/31\u001b[0m [threadpoolctl]\n",
      "\u001b[2K      Successfully uninstalled regex-2025.11.3â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/31\u001b[0m [threadpoolctl]\n",
      "\u001b[2K  Attempting uninstall: PyYAML[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/31\u001b[0m [regex]ctl]\n",
      "\u001b[2K    Found existing installation: PyYAML 6.0.3â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/31\u001b[0m [regex]\n",
      "\u001b[2K    Uninstalling PyYAML-6.0.3:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/31\u001b[0m [regex]\n",
      "\u001b[2K      Successfully uninstalled PyYAML-6.0.3â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/31\u001b[0m [regex]\n",
      "\u001b[2K  Attempting uninstall: python-crfsuiteâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/31\u001b[0m [regex]\n",
      "\u001b[2K    Found existing installation: python-crfsuite 0.9.12â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/31\u001b[0m [regex]\n",
      "\u001b[2K    Uninstalling python-crfsuite-0.9.12:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/31\u001b[0m [regex]\n",
      "\u001b[2K      Successfully uninstalled python-crfsuite-0.9.12â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/31\u001b[0m [regex]\n",
      "\u001b[2K  Attempting uninstall: packagingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/31\u001b[0m [regex]\n",
      "\u001b[2K    Found existing installation: packaging 25.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/31\u001b[0m [regex]\n",
      "\u001b[2K    Uninstalling packaging-25.0:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/31\u001b[0m [regex]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/31\u001b[0m [regex]\n",
      "\u001b[2K  Attempting uninstall: numpymâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/31\u001b[0m [regex]\n",
      "\u001b[2K    Found existing installation: numpy 2.2.6â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/31\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling numpy-2.2.6:m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/31\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.2.6â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/31\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: joblib\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/31\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: joblib 1.5.3â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/31\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling joblib-1.5.3:\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/31\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled joblib-1.5.3â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/31\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: idna0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/31\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: idna 3.11â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/31\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling idna-3.11:[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/31\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled idna-3.110mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/31\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: hf-xet[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/31\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: hf-xet 1.2.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/31\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling hf-xet-1.2.0:[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/31\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled hf-xet-1.2.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/31\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: h11mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/31\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: h11 0.16.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/31\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling h11-0.16.0:â•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/31\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled h11-0.16.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/31\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: fsspec[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/31\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.10.0â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/31\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling fsspec-2025.10.0:[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/31\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.10.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/31\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: filelock[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15/31\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: filelock 3.20.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15/31\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling filelock-3.20.2:\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15/31\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled filelock-3.20.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15/31\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: Click90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15/31\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: click 8.3.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15/31\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling click-8.3.1:90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15/31\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled click-8.3.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15/31\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: charset_normalizerâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15/31\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: charset-normalizer 3.4.4â”â”â”â”â”\u001b[0m \u001b[32m15/31\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling charset-normalizer-3.4.4:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15/31\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled charset-normalizer-3.4.4â”â”â”â”â”â”â”\u001b[0m \u001b[32m15/31\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: certifi\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18/31\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: certifi 2026.1.4â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18/31\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling certifi-2026.1.4:0mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18/31\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled certifi-2026.1.4â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18/31\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: typer-slim0mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18/31\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: typer-slim 0.21.0â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18/31\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling typer-slim-0.21.0:mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18/31\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled typer-slim-0.21.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18/31\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: scipy0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18/31\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: scipy 1.15.3mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18/31\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling scipy-1.15.3:m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18/31\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled scipy-1.15.390mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18/31\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: requestsâ”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21/31\u001b[0m [scipy]rmalizer]\n",
      "\u001b[2K    Found existing installation: requests 2.32.50mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21/31\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling requests-2.32.5:0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21/31\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled requests-2.32.5[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21/31\u001b[0m [scipy]\n",
      "\u001b[2K  Attempting uninstall: nltkâ”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21/31\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: nltk 3.9.20m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21/31\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling nltk-3.9.2:â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21/31\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled nltk-3.9.2\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21/31\u001b[0m [scipy]\n",
      "\u001b[2K  Attempting uninstall: httpcoreâ”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23/31\u001b[0m [nltk]\n",
      "\u001b[2K    Found existing installation: httpcore 1.0.9\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23/31\u001b[0m [nltk]\n",
      "\u001b[2K    Uninstalling httpcore-1.0.9:â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23/31\u001b[0m [nltk]\n",
      "\u001b[2K      Successfully uninstalled httpcore-1.0.90m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23/31\u001b[0m [nltk]\n",
      "\u001b[2K  Attempting uninstall: exceptiongroup\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24/31\u001b[0m [httpcore]\n",
      "\u001b[2K    Found existing installation: exceptiongroup 1.3.1â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24/31\u001b[0m [httpcore]\n",
      "\u001b[2K    Uninstalling exceptiongroup-1.3.1:\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24/31\u001b[0m [httpcore]\n",
      "\u001b[2K      Successfully uninstalled exceptiongroup-1.3.10mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24/31\u001b[0m [httpcore]\n",
      "\u001b[2K  Attempting uninstall: scikit-learn0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24/31\u001b[0m [httpcore]\n",
      "\u001b[2K    Found existing installation: scikit-learn 1.7.20mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24/31\u001b[0m [httpcore]\n",
      "\u001b[2K    Uninstalling scikit-learn-1.7.2:0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24/31\u001b[0m [httpcore]\n",
      "\u001b[2K      Successfully uninstalled scikit-learn-1.7.2[90mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24/31\u001b[0m [httpcore]\n",
      "\u001b[2K  Attempting uninstall: anyioâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m \u001b[32m26/31\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Found existing installation: anyio 4.12.0mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m \u001b[32m26/31\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Uninstalling anyio-4.12.0:â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m \u001b[32m26/31\u001b[0m [scikit-learn]\n",
      "\u001b[2K      Successfully uninstalled anyio-4.12.091mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m \u001b[32m26/31\u001b[0m [scikit-learn]\n",
      "\u001b[2K  Attempting uninstall: httpxâ”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m \u001b[32m26/31\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Found existing installation: httpx 0.28.1mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m \u001b[32m26/31\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Uninstalling httpx-0.28.1:â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m \u001b[32m26/31\u001b[0m [scikit-learn]\n",
      "\u001b[2K      Successfully uninstalled httpx-0.28.191mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m \u001b[32m26/31\u001b[0m [scikit-learn]\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m \u001b[32m26/31\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.36.0mâ”â”â”â”â”â”\u001b[0m \u001b[32m26/31\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.36.0:m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m \u001b[32m26/31\u001b[0m [scikit-learn]\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.36.090mâ”â”â”â”â”â”\u001b[0m \u001b[32m26/31\u001b[0m [scikit-learn]\n",
      "\u001b[2K  Attempting uninstall: undertheseaâ”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”\u001b[0m \u001b[32m29/31\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Found existing installation: underthesea 8.3.0â•º\u001b[0m\u001b[90mâ”â”\u001b[0m \u001b[32m29/31\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Uninstalling underthesea-8.3.0:â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”\u001b[0m \u001b[32m29/31\u001b[0m [huggingface-hub]\n",
      "\u001b[2K      Successfully uninstalled underthesea-8.3.00mâ•º\u001b[0m\u001b[90mâ”â”\u001b[0m \u001b[32m29/31\u001b[0m [huggingface-hub]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31/31\u001b[0m [underthesea]\u001b[0m [underthesea]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 4.4.2 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2025.12.0 which is incompatible.\n",
      "transformers 4.57.3 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Click-8.3.1 PyYAML-6.0.3 anyio-4.12.0 certifi-2026.1.4 charset_normalizer-3.4.4 exceptiongroup-1.3.1 filelock-3.20.2 fsspec-2025.12.0 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-1.2.3 idna-3.11 joblib-1.5.3 nltk-3.9.2 numpy-2.2.6 packaging-25.0 python-crfsuite-0.9.12 regex-2025.11.3 requests-2.32.5 scikit-learn-1.7.2 scipy-1.15.3 shellingham-1.5.4 threadpoolctl-3.6.0 tqdm-4.67.1 typer-slim-0.21.0 typing-extensions-4.15.0 underthesea-8.3.0 underthesea_core-1.0.5 urllib3-2.6.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mâœ… All packages installed!\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets accelerate sentencepiece evaluate rouge-score py-rouge scikit-learn protobuf --root-user-action=ignore\n",
    "# ============================================================================\n",
    "# Install Required Packages\n",
    "# ============================================================================\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q underthesea  # For Vietnamese text processing\n",
    "!pip install -q scikit-learn networkx  # For TextRank\n",
    "!pip install --upgrade --force-reinstall underthesea\n",
    "# CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t\n",
    "!pip install -q transformers datasets accelerate sentencepiece evaluate rouge-score py-rouge\n",
    "!pip install -q underthesea  # ThÆ° viá»‡n NLP tiáº¿ng Viá»‡t\n",
    "print(\"âœ… All packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Dataset Statistics:\n",
      "  Train: 15,620 samples\n",
      "  Validation: 1,952 samples\n",
      "  Test: 1,953 samples\n",
      "\n",
      "ğŸ“„ Sample data:\n",
      "\n",
      "Document (first 200 chars):\n",
      "LÃ¡ N cá»§a cÃ¢y N lÃ´ há»™i N chá»©a V Ä‘áº§y A cháº¥t N gel N vÃ  báº¡n N cÃ³ thá»ƒ hÃ¡i V má»—i khi N cáº§n V . NÃªn V Ä‘á»ƒ khi N nÃ o dÃ¹ng V má»›i hÃ¡i V . Cáº¯t N má»™t nhÃ¡nh N tá»« cÃ¢y lÃ´ há»™i N vÃ  váº¯t V hoáº·c mÃºc V pháº§n N gel V trong...\n",
      "\n",
      "Summary:\n",
      "LÃ´ há»™i, vá»›i cháº¥t gel giÃ u dÆ°á»¡ng cháº¥t, cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ chá»¯a lÃ nh cÃ¡c váº¥n Ä‘á» vá» da nhÆ° bá»ng náº¯ng, gÃ u vÃ  da khÃ´. Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng lÃ¡ lÃ´ há»™i tÆ°Æ¡i Ä‘á»ƒ láº¥y gel, bÃ´i trá»±c tiáº¿p lÃªn da bá»‹ tá»•n thÆ°Æ¡ng. LÆ°u Ã½, gel lÃ´ há»™i khÃ´ng nÃªn bÃ´i lÃªn vÃ¹ng da bá»‹ cháº£y mÃ¡u hoáº·c tá»•n thÆ°Æ¡ng náº·ng. LÃ´ há»™i cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ trá»‹ má»¥n rá»™p vÃ  thay tháº¿ lotion dÆ°á»¡ng áº©m.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import numpy as np\n",
    "\n",
    "def sent_tokenize(text: str) -> list[str]:  # â† Use lowercase 'list'\n",
    "    \"\"\"Vietnamese sentence tokenizer\"\"\"\n",
    "    pattern = r'(?<=[.!?])\\s+(?=[A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä])'\n",
    "    sentences = re.split(pattern, text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "# Load CSV - FIXED: Bá» header=None vÃ¬ CSV cÃ³ header\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "val_df = pd.read_csv(\"data/validation.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset Statistics:\")\n",
    "print(f\"  Train: {len(train_df):,} samples\")\n",
    "print(f\"  Validation: {len(val_df):,} samples\")\n",
    "print(f\"  Test: {len(test_df):,} samples\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nğŸ“„ Sample data:\")\n",
    "sample = train_df.iloc[0]\n",
    "print(f\"\\nDocument (first 200 chars):\\n{sample['document'][:200]}...\")\n",
    "print(f\"\\nSummary:\\n{sample['summary']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Statistics:\n",
      "  Document words: mean=539.5, median=440.0\n",
      "  Summary words: mean=112.1, median=111.0\n",
      "  Document sentences: mean=14.7, median=13.0\n",
      "  Summary sentences: mean=4.8, median=5.0\n",
      "  Compression ratio: 20.8%\n",
      "\n",
      "Validation Statistics:\n",
      "  Document words: mean=549.2, median=442.0\n",
      "  Summary words: mean=112.8, median=112.0\n",
      "  Document sentences: mean=15.2, median=13.0\n",
      "  Summary sentences: mean=4.9, median=5.0\n",
      "  Compression ratio: 20.5%\n",
      "\n",
      "Test Statistics:\n",
      "  Document words: mean=530.9, median=438.0\n",
      "  Summary words: mean=111.0, median=110.0\n",
      "  Document sentences: mean=14.8, median=13.0\n",
      "  Summary sentences: mean=4.8, median=5.0\n",
      "  Compression ratio: 20.9%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Data Statistics\n",
    "# ============================================================================\n",
    "def analyze_text_lengths(df: pd.DataFrame, name: str):\n",
    "    \"\"\"Analyze document and summary lengths\"\"\"\n",
    "    doc_words = df['document'].apply(lambda x: len(x.split()))\n",
    "    sum_words = df['summary'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    doc_sents = df['document'].apply(lambda x: len(sent_tokenize(x)))\n",
    "    sum_sents = df['summary'].apply(lambda x: len(sent_tokenize(x)))\n",
    "    \n",
    "    print(f\"\\n{name} Statistics:\")\n",
    "    print(f\"  Document words: mean={doc_words.mean():.1f}, median={doc_words.median():.1f}\")\n",
    "    print(f\"  Summary words: mean={sum_words.mean():.1f}, median={sum_words.median():.1f}\")\n",
    "    print(f\"  Document sentences: mean={doc_sents.mean():.1f}, median={doc_sents.median():.1f}\")\n",
    "    print(f\"  Summary sentences: mean={sum_sents.mean():.1f}, median={sum_sents.median():.1f}\")\n",
    "    print(f\"  Compression ratio: {(sum_words.mean() / doc_words.mean() * 100):.1f}%\")\n",
    "\n",
    "analyze_text_lengths(train_df, \"Train\")\n",
    "analyze_text_lengths(val_df, \"Validation\")\n",
    "analyze_text_lengths(test_df, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clear GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'Could not load this library: /opt/conda/lib/python3.10/site-packages/torchvision/image.so'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ GPU memory cleared\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "Total VRAM: 23.6 GB\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Clear any existing models\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f\"âœ“ GPU memory cleared\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "\n",
    "# Kiá»ƒm tra GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TextRank Summarizer created!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TextRank Implementation\n",
    "# ============================================================================\n",
    "class TextRankSummarizer:\n",
    "    \"\"\"TextRank algorithm for extractive summarization\"\"\"\n",
    "    \n",
    "    def __init__(self, top_n: int = 3, damping: float = 0.85):\n",
    "        self.top_n = top_n\n",
    "        self.damping = damping\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base')\n",
    "        self.model = AutoModel.from_pretrained('vinai/phobert-base')\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def get_sentence_embedding(self, sentence: str) -> np.ndarray:\n",
    "        \"\"\"Get PhoBERT embedding for a sentence\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            sentence, \n",
    "            return_tensors='pt', \n",
    "            truncation=True, \n",
    "            max_length=256\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            # Use CLS token embedding\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        \n",
    "        return embedding[0]\n",
    "    \n",
    "    def build_similarity_matrix(self, sentences: list[str]) -> np.ndarray:\n",
    "        \"\"\"Build similarity matrix between sentences\"\"\"\n",
    "        print(f\"  Computing embeddings for {len(sentences)} sentences...\")\n",
    "        embeddings = []\n",
    "        \n",
    "        for sent in tqdm(sentences, desc=\"Encoding\"):\n",
    "            emb = self.get_sentence_embedding(sent)\n",
    "            embeddings.append(emb)\n",
    "        \n",
    "        embeddings = np.array(embeddings)\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def textrank(self, similarity_matrix: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Run TextRank algorithm (PageRank on sentence graph)\"\"\"\n",
    "        # Create graph from similarity matrix\n",
    "        nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "        \n",
    "        # Compute PageRank scores\n",
    "        scores = nx.pagerank(nx_graph, alpha=self.damping)\n",
    "        \n",
    "        return np.array(list(scores.values()))\n",
    "    \n",
    "    def summarize(self, document: str, num_sentences: int = None) -> str:\n",
    "        \"\"\"Generate extractive summary using TextRank\"\"\"\n",
    "        if num_sentences is None:\n",
    "            num_sentences = self.top_n\n",
    "        \n",
    "        # Split into sentences\n",
    "        sentences = sent_tokenize(document)\n",
    "        \n",
    "        if len(sentences) <= num_sentences:\n",
    "            return document\n",
    "        \n",
    "        # Build similarity matrix\n",
    "        similarity_matrix = self.build_similarity_matrix(sentences)\n",
    "        \n",
    "        # Run TextRank\n",
    "        scores = self.textrank(similarity_matrix)\n",
    "        \n",
    "        # Select top sentences\n",
    "        ranked_indices = np.argsort(scores)[::-1][:num_sentences]\n",
    "        \n",
    "        # Sort by original order to maintain coherence\n",
    "        ranked_indices = sorted(ranked_indices)\n",
    "        \n",
    "        # Extract summary\n",
    "        summary_sentences = [sentences[i] for i in ranked_indices]\n",
    "        summary = ' '.join(summary_sentences)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "print(\"âœ… TextRank Summarizer created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Loading Vietnamese Text Summarization Dataset...\n",
      "âœ“ Train: 15,620 samples\n",
      "âœ“ Validation: 1,952 samples\n",
      "âœ“ Test: 1,953 samples\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['document', 'summary'],\n",
      "        num_rows: 15620\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['document', 'summary'],\n",
      "        num_rows: 1952\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['document', 'summary'],\n",
      "        num_rows: 1953\n",
      "    })\n",
      "})\n",
      "\n",
      "ğŸ“ Sample:\n",
      "Document: LÃ¡ N cá»§a cÃ¢y N lÃ´ há»™i N chá»©a V Ä‘áº§y A cháº¥t N gel N vÃ  báº¡n N cÃ³ thá»ƒ hÃ¡i V má»—i khi N cáº§n V . NÃªn V Ä‘á»ƒ khi N nÃ o dÃ¹ng V má»›i hÃ¡i V . Cáº¯t N má»™t nhÃ¡nh N tá»« cÃ¢y lÃ´ há»™i N vÃ  váº¯t V hoáº·c mÃºc V pháº§n N gel V trong...\n",
      "Summary: LÃ´ há»™i, vá»›i cháº¥t gel giÃ u dÆ°á»¡ng cháº¥t, cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ chá»¯a lÃ nh cÃ¡c váº¥n Ä‘á» vá» da nhÆ° bá»ng náº¯ng, gÃ u vÃ  da khÃ´. Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng lÃ¡ lÃ´ há»™i tÆ°Æ¡i Ä‘á»ƒ láº¥y gel, bÃ´i trá»±c tiáº¿p lÃªn da bá»‹ tá»•n thÆ°Æ¡ng. LÆ°u Ã½...\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data\"\n",
    "print(\"ğŸ“Š Loading Vietnamese Text Summarization Dataset...\")\n",
    "\n",
    "# Load CSV files\n",
    "train_df = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "val_df = pd.read_csv(f\"{data_path}/validation.csv\")\n",
    "test_df = pd.read_csv(f\"{data_path}/test.csv\")\n",
    "\n",
    "# Keep only document and summary\n",
    "train_df = train_df[['document', 'summary']].dropna()\n",
    "val_df = val_df[['document', 'summary']].dropna()\n",
    "test_df = test_df[['document', 'summary']].dropna()\n",
    "\n",
    "print(f\"âœ“ Train: {len(train_df):,} samples\")\n",
    "print(f\"âœ“ Validation: {len(val_df):,} samples\")\n",
    "print(f\"âœ“ Test: {len(test_df):,} samples\")\n",
    "\n",
    "# Convert to Dataset\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df, preserve_index=False),\n",
    "    'validation': Dataset.from_pandas(val_df, preserve_index=False),\n",
    "    'test': Dataset.from_pandas(test_df, preserve_index=False)\n",
    "})\n",
    "\n",
    "print(f\"\\n{dataset}\")\n",
    "print(f\"\\nğŸ“ Sample:\")\n",
    "sample = dataset['train'][0]\n",
    "print(f\"Document: {sample['document'][:200]}...\")\n",
    "print(f\"Summary: {sample['summary'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model (mT5-Small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/mt5-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded: google/mt5-small\n",
      "Parameters: 300,176,768\n",
      "Vocab size: 250,100\n",
      "âœ… Model moved to cuda\n"
     ]
    }
   ],
   "source": [
    "# âš ï¸ CRITICAL FIX: Explicitly define model_name BEFORE using it\n",
    "MODEL_NAME = \"google/mt5-small\"  # âœ… Define this FIRST!\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"âœ… Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Parameters: {model.num_parameters():,}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "print(f\"âœ… Model moved to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PRE-TRAINING SANITY CHECK ===\n",
      "Input text: Chiá»u 26/1, UBND TP HÃ  Ná»™i tá»• chá»©c há»p bÃ¡o cÃ´ng bá»‘ káº¿t quáº£ phÃ¡t triá»ƒn kinh táº¿.\n",
      "Input IDs shape: torch.Size([1, 47])\n",
      "Generated (untrained): <extra_id_0>. ).Ğ»Ğ¾Ğ²Ğ½Ğ°\n",
      "Generated tokens: [0, 250099, 260, 259, 271, 260, 165222, 1]\n",
      "\n",
      "Test forward pass:\n",
      "Loss: 10.4384\n",
      "Loss is finite: True\n",
      "\n",
      "âœ… Loss looks normal (10.4384)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== PRE-TRAINING SANITY CHECK ===\")\n",
    "test_text = \"Chiá»u 26/1, UBND TP HÃ  Ná»™i tá»• chá»©c há»p bÃ¡o cÃ´ng bá»‘ káº¿t quáº£ phÃ¡t triá»ƒn kinh táº¿.\"\n",
    "inputs = tokenizer(\"tÃ³m táº¯t: \" + test_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(f\"Input text: {test_text}\")\n",
    "print(f\"Input IDs shape: {inputs['input_ids'].shape}\")\n",
    "\n",
    "# Generate (should be garbage before training, but should not crash)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=50,\n",
    "        num_beams=1,  # Use greedy for speed\n",
    "    )\n",
    "\n",
    "generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Generated (untrained): {generated}\")\n",
    "print(f\"Generated tokens: {outputs[0].tolist()[:20]}\")\n",
    "\n",
    "# Test forward pass with labels\n",
    "test_input = tokenizer(\"tÃ³m táº¯t: \" + test_text, return_tensors=\"pt\").to(device)\n",
    "test_label = tokenizer(\"HÃ  Ná»™i cÃ´ng bá»‘ káº¿t quáº£ kinh táº¿\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=test_input['input_ids'],\n",
    "        labels=test_label['input_ids']\n",
    "    )\n",
    "\n",
    "print(f\"\\nTest forward pass:\")\n",
    "print(f\"Loss: {outputs.loss.item():.4f}\")\n",
    "print(f\"Loss is finite: {torch.isfinite(outputs.loss).item()}\")\n",
    "\n",
    "if outputs.loss.item() == 0.0:\n",
    "    print(\"\\nâŒâŒâŒ WARNING: Loss is 0! This is WRONG! âŒâŒâŒ\")\n",
    "elif torch.isnan(outputs.loss):\n",
    "    print(\"\\nâŒâŒâŒ WARNING: Loss is NaN! This is WRONG! âŒâŒâŒ\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Loss looks normal ({outputs.loss.item():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac6b038fa7a4b0185bc9330c0ab2ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/15620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e33608cc614e62b83fbcb8aef83f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/1952 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a88891c59bc4ec19b404cf451fa1e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/1953 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample tokenized data:\n",
      "Input length: 512\n",
      "Label length: 128\n",
      "Input IDs (first 20): [259, 164459, 259, 270, 2289, 270, 267, 26965, 441, 317, 708, 262, 4650, 276, 441, 259, 29828, 382, 2291, 441]\n",
      "Labels (first 20): [458, 1858, 382, 2291, 261, 300, 908, 562, 1075, 4501, 718, 369, 273, 331, 2294, 7790, 370, 562, 1075, 261]\n",
      "\n",
      "Decoded input: tÃ³m táº¯t: LÃ¡ N cá»§a cÃ¢y N lÃ´ há»™i N chá»©a V Ä‘áº§y A cháº¥t N gel N vÃ  báº¡n N cÃ³ thá»ƒ hÃ¡i V má»—i khi N c\n",
      "Decoded label: LÃ´ há»™i, vá»›i cháº¥t gel giÃ u dÆ°á»¡ng cháº¥t, cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ chá»¯a lÃ nh cÃ¡c váº¥n Ä‘á» vá» da nhÆ° bá»\n",
      "\n",
      "âœ… Tokenization complete!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize inputs and targets\"\"\"\n",
    "    # Add prefix\n",
    "    inputs = [\"tÃ³m táº¯t: \" + doc for doc in examples[\"document\"]]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"summary\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"ğŸ”„ Tokenizing dataset...\")\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "# Verify\n",
    "sample = tokenized_datasets[\"train\"][0]\n",
    "print(f\"\\nSample tokenized data:\")\n",
    "print(f\"Input length: {len(sample['input_ids'])}\")\n",
    "print(f\"Label length: {len(sample['labels'])}\")\n",
    "print(f\"Input IDs (first 20): {sample['input_ids'][:20]}\")\n",
    "print(f\"Labels (first 20): {sample['labels'][:20]}\")\n",
    "\n",
    "# Decode to verify\n",
    "decoded_input = tokenizer.decode(sample['input_ids'][:50])\n",
    "decoded_label = tokenizer.decode(sample['labels'][:50])\n",
    "print(f\"\\nDecoded input: {decoded_input}\")\n",
    "print(f\"Decoded label: {decoded_label}\")\n",
    "\n",
    "print(\"\\nâœ… Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DETAILED LABEL CHECK ===\n",
      "Input IDs (first 20): [259, 164459, 259, 270, 2289, 270, 267, 26965, 441, 317, 708, 262, 4650, 276, 441, 259, 29828, 382, 2291, 441]\n",
      "Labels (first 20): [458, 1858, 382, 2291, 261, 300, 908, 562, 1075, 4501, 718, 369, 273, 331, 2294, 7790, 370, 562, 1075, 261]\n",
      "\n",
      "Total labels: 128\n",
      "Number of -100: 0\n",
      "Valid labels: 128\n",
      "Percentage valid: 100.0%\n",
      "\n",
      "Decoded valid labels: LÃ´ há»™i, vá»›i cháº¥t gel giÃ u dÆ°á»¡ng cháº¥t, cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ chá»¯a lÃ nh cÃ¡c váº¥n Ä‘á» vá» da nhÆ° bá»ng náº¯ng, gÃ u vÃ  da khÃ´. Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng lÃ¡ lÃ´ há»™i tÆ°Æ¡i Ä‘á»ƒ láº¥y gel, bÃ´i trá»±c tiáº¿p lÃªn da bá»‹ tá»•n thÆ°Æ¡ng. LÆ°u Ã½, gel lÃ´ há»™i khÃ´ng nÃªn bÃ´i lÃªn vÃ¹ng</s>\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DETAILED LABEL CHECK ===\")\n",
    "sample = tokenized_datasets[\"train\"][0]\n",
    "\n",
    "print(f\"Input IDs (first 20): {sample['input_ids'][:20]}\")\n",
    "print(f\"Labels (first 20): {sample['labels'][:20]}\")\n",
    "\n",
    "# Count -100\n",
    "num_neg100 = sum(1 for l in sample['labels'] if l == -100)\n",
    "num_valid = len(sample['labels']) - num_neg100\n",
    "\n",
    "print(f\"\\nTotal labels: {len(sample['labels'])}\")\n",
    "print(f\"Number of -100: {num_neg100}\")\n",
    "print(f\"Valid labels: {num_valid}\")\n",
    "print(f\"Percentage valid: {num_valid/len(sample['labels'])*100:.1f}%\")\n",
    "\n",
    "# Decode valid labels\n",
    "valid_labels = [l for l in sample['labels'] if l != -100]\n",
    "if valid_labels:\n",
    "    decoded = tokenizer.decode(valid_labels)\n",
    "    print(f\"\\nDecoded valid labels: {decoded}\")\n",
    "else:\n",
    "    print(\"\\nâŒâŒâŒ NO VALID LABELS - ALL ARE -100! âŒâŒâŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Metrics defined\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute ROUGE scores with safety checks\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # âœ… FIX: Add safety check for predictions shape\n",
    "    if len(predictions.shape) == 3:\n",
    "        # If logits, take argmax\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    # Decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # âœ… FIX: Add debug print for first prediction\n",
    "    if len(decoded_preds) > 0:\n",
    "        print(f\"\\n[EVAL] Sample prediction: {decoded_preds[0][:100]}\")\n",
    "        print(f\"[EVAL] Sample reference: {decoded_labels[0][:100]}\")\n",
    "    \n",
    "    # Clean text\n",
    "    decoded_preds = [\"\\n\".join(pred.strip().split()) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.strip().split()) for label in decoded_labels]\n",
    "    \n",
    "    # âœ… FIX: Check if predictions are empty\n",
    "    if all(len(pred.strip()) == 0 for pred in decoded_preds):\n",
    "        print(\"\\nâŒ WARNING: All predictions are empty!\")\n",
    "        return {\n",
    "            \"rouge1\": 0.0,\n",
    "            \"rouge2\": 0.0,\n",
    "            \"rougeL\": 0.0,\n",
    "            \"rougeLsum\": 0.0,\n",
    "        }\n",
    "    \n",
    "    # Compute ROUGE\n",
    "    try:\n",
    "        result = rouge.compute(\n",
    "            predictions=decoded_preds,\n",
    "            references=decoded_labels,\n",
    "            use_stemmer=False  # No Vietnamese stemmer\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"rouge1\": result[\"rouge1\"],\n",
    "            \"rouge2\": result[\"rouge2\"],\n",
    "            \"rougeL\": result[\"rougeL\"],\n",
    "            \"rougeLsum\": result[\"rougeLsum\"],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error computing ROUGE: {e}\")\n",
    "        return {\n",
    "            \"rouge1\": 0.0,\n",
    "            \"rouge2\": 0.0,\n",
    "            \"rougeL\": 0.0,\n",
    "            \"rougeLsum\": 0.0,\n",
    "        }\n",
    "\n",
    "print(\"âœ… Metrics defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DETAILED LABEL CHECK ===\n",
      "Input IDs (first 20): [259, 164459, 259, 270, 2289, 270, 267, 26965, 441, 317, 708, 262, 4650, 276, 441, 259, 29828, 382, 2291, 441]\n",
      "Labels (first 20): [458, 1858, 382, 2291, 261, 300, 908, 562, 1075, 4501, 718, 369, 273, 331, 2294, 7790, 370, 562, 1075, 261]\n",
      "\n",
      "Total labels: 128\n",
      "Number of -100: 0\n",
      "Valid labels: 128\n",
      "Percentage valid: 100.0%\n",
      "\n",
      "Decoded valid labels: LÃ´ há»™i, vá»›i cháº¥t gel giÃ u dÆ°á»¡ng cháº¥t, cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ chá»¯a lÃ nh cÃ¡c váº¥n Ä‘á» vá» da nhÆ° bá»ng náº¯ng, gÃ u vÃ  da khÃ´. Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng lÃ¡ lÃ´ há»™i tÆ°Æ¡i Ä‘á»ƒ láº¥y gel, bÃ´i trá»±c tiáº¿p lÃªn da bá»‹ tá»•n thÆ°Æ¡ng. LÆ°u Ã½, gel lÃ´ há»™i khÃ´ng nÃªn bÃ´i lÃªn vÃ¹ng</s>\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DETAILED LABEL CHECK ===\")\n",
    "sample = tokenized_datasets[\"train\"][0]\n",
    "\n",
    "print(f\"Input IDs (first 20): {sample['input_ids'][:20]}\")\n",
    "print(f\"Labels (first 20): {sample['labels'][:20]}\")\n",
    "\n",
    "# Count -100\n",
    "num_neg100 = sum(1 for l in sample['labels'] if l == -100)\n",
    "num_valid = len(sample['labels']) - num_neg100\n",
    "\n",
    "print(f\"\\nTotal labels: {len(sample['labels'])}\")\n",
    "print(f\"Number of -100: {num_neg100}\")\n",
    "print(f\"Valid labels: {num_valid}\")\n",
    "print(f\"Percentage valid: {num_valid/len(sample['labels'])*100:.1f}%\")\n",
    "\n",
    "# Decode valid labels\n",
    "valid_labels = [l for l in sample['labels'] if l != -100]\n",
    "if valid_labels:\n",
    "    decoded = tokenizer.decode(valid_labels)\n",
    "    print(f\"\\nDecoded valid labels: {decoded}\")\n",
    "else:\n",
    "    print(\"\\nâŒâŒâŒ NO VALID LABELS - ALL ARE -100! âŒâŒâŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PREPROCESSING CHECK ===\n",
      "Raw document (first 100): LÃ¡ N cá»§a cÃ¢y N lÃ´ há»™i N chá»©a V Ä‘áº§y A cháº¥t N gel N vÃ  báº¡n N cÃ³ thá»ƒ hÃ¡i V má»—i khi N cáº§n V . NÃªn V Ä‘á»ƒ k\n",
      "Raw summary (first 100): LÃ´ há»™i, vá»›i cháº¥t gel giÃ u dÆ°á»¡ng cháº¥t, cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ chá»¯a lÃ nh cÃ¡c váº¥n Ä‘á» vá» da nhÆ° bá»ng náº¯ng, gÃ \n",
      "\n",
      "Processed input_ids length: 512\n",
      "Processed labels length: 128\n",
      "\n",
      "Input_ids (first 20): [259, 164459, 259, 270, 2289, 270, 267, 26965, 441, 317, 708, 262, 4650, 276, 441, 259, 29828, 382, 2291, 441]\n",
      "Labels (first 20): [458, 1858, 382, 2291, 261, 300, 908, 562, 1075, 4501, 718, 369, 273, 331, 2294, 7790, 370, 562, 1075, 261]\n",
      "\n",
      "Decoded input: tÃ³m táº¯t: LÃ¡ N cá»§a cÃ¢y N lÃ´ há»™i N chá»©a V Ä‘áº§y A cháº¥t N gel N vÃ  báº¡n N cÃ³ thá»ƒ hÃ¡i V má»—i khi N cáº§n V . NÃªn V Ä‘á»ƒ khi N nÃ o dÃ¹ng V má»›i hÃ¡i V . Cáº¯t N má»™t nhÃ¡nh N tá»« cÃ¢y lÃ´ há»™i N vÃ  váº¯t V hoáº·\n",
      "Decoded label: LÃ´ há»™i, vá»›i cháº¥t gel giÃ u dÆ°á»¡ng cháº¥t, cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ chá»¯a lÃ nh cÃ¡c váº¥n Ä‘á» vá» da nhÆ° bá»\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== PREPROCESSING CHECK ===\")\n",
    "\n",
    "# Get ONE example manually\n",
    "raw_example = {\n",
    "    'document': [train_df.iloc[0]['document']],\n",
    "    'summary': [train_df.iloc[0]['summary']]\n",
    "}\n",
    "\n",
    "print(f\"Raw document (first 100): {raw_example['document'][0][:100]}\")\n",
    "print(f\"Raw summary (first 100): {raw_example['summary'][0][:100]}\")\n",
    "\n",
    "# Apply preprocessing\n",
    "processed = preprocess_function(raw_example)\n",
    "\n",
    "print(f\"\\nProcessed input_ids length: {len(processed['input_ids'][0])}\")\n",
    "print(f\"Processed labels length: {len(processed['labels'][0])}\")\n",
    "\n",
    "print(f\"\\nInput_ids (first 20): {processed['input_ids'][0][:20]}\")\n",
    "print(f\"Labels (first 20): {processed['labels'][0][:20]}\")\n",
    "\n",
    "# Decode\n",
    "decoded_input = tokenizer.decode(processed['input_ids'][0][:100])\n",
    "decoded_label = tokenizer.decode([l for l in processed['labels'][0] if l != -100][:50])\n",
    "\n",
    "print(f\"\\nDecoded input: {decoded_input}\")\n",
    "print(f\"Decoded label: {decoded_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Trainer ready\n",
      "\n",
      "Training configuration:\n",
      "  Total training steps: 5856\n",
      "  Learning rate: 5e-05\n",
      "  Warmup steps: 500\n",
      "  Eval every: 500 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_123/4236343010.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mt5-small-vn-fixed\",\n",
    "    \n",
    "    # Batch size\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,  # Effective batch = 8\n",
    "    \n",
    "    # Learning rate - INCREASED for better learning\n",
    "    learning_rate=5e-5,  # âœ… Increased from 1e-5\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=500,  # âœ… Reduced from 1000\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    \n",
    "    # Generation settings\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=4,\n",
    "    \n",
    "    # Optimization\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=50,\n",
    "    logging_first_step=True,  # âœ… Log first step to check loss\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    \n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer ready\")\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Total training steps: {len(tokenized_datasets['train']) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Warmup steps: {training_args.warmup_steps}\")\n",
    "print(f\"  Eval every: {training_args.eval_steps} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Model ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting training...\n",
      "This will take ~1-1.5 hours on RTX 4070 SUPER\n",
      "============================================================\n",
      "\n",
      "âš ï¸ WATCH THE FIRST FEW STEPS:\n",
      "  1. Training loss should be > 0 (typically 2-8)\n",
      "  2. Loss should decrease over time\n",
      "  3. First eval should show some non-zero ROUGE scores\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1138' max='5859' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1138/5859 04:31 < 18:48, 4.18 it/s, Epoch 0.58/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EVAL] Sample prediction: <0x03>\n",
      "[EVAL] Sample reference: CÃ¡c nghiÃªn cá»©u chá»‰ ra ráº±ng hoáº¡t Ä‘á»™ng thá»ƒ cháº¥t cÃ³ thá»ƒ giÃºp Ä‘á»‘i phÃ³ vá»›i cÃ¡c triá»‡u chá»©ng liÃªn quan Ä‘áº¿n \n",
      "\n",
      "[EVAL] Sample prediction: <0x03>\n",
      "[EVAL] Sample reference: CÃ¡c nghiÃªn cá»©u chá»‰ ra ráº±ng hoáº¡t Ä‘á»™ng thá»ƒ cháº¥t cÃ³ thá»ƒ giÃºp Ä‘á»‘i phÃ³ vá»›i cÃ¡c triá»‡u chá»©ng liÃªn quan Ä‘áº¿n \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  3. First eval should show some non-zero ROUGE scores\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Training complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2676\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m-> 2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2680\u001b[0m ):\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"ğŸš€ Starting training...\")\n",
    "print(\"This will take ~1-1.5 hours on RTX 4070 SUPER\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nâš ï¸ WATCH THE FIRST FEW STEPS:\")\n",
    "print(\"  1. Training loss should be > 0 (typically 2-8)\")\n",
    "print(\"  2. Loss should decrease over time\")\n",
    "print(\"  3. First eval should show some non-zero ROUGE scores\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š Evaluating on test set...\")\n",
    "results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*50)\n",
    "for key, value in results.items():\n",
    "    if 'rouge' in key:\n",
    "        print(f\"{key.upper()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text, max_length=128, num_beams=4):\n",
    "    \"\"\"Generate summary for input text\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        \"tÃ³m táº¯t: \" + text,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        length_penalty=1.0,\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test with examples\n",
    "print(\"\\n=== INFERENCE EXAMPLES ===\")\n",
    "for i in range(3):\n",
    "    test_text = dataset['test'][i]['document']\n",
    "    ground_truth = dataset['test'][i]['summary']\n",
    "    \n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Original ({len(test_text)} chars):\")\n",
    "    print(test_text[:200], \"...\\n\")\n",
    "    \n",
    "    print(\"Generated Summary:\")\n",
    "    generated = generate_summary(test_text)\n",
    "    print(generated)\n",
    "    \n",
    "    print(\"\\nGround Truth:\")\n",
    "    print(ground_truth)\n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./mt5-small-vietnamese-final\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"âœ… Model saved to: {output_dir}\")\n",
    "print(f\"\\nTo load later:\")\n",
    "print(f'tokenizer = AutoTokenizer.from_pretrained(\"{output_dir}\")')\n",
    "print(f'model = AutoModelForSeq2SeqLM.from_pretrained(\"{output_dir}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Quick Test with New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with your own text\n",
    "custom_text = \"\"\"\n",
    "Chiá»u 26/1, UBND TP HÃ  Ná»™i tá»• chá»©c há»p bÃ¡o cÃ´ng bá»‘ káº¿t quáº£ thá»±c hiá»‡n \n",
    "nhiá»‡m vá»¥ phÃ¡t triá»ƒn kinh táº¿ - xÃ£ há»™i nÄƒm 2024. Theo Ä‘Ã³, tá»•ng sáº£n pháº©m \n",
    "trÃªn Ä‘á»‹a bÃ n (GRDP) cá»§a HÃ  Ná»™i nÄƒm 2024 Æ°á»›c tÄƒng 7,5% so vá»›i nÄƒm 2023, \n",
    "cao hÆ¡n má»©c tÄƒng trÆ°á»Ÿng chung cá»§a cáº£ nÆ°á»›c (7,09%).\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(custom_text)\n",
    "print(\"\\nGenerated summary:\")\n",
    "print(generate_summary(custom_text.strip()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
