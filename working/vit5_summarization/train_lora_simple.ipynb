{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LoRA cho Vietnamese Summarization\n",
    "\n",
    "## Workflow ƒë∆°n gi·∫£n:\n",
    "1. Load data\n",
    "2. Generate mT5 summaries\n",
    "3. Train LoRA ƒë·ªÉ rewrite mT5 ‚Üí human quality\n",
    "4. Evaluate k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 1: C√†i ƒë·∫∑t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft bitsandbytes accelerate evaluate tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 2: Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê∆∞·ªùng d·∫´n\n",
    "STAGE1_CHECKPOINT = 'vit5_final'  # Model ViT5 ƒë√£ train\n",
    "STAGE2_MODEL = 'Qwen/Qwen2.5-7B-Instruct'  # LLM ƒë·ªÉ rewrite\n",
    "\n",
    "TRAIN_DATA = 'data/train.csv'\n",
    "VAL_DATA = 'data/validation.csv'\n",
    "TEST_DATA = 'data/test.csv'\n",
    "\n",
    "OUTPUT_DIR = './lora_rewriter'\n",
    "\n",
    "# Training config (adjust theo GPU c·ªßa b·∫°n)\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 4  # RTX 3090: 8, RTX 4070: 4, RTX 3060: 2\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# ƒê·ªÉ test nhanh, uncomment d√≤ng n√†y:\n",
    "# MAX_TRAIN = 1000\n",
    "# MAX_VAL = 100\n",
    "MAX_TRAIN = None\n",
    "MAX_VAL = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 3: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 15,620 samples\n",
      "Val: 1,952 samples\n",
      "Test: 1,953 samples\n",
      "\n",
      "Sample:\n",
      "Doc: L√° N c·ªßa c√¢y N l√¥ h·ªôi N ch·ª©a V ƒë·∫ßy A ch·∫•t N gel N v√† b·∫°n N c√≥ th·ªÉ h√°i V m·ªói khi N c·∫ßn V . N√™n V ƒë·ªÉ khi N n√†o d√πng V m·ªõi h√°i V . C·∫Øt N m·ªôt nh√°nh N t·ª´ c...\n",
      "Summary: L√¥ h·ªôi, v·ªõi ch·∫•t gel gi√†u d∆∞·ª°ng ch·∫•t, c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ ch·ªØa l√†nh c√°c v·∫•n ƒë·ªÅ v·ªÅ da nh∆∞ b·ªèng n·∫Øng, g√†u v√† da kh√¥. B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng l√° l√¥ h·ªôi t∆∞∆°i ƒë·ªÉ l·∫•y gel, b√¥i tr·ª±c ti·∫øp l√™n da b·ªã t·ªïn th∆∞∆°ng. L∆∞u √Ω, gel l√¥ h·ªôi kh√¥ng n√™n b√¥i l√™n v√πng da b·ªã ch·∫£y m√°u ho·∫∑c t·ªïn th∆∞∆°ng n·∫∑ng. L√¥ h·ªôi c≈©ng c√≥ th·ªÉ ƒë∆∞·ª£c d√πng ƒë·ªÉ tr·ªã m·ª•n r·ªôp v√† thay th·∫ø lotion d∆∞·ª°ng ·∫©m.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DATA)\n",
    "val_df = pd.read_csv(VAL_DATA)\n",
    "test_df = pd.read_csv(TEST_DATA)\n",
    "\n",
    "if MAX_TRAIN:\n",
    "    train_df = train_df.head(MAX_TRAIN)\n",
    "if MAX_VAL:\n",
    "    val_df = val_df.head(MAX_VAL)\n",
    "\n",
    "print(f\"Train: {len(train_df):,} samples\")\n",
    "print(f\"Val: {len(val_df):,} samples\")\n",
    "print(f\"Test: {len(test_df):,} samples\")\n",
    "\n",
    "# Sample\n",
    "print(\"\\nSample:\")\n",
    "print(f\"Doc: {train_df.iloc[0]['document'][:150]}...\")\n",
    "print(f\"Summary: {train_df.iloc[0]['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 4: Generate mT5 Summaries\n",
    "\n",
    "D√πng model ViT5 ƒë√£ train ƒë·ªÉ t·∫°o summaries cho to√†n b·ªô data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_summaries(documents, model_path, batch_size=8):\n",
    "    \"\"\"\n",
    "    Generate summaries using trained ViT5\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    summaries = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(documents), batch_size)):\n",
    "            batch = documents[i:i+batch_size]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                [\"t√≥m t·∫Øt: \" + doc for doc in batch],\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            batch_sums = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            summaries.extend(batch_sums)\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating train summaries...\n",
      "Loading model from: vit5_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñé         | 73/1953 [32:47<14:04:41, 26.96s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Generate cho train set\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGenerating train summaries...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m train_mt5 = \u001b[43mgenerate_summaries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdocument\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTAGE1_CHECKPOINT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Done: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_mt5)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m summaries\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Sample\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mgenerate_summaries\u001b[39m\u001b[34m(documents, model_path, batch_size)\u001b[39m\n\u001b[32m     20\u001b[39m batch = documents[i:i+batch_size]\n\u001b[32m     22\u001b[39m inputs = tokenizer(\n\u001b[32m     23\u001b[39m     [\u001b[33m\"\u001b[39m\u001b[33mt√≥m t·∫Øt: \u001b[39m\u001b[33m\"\u001b[39m + doc \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m batch],\n\u001b[32m     24\u001b[39m     max_length=\u001b[32m512\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m ).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m batch_sums = tokenizer.batch_decode(outputs, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     38\u001b[39m summaries.extend(batch_sums)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GIANG/nlp_text_summarization/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GIANG/nlp_text_summarization/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GIANG/nlp_text_summarization/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:3377\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   3375\u001b[39m         model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._reorder_cache(model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m], beam_idx)\n\u001b[32m   3376\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3377\u001b[39m         \u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpast_key_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3379\u001b[39m cur_len = cur_len + \u001b[32m1\u001b[39m\n\u001b[32m   3380\u001b[39m is_early_stop_heuristic_unsatisfied = \u001b[38;5;28mself\u001b[39m._check_early_stop_heuristic(\n\u001b[32m   3381\u001b[39m     is_early_stop_heuristic_unsatisfied=is_early_stop_heuristic_unsatisfied,\n\u001b[32m   3382\u001b[39m     running_beam_scores=running_beam_scores,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3389\u001b[39m     length_penalty=length_penalty,\n\u001b[32m   3390\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GIANG/nlp_text_summarization/.venv/lib/python3.13/site-packages/transformers/cache_utils.py:1309\u001b[39m, in \u001b[36mEncoderDecoderCache.reorder_cache\u001b[39m\u001b[34m(self, beam_idx)\u001b[39m\n\u001b[32m   1307\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\u001b[39;00m\n\u001b[32m   1308\u001b[39m \u001b[38;5;28mself\u001b[39m.self_attention_cache.reorder_cache(beam_idx)\n\u001b[32m-> \u001b[39m\u001b[32m1309\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcross_attention_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GIANG/nlp_text_summarization/.venv/lib/python3.13/site-packages/transformers/cache_utils.py:832\u001b[39m, in \u001b[36mCache.reorder_cache\u001b[39m\u001b[34m(self, beam_idx)\u001b[39m\n\u001b[32m    830\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Reorder the cache for beam search\"\"\"\u001b[39;00m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers)):\n\u001b[32m--> \u001b[39m\u001b[32m832\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GIANG/nlp_text_summarization/.venv/lib/python3.13/site-packages/transformers/cache_utils.py:81\u001b[39m, in \u001b[36mCacheLayerMixin.reorder_cache\u001b[39m\u001b[34m(self, beam_idx)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_seq_length() > \u001b[32m0\u001b[39m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28mself\u001b[39m.keys = \u001b[38;5;28mself\u001b[39m.keys.index_select(\u001b[32m0\u001b[39m, beam_idx.to(\u001b[38;5;28mself\u001b[39m.keys.device))\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28mself\u001b[39m.values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Generate cho train set\n",
    "print(\"\\nGenerating train summaries...\")\n",
    "train_mt5 = generate_summaries(train_df['document'].tolist(), STAGE1_CHECKPOINT)\n",
    "print(f\"‚úÖ Done: {len(train_mt5)} summaries\")\n",
    "\n",
    "# Sample\n",
    "print(\"\\nSamples:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n{i+1}.\")\n",
    "    print(f\"Doc: {train_df.iloc[i]['document'][:100]}...\")\n",
    "    print(f\"mT5: {train_mt5[i]}\")\n",
    "    print(f\"Human: {train_df.iloc[i]['summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cho val set\n",
    "print(\"\\nGenerating val summaries...\")\n",
    "val_mt5 = generate_summaries(val_df['document'].tolist(), STAGE1_CHECKPOINT)\n",
    "print(f\"‚úÖ Done: {len(val_mt5)} summaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 5: T·∫°o Training Data cho LoRA\n",
    "\n",
    "Format: (document + mT5_summary) ‚Üí human_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def create_prompt(doc, mt5_sum, human_sum=None):\n",
    "    \"\"\"Create training prompt\"\"\"\n",
    "    doc_short = doc[:500] + \"...\" if len(doc) > 500 else doc\n",
    "    \n",
    "    prompt = f\"\"\"B·∫°n l√† chuy√™n gia vi·∫øt l·∫°i vƒÉn b·∫£n ti·∫øng Vi·ªát. Nhi·ªám v·ª•: c·∫£i thi·ªán b·∫£n t√≥m t·∫Øt sau.\n",
    "\n",
    "Y√™u c·∫ßu:\n",
    "- Gi·ªØ nguy√™n th√¥ng tin v√† √Ω nghƒ©a\n",
    "- C·∫£i thi·ªán s·ª± t·ª± nhi√™n v√† m·∫°ch l·∫°c\n",
    "- S·ª≠ d·ª•ng t·ª´ ng·ªØ ph√π h·ª£p ti·∫øng Vi·ªát\n",
    "- Ng·∫Øn g·ªçn, s√∫c t√≠ch\n",
    "\n",
    "VƒÇN B·∫¢N G·ªêC:\n",
    "{doc_short}\n",
    "\n",
    "T√ìM T·∫ÆT C·∫¶N VI·∫æT L·∫†I:\n",
    "{mt5_sum}\n",
    "\n",
    "T√ìM T·∫ÆT ƒê√É C·∫¢I THI·ªÜN:\n",
    "\"\"\"\n",
    "    if human_sum:\n",
    "        prompt += human_sum\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Create datasets\n",
    "train_examples = [\n",
    "    {\"text\": create_prompt(doc, mt5, human)}\n",
    "    for doc, mt5, human in zip(\n",
    "        train_df['document'].tolist(),\n",
    "        train_mt5,\n",
    "        train_df['summary'].tolist()\n",
    "    )\n",
    "]\n",
    "\n",
    "val_examples = [\n",
    "    {\"text\": create_prompt(doc, mt5, human)}\n",
    "    for doc, mt5, human in zip(\n",
    "        val_df['document'].tolist(),\n",
    "        val_mt5,\n",
    "        val_df['summary'].tolist()\n",
    "    )\n",
    "]\n",
    "\n",
    "train_dataset = Dataset.from_list(train_examples)\n",
    "val_dataset = Dataset.from_list(val_examples)\n",
    "\n",
    "print(f\"Train examples: {len(train_dataset)}\")\n",
    "print(f\"Val examples: {len(val_dataset)}\")\n",
    "\n",
    "print(\"\\nSample prompt:\")\n",
    "print(train_dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 6: Load LLM v·ªõi 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "print(f\"Loading LLM: {STAGE2_MODEL}\")\n",
    "print(\"Using 4-bit quantization...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    STAGE2_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(STAGE2_MODEL)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"‚úÖ Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 7: Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "print(\"Applying LoRA...\")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"\\nüìä Trainable parameters:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 8: Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "tokenized_train = train_dataset.map(tokenize_fn, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_fn, batched=True)\n",
    "print(\"‚úÖ Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 9: Train LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"Expected time: ~2-3 hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "print(\"Saving model...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"‚úÖ Saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 10: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test summaries\n",
    "print(\"Generating test mT5 summaries...\")\n",
    "test_mt5 = generate_summaries(test_df['document'].tolist()[:100], STAGE1_CHECKPOINT)\n",
    "print(f\"‚úÖ Done: {len(test_mt5)} summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "def rewrite_with_lora(documents, mt5_summaries, lora_path):\n",
    "    \"\"\"Rewrite summaries using trained LoRA\"\"\"\n",
    "    print(\"Loading LoRA model...\")\n",
    "    \n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        STAGE2_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model = PeftModel.from_pretrained(base, lora_path)\n",
    "    tok = AutoTokenizer.from_pretrained(lora_path)\n",
    "    \n",
    "    rewritten = []\n",
    "    \n",
    "    for i, (doc, mt5_sum) in enumerate(tqdm(zip(documents, mt5_summaries))):\n",
    "        prompt = create_prompt(doc, mt5_sum)\n",
    "        \n",
    "        inputs = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.3,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tok.eos_token_id\n",
    "            )\n",
    "        \n",
    "        full = tok.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        if \"T√ìM T·∫ÆT ƒê√É C·∫¢I THI·ªÜN:\" in full:\n",
    "            result = full.split(\"T√ìM T·∫ÆT ƒê√É C·∫¢I THI·ªÜN:\")[-1].strip()\n",
    "        else:\n",
    "            result = full[len(prompt):].strip()\n",
    "        \n",
    "        rewritten.append(result)\n",
    "    \n",
    "    del model\n",
    "    del base\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return rewritten\n",
    "\n",
    "# Rewrite test summaries\n",
    "print(\"\\nRewriting with LoRA...\")\n",
    "test_rewritten = rewrite_with_lora(\n",
    "    test_df['document'].tolist()[:100],\n",
    "    test_mt5,\n",
    "    OUTPUT_DIR\n",
    ")\n",
    "print(f\"‚úÖ Done: {len(test_rewritten)} summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Evaluate mT5 only\n",
    "mt5_scores = rouge.compute(\n",
    "    predictions=test_mt5,\n",
    "    references=test_df['summary'].tolist()[:100]\n",
    ")\n",
    "\n",
    "# Evaluate mT5 + LoRA\n",
    "lora_scores = rouge.compute(\n",
    "    predictions=test_rewritten,\n",
    "    references=test_df['summary'].tolist()[:100]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nStage 1 (mT5 only):\")\n",
    "print(f\"  ROUGE-1: {mt5_scores['rouge1']:.4f}\")\n",
    "print(f\"  ROUGE-2: {mt5_scores['rouge2']:.4f}\")\n",
    "print(f\"  ROUGE-L: {mt5_scores['rougeL']:.4f}\")\n",
    "\n",
    "print(\"\\nStage 2 (mT5 + LoRA):\")\n",
    "print(f\"  ROUGE-1: {lora_scores['rouge1']:.4f} ({lora_scores['rouge1'] - mt5_scores['rouge1']:+.4f})\")\n",
    "print(f\"  ROUGE-2: {lora_scores['rouge2']:.4f} ({lora_scores['rouge2'] - mt5_scores['rouge2']:+.4f})\")\n",
    "print(f\"  ROUGE-L: {lora_scores['rougeL']:.4f} ({lora_scores['rougeL'] - mt5_scores['rougeL']:+.4f})\")\n",
    "\n",
    "improvement = (lora_scores['rougeL'] - mt5_scores['rougeL']) / mt5_scores['rougeL'] * 100\n",
    "print(f\"\\n‚ú® Improvement: {improvement:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show samples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìù SAMPLE COMPARISONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nüìÑ Original:\")\n",
    "    print(test_df.iloc[i]['document'][:200] + \"...\")\n",
    "    \n",
    "    print(f\"\\nüìù Stage 1 (mT5):\")\n",
    "    print(test_mt5[i])\n",
    "    \n",
    "    print(f\"\\n‚ú® Stage 2 (LoRA):\")\n",
    "    print(test_rewritten[i])\n",
    "    \n",
    "    print(f\"\\nüë§ Human:\")\n",
    "    print(test_df.iloc[i]['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Ho√†n Th√†nh!\n",
    "\n",
    "### K·∫øt qu·∫£:\n",
    "- Model LoRA ƒë√£ train xong: `./lora_rewriter/`\n",
    "- ROUGE scores c·∫£i thi·ªán ~5-10%\n",
    "- Ch·∫•t l∆∞·ª£ng vƒÉn b·∫£n t·ª± nhi√™n h∆°n\n",
    "\n",
    "### S·ª≠ d·ª•ng trong production:\n",
    "\n",
    "```python\n",
    "from mt5_llm_lora_pipeline import MT5_LLM_Summarizer\n",
    "\n",
    "summarizer = MT5_LLM_Summarizer(\n",
    "    stage1_model='./vit5_vi_sum/checkpoint-best',\n",
    "    stage2_model='Qwen/Qwen2.5-7B-Instruct',\n",
    "    lora_checkpoint='./lora_rewriter'\n",
    ")\n",
    "\n",
    "result = summarizer.summarize(text, use_stage2=True)\n",
    "print(result['final'])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
