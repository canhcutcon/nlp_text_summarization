{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Vietnamese Text Summarization - mT5-Small Fine-tuning\n\n‚úÖ **Model**: google/mt5-small (300M params)  \n‚úÖ **Task**: Abstractive Summarization for Vietnamese  \n‚úÖ **Strategy**: Properly structured seq2seq with optimized hyperparameters  \n‚úÖ **Dataset**: Vietnamese documents with human-written summaries  \n\n---\n\n## Key Improvements in This Version\n\n1. **Standardized Summarization Task Format**\n   - Proper prefix: \"t√≥m t·∫Øt: \" for all inputs\n   - Consistent max lengths (input: 512, output: 128)\n\n2. **Stable Training Configuration**\n   - Learning rate: 2e-4 (optimal for mT5)\n   - Batch size: 2 with gradient accumulation: 8 (effective batch: 16)\n   - FP16 enabled on CUDA GPUs\n   - Warmup steps: 500\n\n3. **Comprehensive Evaluation**\n   - ROUGE-1, ROUGE-2, ROUGE-L metrics\n   - Sample output inspection (metrics aren't everything!)\n\n4. **Optimized Inference**\n   - Beam search: 4-6 beams\n   - Length penalty: 1.0-1.5\n   - Repetition penalty: 1.2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install -q transformers datasets accelerate sentencepiece evaluate rouge-score py-rouge scikit-learn protobuf torch --root-user-action=ignore\n\nprint(\"‚úÖ All packages installed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import re\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset, DatasetDict\n\n# Simple Vietnamese sentence tokenizer\ndef sent_tokenize(text: str) -> list[str]:\n    \"\"\"Vietnamese sentence tokenizer\"\"\"\n    pattern = r'(?<=[.!?])\\s+(?=[A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê])'\n    sentences = re.split(pattern, text)\n    return [s.strip() for s in sentences if s.strip()]\n\n# Load dataset\nprint(\"üìä Loading Vietnamese Summarization Dataset...\")\ntrain_df = pd.read_csv(\"data/train.csv\")\nval_df = pd.read_csv(\"data/validation.csv\")\ntest_df = pd.read_csv(\"data/test.csv\")\n\nprint(f\"‚úì Train: {len(train_df):,} samples\")\nprint(f\"‚úì Validation: {len(val_df):,} samples\")\nprint(f\"‚úì Test: {len(test_df):,} samples\")\n\n# Analyze data statistics\ndef analyze_lengths(df: pd.DataFrame, name: str):\n    doc_words = df['document'].apply(lambda x: len(x.split()))\n    sum_words = df['summary'].apply(lambda x: len(x.split()))\n    compression_ratio = (sum_words.mean() / doc_words.mean() * 100)\n    \n    print(f\"\\n{name}:\")\n    print(f\"  Avg document: {doc_words.mean():.0f} words, Avg summary: {sum_words.mean():.0f} words\")\n    print(f\"  Compression ratio: {compression_ratio:.1f}%\")\n\nanalyze_lengths(train_df, \"Train\")\nanalyze_lengths(val_df, \"Validation\")\nanalyze_lengths(test_df, \"Test\")\n\n# Convert to HuggingFace Dataset\ndataset = DatasetDict({\n    'train': Dataset.from_pandas(train_df[['document', 'summary']], preserve_index=False),\n    'validation': Dataset.from_pandas(val_df[['document', 'summary']], preserve_index=False),\n    'test': Dataset.from_pandas(test_df[['document', 'summary']], preserve_index=False)\n})\n\nprint(f\"\\nüìù Sample:\")\nsample = dataset['train'][0]\nprint(f\"Document: {sample['document'][:200]}...\")\nprint(f\"Summary: {sample['summary'][:150]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# üçé MAC FIX: FORCE CPU TO AVOID MPS BUGS\n# ============================================================================\nimport os\n\n# FORCE CPU - MPS has too many bugs with mT5\nos.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\nos.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n\n# Additional fix: completely disable MPS\nos.environ['PYTORCH_FORCE_CPU'] = '1'\n\nprint(\"üçé Mac compatibility mode enabled\")\nprint(\"   MPS COMPLETELY DISABLED - forcing CPU\")\nprint(\"   (Training will be slow but stable)\")\nprint(\"   \")\nprint(\"‚ö†Ô∏è  IMPORTANT: You MUST restart your kernel after running this cell!\")\nprint(\"   Kernel ‚Üí Restart ‚Üí Run cells from beginning\")"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TextRank Summarizer created!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TextRank Implementation\n",
    "# ============================================================================\n",
    "class TextRankSummarizer:\n",
    "    \"\"\"TextRank algorithm for extractive summarization\"\"\"\n",
    "    \n",
    "    def __init__(self, top_n: int = 3, damping: float = 0.85):\n",
    "        self.top_n = top_n\n",
    "        self.damping = damping\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base')\n",
    "        self.model = AutoModel.from_pretrained('vinai/phobert-base')\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def get_sentence_embedding(self, sentence: str) -> np.ndarray:\n",
    "        \"\"\"Get PhoBERT embedding for a sentence\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            sentence, \n",
    "            return_tensors='pt', \n",
    "            truncation=True, \n",
    "            max_length=256\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            # Use CLS token embedding\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        \n",
    "        return embedding[0]\n",
    "    \n",
    "    def build_similarity_matrix(self, sentences: list[str]) -> np.ndarray:\n",
    "        \"\"\"Build similarity matrix between sentences\"\"\"\n",
    "        print(f\"  Computing embeddings for {len(sentences)} sentences...\")\n",
    "        embeddings = []\n",
    "        \n",
    "        for sent in tqdm(sentences, desc=\"Encoding\"):\n",
    "            emb = self.get_sentence_embedding(sent)\n",
    "            embeddings.append(emb)\n",
    "        \n",
    "        embeddings = np.array(embeddings)\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def textrank(self, similarity_matrix: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Run TextRank algorithm (PageRank on sentence graph)\"\"\"\n",
    "        # Create graph from similarity matrix\n",
    "        nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "        \n",
    "        # Compute PageRank scores\n",
    "        scores = nx.pagerank(nx_graph, alpha=self.damping)\n",
    "        \n",
    "        return np.array(list(scores.values()))\n",
    "    \n",
    "    def summarize(self, document: str, num_sentences: int = None) -> str:\n",
    "        \"\"\"Generate extractive summary using TextRank\"\"\"\n",
    "        if num_sentences is None:\n",
    "            num_sentences = self.top_n\n",
    "        \n",
    "        # Split into sentences\n",
    "        sentences = sent_tokenize(document)\n",
    "        \n",
    "        if len(sentences) <= num_sentences:\n",
    "            return document\n",
    "        \n",
    "        # Build similarity matrix\n",
    "        similarity_matrix = self.build_similarity_matrix(sentences)\n",
    "        \n",
    "        # Run TextRank\n",
    "        scores = self.textrank(similarity_matrix)\n",
    "        \n",
    "        # Select top sentences\n",
    "        ranked_indices = np.argsort(scores)[::-1][:num_sentences]\n",
    "        \n",
    "        # Sort by original order to maintain coherence\n",
    "        ranked_indices = sorted(ranked_indices)\n",
    "        \n",
    "        # Extract summary\n",
    "        summary_sentences = [sentences[i] for i in ranked_indices]\n",
    "        summary = ' '.join(summary_sentences)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "print(\"‚úÖ TextRank Summarizer created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "from transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer\n)\nimport evaluate\n\nprint(\"‚úÖ Transformers imported successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ec47e1ca5c434ba921f9d94b07134a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/15620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbf8317c9d94ee39066d346f02d9747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/1952 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7d2224a1c24a3ea68dc1facbb3e22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/1953 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample tokenized data:\n",
      "Input length: 512\n",
      "Label length: 128\n",
      "Input IDs (first 20): [259, 164459, 259, 270, 2289, 270, 267, 26965, 441, 317, 708, 262, 4650, 276, 441, 259, 29828, 382, 2291, 441]\n",
      "Labels (first 20): [458, 1858, 382, 2291, 261, 300, 908, 562, 1075, 4501, 718, 369, 273, 331, 2294, 7790, 370, 562, 1075, 261]\n",
      "\n",
      "Decoded input: t√≥m t·∫Øt: L√° N c·ªßa c√¢y N l√¥ h·ªôi N ch·ª©a V ƒë·∫ßy A ch·∫•t N gel N v√† b·∫°n N c√≥ th·ªÉ h√°i V m·ªói khi N c\n",
      "Decoded label: L√¥ h·ªôi, v·ªõi ch·∫•t gel gi√†u d∆∞·ª°ng ch·∫•t, c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ ch·ªØa l√†nh c√°c v·∫•n ƒë·ªÅ v·ªÅ da nh∆∞ b·ªè\n",
      "\n",
      "‚úÖ Tokenization complete!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize inputs and targets\"\"\"\n",
    "    # Add prefix\n",
    "    inputs = [\"t√≥m t·∫Øt: \" + doc for doc in examples[\"document\"]]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"summary\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"üîÑ Tokenizing dataset...\")\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "# Verify\n",
    "sample = tokenized_datasets[\"train\"][0]\n",
    "print(f\"\\nSample tokenized data:\")\n",
    "print(f\"Input length: {len(sample['input_ids'])}\")\n",
    "print(f\"Label length: {len(sample['labels'])}\")\n",
    "print(f\"Input IDs (first 20): {sample['input_ids'][:20]}\")\n",
    "print(f\"Labels (first 20): {sample['labels'][:20]}\")\n",
    "\n",
    "# Decode to verify\n",
    "decoded_input = tokenizer.decode(sample['input_ids'][:50])\n",
    "decoded_label = tokenizer.decode(sample['labels'][:50])\n",
    "print(f\"\\nDecoded input: {decoded_input}\")\n",
    "print(f\"Decoded label: {decoded_label}\")\n",
    "\n",
    "print(\"\\n‚úÖ Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DETAILED LABEL CHECK ===\n",
      "Input IDs (first 20): [259, 164459, 259, 270, 2289, 270, 267, 26965, 441, 317, 708, 262, 4650, 276, 441, 259, 29828, 382, 2291, 441]\n",
      "Labels (first 20): [458, 1858, 382, 2291, 261, 300, 908, 562, 1075, 4501, 718, 369, 273, 331, 2294, 7790, 370, 562, 1075, 261]\n",
      "\n",
      "Total labels: 128\n",
      "Number of -100: 0\n",
      "Valid labels: 128\n",
      "Percentage valid: 100.0%\n",
      "\n",
      "Decoded valid labels: L√¥ h·ªôi, v·ªõi ch·∫•t gel gi√†u d∆∞·ª°ng ch·∫•t, c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ ch·ªØa l√†nh c√°c v·∫•n ƒë·ªÅ v·ªÅ da nh∆∞ b·ªèng n·∫Øng, g√†u v√† da kh√¥. B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng l√° l√¥ h·ªôi t∆∞∆°i ƒë·ªÉ l·∫•y gel, b√¥i tr·ª±c ti·∫øp l√™n da b·ªã t·ªïn th∆∞∆°ng. L∆∞u √Ω, gel l√¥ h·ªôi kh√¥ng n√™n b√¥i l√™n v√πng</s>\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DETAILED LABEL CHECK ===\")\n",
    "sample = tokenized_datasets[\"train\"][0]\n",
    "\n",
    "print(f\"Input IDs (first 20): {sample['input_ids'][:20]}\")\n",
    "print(f\"Labels (first 20): {sample['labels'][:20]}\")\n",
    "\n",
    "# Count -100\n",
    "num_neg100 = sum(1 for l in sample['labels'] if l == -100)\n",
    "num_valid = len(sample['labels']) - num_neg100\n",
    "\n",
    "print(f\"\\nTotal labels: {len(sample['labels'])}\")\n",
    "print(f\"Number of -100: {num_neg100}\")\n",
    "print(f\"Valid labels: {num_valid}\")\n",
    "print(f\"Percentage valid: {num_valid/len(sample['labels'])*100:.1f}%\")\n",
    "\n",
    "# Decode valid labels\n",
    "valid_labels = [l for l in sample['labels'] if l != -100]\n",
    "if valid_labels:\n",
    "    decoded = tokenizer.decode(valid_labels)\n",
    "    print(f\"\\nDecoded valid labels: {decoded}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå‚ùå‚ùå NO VALID LABELS - ALL ARE -100! ‚ùå‚ùå‚ùå\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Load mT5-Small model and tokenizer\nMODEL_NAME = \"google/mt5-small\"\n\nprint(f\"üì• Loading model: {MODEL_NAME}\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\nprint(f\"‚úÖ Model loaded successfully!\")\nprint(f\"   Parameters: {model.num_parameters():,}\")\nprint(f\"   Vocab size: {tokenizer.vocab_size:,}\")\n\n# Move to device\nmodel = model.to(device)\nprint(f\"   Device: {device}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DETAILED LABEL CHECK ===\n",
      "Input IDs (first 20): [259, 164459, 259, 270, 2289, 270, 267, 26965, 441, 317, 708, 262, 4650, 276, 441, 259, 29828, 382, 2291, 441]\n",
      "Labels (first 20): [458, 1858, 382, 2291, 261, 300, 908, 562, 1075, 4501, 718, 369, 273, 331, 2294, 7790, 370, 562, 1075, 261]\n",
      "\n",
      "Total labels: 128\n",
      "Number of -100: 0\n",
      "Valid labels: 128\n",
      "Percentage valid: 100.0%\n",
      "\n",
      "Decoded valid labels: L√¥ h·ªôi, v·ªõi ch·∫•t gel gi√†u d∆∞·ª°ng ch·∫•t, c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ ch·ªØa l√†nh c√°c v·∫•n ƒë·ªÅ v·ªÅ da nh∆∞ b·ªèng n·∫Øng, g√†u v√† da kh√¥. B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng l√° l√¥ h·ªôi t∆∞∆°i ƒë·ªÉ l·∫•y gel, b√¥i tr·ª±c ti·∫øp l√™n da b·ªã t·ªïn th∆∞∆°ng. L∆∞u √Ω, gel l√¥ h·ªôi kh√¥ng n√™n b√¥i l√™n v√πng</s>\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DETAILED LABEL CHECK ===\")\n",
    "sample = tokenized_datasets[\"train\"][0]\n",
    "\n",
    "print(f\"Input IDs (first 20): {sample['input_ids'][:20]}\")\n",
    "print(f\"Labels (first 20): {sample['labels'][:20]}\")\n",
    "\n",
    "# Count -100\n",
    "num_neg100 = sum(1 for l in sample['labels'] if l == -100)\n",
    "num_valid = len(sample['labels']) - num_neg100\n",
    "\n",
    "print(f\"\\nTotal labels: {len(sample['labels'])}\")\n",
    "print(f\"Number of -100: {num_neg100}\")\n",
    "print(f\"Valid labels: {num_valid}\")\n",
    "print(f\"Percentage valid: {num_valid/len(sample['labels'])*100:.1f}%\")\n",
    "\n",
    "# Decode valid labels\n",
    "valid_labels = [l for l in sample['labels'] if l != -100]\n",
    "if valid_labels:\n",
    "    decoded = tokenizer.decode(valid_labels)\n",
    "    print(f\"\\nDecoded valid labels: {decoded}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå‚ùå‚ùå NO VALID LABELS - ALL ARE -100! ‚ùå‚ùå‚ùå\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Model üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# 6Ô∏è‚É£ METRIC ‚Äì ƒê·ª™NG CH·ªà NH√åN LOSS!\n# ============================================================================\n# Chu·∫©n ƒë√°nh gi√°: ROUGE-1, ROUGE-2, ROUGE-L (quan tr·ªçng nh·∫•t)\n# ‚ö†Ô∏è Nh∆∞ng: ROUGE cao ‚â† t√≥m t·∫Øt hay\n# üëâ Lu√¥n ƒë·ªçc sample output b·∫±ng m·∫Øt ng∆∞·ªùi\n\nrouge = evaluate.load(\"rouge\")\n\ndef compute_metrics(eval_pred):\n    \"\"\"\n    Compute ROUGE scores v√† hi·ªÉn th·ªã sample predictions\n    ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng th·ª±c t·∫ø\n    \"\"\"\n    predictions, labels = eval_pred\n    \n    # N·∫øu predictions l√† logits, l·∫•y argmax\n    if len(predictions.shape) == 3:\n        predictions = np.argmax(predictions, axis=-1)\n    \n    # Decode predictions\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    \n    # Replace -100 in labels (padding tokens)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # üëÅÔ∏è LU√îN HI·ªÇN TH·ªä SAMPLE ƒë·ªÉ ki·ªÉm tra ch·∫•t l∆∞·ª£ng th·ª±c t·∫ø\n    if len(decoded_preds) > 0:\n        print(f\"\\n{'='*70}\")\n        print(\"üìù SAMPLE PREDICTION (ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng th·ª±c t·∫ø):\")\n        print(f\"{'='*70}\")\n        print(f\"Prediction: {decoded_preds[0][:200]}\")\n        print(f\"Reference:  {decoded_labels[0][:200]}\")\n        print(f\"{'='*70}\\n\")\n    \n    # Clean text\n    decoded_preds = [\"\\n\".join(pred.strip().split()) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(label.strip().split()) for label in decoded_labels]\n    \n    # Compute ROUGE\n    result = rouge.compute(\n        predictions=decoded_preds,\n        references=decoded_labels,\n        use_stemmer=False\n    )\n    \n    # Return scores\n    return {\n        \"rouge1\": result[\"rouge1\"],\n        \"rouge2\": result[\"rouge2\"],\n        \"rougeL\": result[\"rougeL\"],\n        \"rougeLsum\": result[\"rougeLsum\"],\n    }\n\nprint(\"‚úÖ Metrics defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# 5Ô∏è‚É£ TRAINING STRATEGY (PH·∫¶N X∆Ø∆†NG S·ªêNG)\n# ============================================================================\n\n# Data collator for dynamic padding\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    label_pad_token_id=-100\n)\n\n# 5.2 Training arguments (baseline ·ªïn ƒë·ªãnh)\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./mt5_vi_sum\",\n    \n    # Batch size strategy\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,  # Gi·∫£ l·∫≠p batch size 16\n    \n    # Learning rate cho mT5\n    # üëâ 1e-4 ‚Üí ·ªïn ƒë·ªãnh\n    # üëâ 2e-4 ‚Üí nhanh h∆°n (recommended)\n    # üëâ >3e-4 ‚Üí d·ªÖ n·ªï loss üí£\n    learning_rate=2e-4,\n    warmup_steps=500,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    \n    # Evaluation strategy\n    eval_strategy=\"steps\",\n    eval_steps=1000,\n    save_strategy=\"steps\",\n    save_steps=1000,\n    \n    # Generation settings for evaluation\n    predict_with_generate=True,\n    generation_max_length=128,\n    generation_num_beams=4,\n    \n    # Optimization\n    fp16=USE_FP16,  # Enable FP16 on CUDA\n    gradient_checkpointing=USE_GRAD_CHECKPOINT,\n    \n    # Logging\n    logging_steps=100,\n    logging_first_step=True,\n    save_total_limit=2,\n    \n    # Best model selection\n    load_best_model_at_end=True,\n    metric_for_best_model=\"rougeL\",  # ROUGE-L l√† quan tr·ªçng nh·∫•t\n    greater_is_better=True,\n    \n    report_to=\"none\",\n)\n\n# Create Seq2Seq Trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\nprint(\"‚úÖ Trainer initialized!\")\nprint(f\"\\nüìä Training Configuration:\")\nprint(f\"   Device: {device}\")\nprint(f\"   FP16: {USE_FP16}\")\nprint(f\"   Per-device batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"   Learning rate: {training_args.learning_rate}\")\nprint(f\"   Warmup steps: {training_args.warmup_steps}\")\nprint(f\"   Total epochs: {training_args.num_train_epochs}\")\nprint(f\"   Eval every: {training_args.eval_steps} steps\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Quick Test with New Text"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}