{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ ViT5 Model - Comprehensive Evaluation & Analysis\n",
    "\n",
    "**ƒê√°nh gi√° chi ti·∫øt model ViT5 ƒë√£ train xong cho task Vietnamese Text Summarization**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Contents:\n",
    "1. Setup & Load Model\n",
    "2. Evaluate on Test Set\n",
    "3. Statistical Analysis\n",
    "4. Performance by Document Length\n",
    "5. Best & Worst Examples\n",
    "6. Comprehensive Visualizations\n",
    "7. Save Results\n",
    "\n",
    "---\n",
    "\n",
    "**Expected Results:**\n",
    "- ROUGE-1: ~75%\n",
    "- ROUGE-2: ~44%\n",
    "- ROUGE-L: ~47%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1Ô∏è‚É£ Setup - Install Dependencies & Import Libraries\n\n**IMPORTANT:** Run the install cell below first to avoid `ModuleNotFoundError`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"Install required packages\"\"\"\n# Run this cell first if you get ModuleNotFoundError\n!pip install transformers datasets evaluate rouge-score sentencepiece -q\n\nprint(\"‚úÖ All packages installed successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import all required libraries\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ViT5 FINAL MODEL - COMPREHENSIVE EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load Model & Data\n",
    "\n",
    "**Important:** Update these paths according to your Kaggle setup:\n",
    "- `MODEL_PATH`: Path to your trained ViT5 model\n",
    "- `DATA_PATH`: Path to your test data CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Configuration - UPDATE THESE PATHS FOR KAGGLE\"\"\"\n",
    "\n",
    "# For Kaggle, update to your dataset paths:\n",
    "MODEL_PATH = '/kaggle/input/your-vit5-model/vit5_final'  # ‚Üê Update this\n",
    "DATA_PATH = '/kaggle/input/your-dataset'  # ‚Üê Update this\n",
    "\n",
    "# Or for local testing:\n",
    "# MODEL_PATH = './vit5_final'\n",
    "# DATA_PATH = 'data'\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the trained ViT5 model and tokenizer\"\"\"\n",
    "\n",
    "print(\"\\nüìÇ Loading model and tokenizer...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"   Path: {MODEL_PATH}\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Parameters: {num_params:,}\")\n",
    "print(f\"   Model type: {model.config.model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load test data\"\"\"\n",
    "\n",
    "print(\"\\nüìÇ Loading test data...\")\n",
    "\n",
    "test_df = pd.read_csv(f'{DATA_PATH}/test.csv')\n",
    "\n",
    "print(f\"‚úÖ Test data loaded successfully!\")\n",
    "print(f\"   Total samples: {len(test_df):,}\")\n",
    "print(f\"   Columns: {list(test_df.columns)}\")\n",
    "print(f\"\\nüìä Data preview:\")\n",
    "display(test_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load ROUGE metric\"\"\"\n",
    "\n",
    "rouge_metric = evaluate.load('rouge')\n",
    "print(\"‚úÖ ROUGE metric loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Evaluate on Test Set\n",
    "\n",
    "This will:\n",
    "- Generate predictions for all test samples\n",
    "- Compute ROUGE scores\n",
    "- Track document/summary lengths\n",
    "\n",
    "**‚è±Ô∏è Time estimate:** ~30-60 minutes for ~2000 samples on CPU, ~5-10 minutes on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run evaluation on test set\"\"\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üî¨ EVALUATING ON TEST SET\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Storage for results\n",
    "results = {\n",
    "    'rouge1': [],\n",
    "    'rouge2': [],\n",
    "    'rougeL': [],\n",
    "    'predictions': [],\n",
    "    'references': [],\n",
    "    'document_lengths': [],\n",
    "    'summary_lengths': [],\n",
    "    'prediction_lengths': [],\n",
    "}\n",
    "\n",
    "print(f\"Generating predictions for {len(test_df):,} test samples...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Max input length: {MAX_LENGTH}\")\n",
    "print(f\"Max output length: {MAX_TARGET_LENGTH}\\n\")\n",
    "\n",
    "# Evaluate with progress bar\n",
    "with torch.no_grad():\n",
    "    for idx in tqdm(range(len(test_df)), desc=\"Evaluating\"):\n",
    "        # Get document and reference summary\n",
    "        document = str(test_df.iloc[idx]['document'])\n",
    "        reference = str(test_df.iloc[idx]['summary'])\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            \"summarize: \" + document,\n",
    "            max_length=MAX_LENGTH,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate prediction\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            num_beams=4,\n",
    "            length_penalty=0.6,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "        \n",
    "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Compute ROUGE scores\n",
    "        scores = rouge_metric.compute(\n",
    "            predictions=[prediction],\n",
    "            references=[reference],\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results['rouge1'].append(scores['rouge1'])\n",
    "        results['rouge2'].append(scores['rouge2'])\n",
    "        results['rougeL'].append(scores['rougeL'])\n",
    "        results['predictions'].append(prediction)\n",
    "        results['references'].append(reference)\n",
    "        results['document_lengths'].append(len(document))\n",
    "        results['summary_lengths'].append(len(reference))\n",
    "        results['prediction_lengths'].append(len(prediction))\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Overall Statistics\n",
    "\n",
    "Compute and display comprehensive statistics for all ROUGE metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute overall statistics\"\"\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üìä TEST RESULTS - OVERALL STATISTICS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Overall metrics\n",
    "rouge1_mean = np.mean(results['rouge1']) * 100\n",
    "rouge1_std = np.std(results['rouge1']) * 100\n",
    "rouge2_mean = np.mean(results['rouge2']) * 100\n",
    "rouge2_std = np.std(results['rouge2']) * 100\n",
    "rougeL_mean = np.mean(results['rougeL']) * 100\n",
    "rougeL_std = np.std(results['rougeL']) * 100\n",
    "\n",
    "print(f\"üéØ ROUGE Scores:\")\n",
    "print(f\"   ROUGE-1: {rouge1_mean:.2f}% ¬± {rouge1_std:.2f}%\")\n",
    "print(f\"   ROUGE-2: {rouge2_mean:.2f}% ¬± {rouge2_std:.2f}%\")\n",
    "print(f\"   ROUGE-L: {rougeL_mean:.2f}% ¬± {rougeL_std:.2f}%\")\n",
    "\n",
    "# Percentiles\n",
    "print(f\"\\nüìà Score Distribution (Percentiles):\")\n",
    "for metric_name, metric_data in [('ROUGE-1', results['rouge1']),\n",
    "                                   ('ROUGE-2', results['rouge2']),\n",
    "                                   ('ROUGE-L', results['rougeL'])]:\n",
    "    p25 = np.percentile(metric_data, 25) * 100\n",
    "    p50 = np.percentile(metric_data, 50) * 100\n",
    "    p75 = np.percentile(metric_data, 75) * 100\n",
    "    print(f\"  {metric_name}: 25th={p25:.1f}%, Median={p50:.1f}%, 75th={p75:.1f}%\")\n",
    "\n",
    "# Length analysis\n",
    "print(f\"\\nüìè Length Analysis:\")\n",
    "print(f\"   Avg document length: {np.mean(results['document_lengths']):.1f} chars\")\n",
    "print(f\"   Avg reference length: {np.mean(results['summary_lengths']):.1f} chars\")\n",
    "print(f\"   Avg prediction length: {np.mean(results['prediction_lengths']):.1f} chars\")\n",
    "print(f\"   Compression ratio: {np.mean(results['summary_lengths'])/np.mean(results['document_lengths']):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Performance by Document Length\n",
    "\n",
    "Analyze how the model performs on short, medium, and long documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Analysis by document length\"\"\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üìè PERFORMANCE BY DOCUMENT LENGTH\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Categorize documents by length (tertiles)\n",
    "doc_lengths = np.array(results['document_lengths'])\n",
    "short_mask = doc_lengths < np.percentile(doc_lengths, 33)\n",
    "medium_mask = (doc_lengths >= np.percentile(doc_lengths, 33)) & (doc_lengths < np.percentile(doc_lengths, 67))\n",
    "long_mask = doc_lengths >= np.percentile(doc_lengths, 67)\n",
    "\n",
    "def print_length_stats(mask, category):\n",
    "    r1 = np.mean([results['rouge1'][i] for i in range(len(mask)) if mask[i]]) * 100\n",
    "    r2 = np.mean([results['rouge2'][i] for i in range(len(mask)) if mask[i]]) * 100\n",
    "    rL = np.mean([results['rougeL'][i] for i in range(len(mask)) if mask[i]]) * 100\n",
    "    count = np.sum(mask)\n",
    "    avg_len = np.mean([doc_lengths[i] for i in range(len(mask)) if mask[i]])\n",
    "    print(f\"{category:12} ({count:4} docs, avg len: {avg_len:6.0f} chars)\")\n",
    "    print(f\"  ROUGE-1: {r1:5.2f}% | ROUGE-2: {r2:5.2f}% | ROUGE-L: {rL:5.2f}%\")\n",
    "\n",
    "print_length_stats(short_mask, \"Short\")\n",
    "print_length_stats(medium_mask, \"Medium\")\n",
    "print_length_stats(long_mask, \"Long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Best & Worst Examples\n",
    "\n",
    "Examine the best and worst predictions to understand model strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Show best predictions\"\"\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üèÜ TOP 5 BEST PREDICTIONS (Highest ROUGE-L)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "rougeL_scores = np.array(results['rougeL'])\n",
    "best_indices = np.argsort(rougeL_scores)[-5:][::-1]\n",
    "\n",
    "for i, idx in enumerate(best_indices, 1):\n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(f\"Example #{i} - ROUGE Scores:\")\n",
    "    print(f\"  ROUGE-1: {results['rouge1'][idx]*100:.2f}%  |  ROUGE-2: {results['rouge2'][idx]*100:.2f}%  |  ROUGE-L: {results['rougeL'][idx]*100:.2f}%\")\n",
    "    print(f\"\\n  üìÑ Reference Summary:\")\n",
    "    print(f\"  {results['references'][idx][:300]}...\")\n",
    "    print(f\"\\n  ü§ñ Predicted Summary:\")\n",
    "    print(f\"  {results['predictions'][idx][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Show worst predictions\"\"\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚ö†Ô∏è  BOTTOM 5 PREDICTIONS (Lowest ROUGE-L)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "worst_indices = np.argsort(rougeL_scores)[:5]\n",
    "\n",
    "for i, idx in enumerate(worst_indices, 1):\n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(f\"Example #{i} - ROUGE Scores:\")\n",
    "    print(f\"  ROUGE-1: {results['rouge1'][idx]*100:.2f}%  |  ROUGE-2: {results['rouge2'][idx]*100:.2f}%  |  ROUGE-L: {results['rougeL'][idx]*100:.2f}%\")\n",
    "    print(f\"\\n  üìÑ Reference Summary:\")\n",
    "    print(f\"  {results['references'][idx][:300]}...\")\n",
    "    print(f\"\\n  ü§ñ Predicted Summary:\")\n",
    "    print(f\"  {results['predictions'][idx][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Comprehensive Visualizations\n",
    "\n",
    "Create a comprehensive visualization with 7 charts:\n",
    "1. ROUGE-1, ROUGE-2, ROUGE-L distributions (histograms)\n",
    "2. Box plots comparing all metrics\n",
    "3. Document length vs ROUGE-L scatter plot\n",
    "4. Prediction vs Reference length comparison\n",
    "5. Performance by document length category\n",
    "6. ROUGE metrics correlation heatmap\n",
    "7. Summary statistics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create comprehensive visualizations\"\"\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üìä GENERATING COMPREHENSIVE VISUALIZATIONS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create figure with subplots\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. ROUGE Score Distributions (3 histograms)\n",
    "for idx, (score, title, color) in enumerate([\n",
    "    (results['rouge1'], 'ROUGE-1', '#3498db'),\n",
    "    (results['rouge2'], 'ROUGE-2', '#e74c3c'),\n",
    "    (results['rougeL'], 'ROUGE-L', '#2ecc71')\n",
    "]):\n",
    "    ax = fig.add_subplot(gs[0, idx])\n",
    "    ax.hist(score, bins=30, alpha=0.7, color=color, edgecolor='black')\n",
    "    ax.axvline(np.mean(score), color='red', linestyle='--', linewidth=2,\n",
    "               label=f'Mean: {np.mean(score):.3f}')\n",
    "    ax.set_xlabel('Score', fontsize=11)\n",
    "    ax.set_ylabel('Frequency', fontsize=11)\n",
    "    ax.set_title(f'{title} Distribution', fontsize=13, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box plots\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "box_data = [results['rouge1'], results['rouge2'], results['rougeL']]\n",
    "bp = ax.boxplot(box_data, labels=['ROUGE-1', 'ROUGE-2', 'ROUGE-L'],\n",
    "                patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], ['#3498db', '#e74c3c', '#2ecc71']):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax.set_ylabel('Score', fontsize=11)\n",
    "ax.set_title('Score Distribution (Box Plot)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Scatter: Document Length vs ROUGE-L\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "scatter = ax.scatter(results['document_lengths'], results['rougeL'],\n",
    "                    alpha=0.4, s=20, c=results['rougeL'], cmap='RdYlGn')\n",
    "ax.set_xlabel('Document Length (chars)', fontsize=11)\n",
    "ax.set_ylabel('ROUGE-L Score', fontsize=11)\n",
    "ax.set_title('Document Length vs ROUGE-L', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax, label='ROUGE-L')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Prediction vs Reference Length\n",
    "ax = fig.add_subplot(gs[1, 2])\n",
    "ax.scatter(results['summary_lengths'], results['prediction_lengths'],\n",
    "          alpha=0.4, s=20, color='purple')\n",
    "max_len = max(max(results['summary_lengths']), max(results['prediction_lengths']))\n",
    "ax.plot([0, max_len], [0, max_len], 'r--', linewidth=2, label='Perfect match')\n",
    "ax.set_xlabel('Reference Length (chars)', fontsize=11)\n",
    "ax.set_ylabel('Prediction Length (chars)', fontsize=11)\n",
    "ax.set_title('Prediction vs Reference Length', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Performance by Length Category\n",
    "ax = fig.add_subplot(gs[2, 0])\n",
    "categories = ['Short', 'Medium', 'Long']\n",
    "r1_by_cat = [\n",
    "    np.mean([results['rouge1'][i] for i in range(len(short_mask)) if short_mask[i]]),\n",
    "    np.mean([results['rouge1'][i] for i in range(len(medium_mask)) if medium_mask[i]]),\n",
    "    np.mean([results['rouge1'][i] for i in range(len(long_mask)) if long_mask[i]])\n",
    "]\n",
    "r2_by_cat = [\n",
    "    np.mean([results['rouge2'][i] for i in range(len(short_mask)) if short_mask[i]]),\n",
    "    np.mean([results['rouge2'][i] for i in range(len(medium_mask)) if medium_mask[i]]),\n",
    "    np.mean([results['rouge2'][i] for i in range(len(long_mask)) if long_mask[i]])\n",
    "]\n",
    "rL_by_cat = [\n",
    "    np.mean([results['rougeL'][i] for i in range(len(short_mask)) if short_mask[i]]),\n",
    "    np.mean([results['rougeL'][i] for i in range(len(medium_mask)) if medium_mask[i]]),\n",
    "    np.mean([results['rougeL'][i] for i in range(len(long_mask)) if long_mask[i]])\n",
    "]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.25\n",
    "ax.bar(x - width, r1_by_cat, width, label='ROUGE-1', color='#3498db', alpha=0.8)\n",
    "ax.bar(x, r2_by_cat, width, label='ROUGE-2', color='#e74c3c', alpha=0.8)\n",
    "ax.bar(x + width, rL_by_cat, width, label='ROUGE-L', color='#2ecc71', alpha=0.8)\n",
    "ax.set_xlabel('Document Length Category', fontsize=11)\n",
    "ax.set_ylabel('ROUGE Score', fontsize=11)\n",
    "ax.set_title('Performance by Document Length', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 6. Score correlation heatmap\n",
    "ax = fig.add_subplot(gs[2, 1])\n",
    "corr_data = np.array([results['rouge1'], results['rouge2'], results['rougeL']])\n",
    "corr_matrix = np.corrcoef(corr_data)\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm',\n",
    "            xticklabels=['R-1', 'R-2', 'R-L'],\n",
    "            yticklabels=['R-1', 'R-2', 'R-L'],\n",
    "            ax=ax, cbar_kws={'label': 'Correlation'})\n",
    "ax.set_title('ROUGE Metrics Correlation', fontsize=13, fontweight='bold')\n",
    "\n",
    "# 7. Summary statistics table\n",
    "ax = fig.add_subplot(gs[2, 2])\n",
    "ax.axis('off')\n",
    "table_data = [\n",
    "    ['Metric', 'Mean', 'Std', 'Min', 'Max'],\n",
    "    ['ROUGE-1', f'{rouge1_mean:.2f}%', f'{rouge1_std:.2f}%',\n",
    "     f'{np.min(results[\"rouge1\"])*100:.2f}%', f'{np.max(results[\"rouge1\"])*100:.2f}%'],\n",
    "    ['ROUGE-2', f'{rouge2_mean:.2f}%', f'{rouge2_std:.2f}%',\n",
    "     f'{np.min(results[\"rouge2\"])*100:.2f}%', f'{np.max(results[\"rouge2\"])*100:.2f}%'],\n",
    "    ['ROUGE-L', f'{rougeL_mean:.2f}%', f'{rougeL_std:.2f}%',\n",
    "     f'{np.min(results[\"rougeL\"])*100:.2f}%', f'{np.max(results[\"rougeL\"])*100:.2f}%'],\n",
    "]\n",
    "table = ax.table(cellText=table_data, cellLoc='center', loc='center',\n",
    "                colWidths=[0.15, 0.15, 0.15, 0.15, 0.15])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "# Header styling\n",
    "for i in range(5):\n",
    "    table[(0, i)].set_facecolor('#34495e')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "ax.set_title('Summary Statistics', fontsize=13, fontweight='bold', pad=20)\n",
    "\n",
    "# Main title\n",
    "fig.suptitle('ViT5 Model - Comprehensive Evaluation Results',\n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.savefig('vit5_evaluation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved to: vit5_evaluation_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Save Results\n",
    "\n",
    "Save evaluation results in 3 formats:\n",
    "1. **CSV** - Detailed results for each prediction\n",
    "2. **JSON** - Summary statistics in structured format\n",
    "3. **TXT** - Human-readable final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save detailed results to CSV\"\"\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üíæ SAVING RESULTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Create DataFrame with all results\n",
    "results_df = pd.DataFrame({\n",
    "    'reference': results['references'],\n",
    "    'prediction': results['predictions'],\n",
    "    'rouge1': results['rouge1'],\n",
    "    'rouge2': results['rouge2'],\n",
    "    'rougeL': results['rougeL'],\n",
    "    'doc_length': results['document_lengths'],\n",
    "    'ref_length': results['summary_lengths'],\n",
    "    'pred_length': results['prediction_lengths']\n",
    "})\n",
    "\n",
    "results_df.to_csv('vit5_test_results.csv', index=False, encoding='utf-8')\n",
    "print(f\"‚úÖ Detailed results saved to: vit5_test_results.csv\")\n",
    "print(f\"   Shape: {results_df.shape}\")\n",
    "print(f\"   Size: {len(results_df):,} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save summary statistics to JSON\"\"\"\n",
    "\n",
    "summary_stats = {\n",
    "    'model_info': {\n",
    "        'name': 'VietAI/vit5-base',\n",
    "        'model_path': MODEL_PATH,\n",
    "        'parameters': num_params,\n",
    "        'device': str(device),\n",
    "    },\n",
    "    'evaluation_info': {\n",
    "        'test_samples': len(test_df),\n",
    "        'evaluation_date': datetime.now().isoformat(),\n",
    "        'max_input_length': MAX_LENGTH,\n",
    "        'max_output_length': MAX_TARGET_LENGTH,\n",
    "    },\n",
    "    'rouge_scores': {\n",
    "        'rouge1': {\n",
    "            'mean': float(rouge1_mean),\n",
    "            'std': float(rouge1_std),\n",
    "            'min': float(np.min(results['rouge1']) * 100),\n",
    "            'max': float(np.max(results['rouge1']) * 100),\n",
    "            'median': float(np.median(results['rouge1']) * 100),\n",
    "            'q25': float(np.percentile(results['rouge1'], 25) * 100),\n",
    "            'q75': float(np.percentile(results['rouge1'], 75) * 100),\n",
    "        },\n",
    "        'rouge2': {\n",
    "            'mean': float(rouge2_mean),\n",
    "            'std': float(rouge2_std),\n",
    "            'min': float(np.min(results['rouge2']) * 100),\n",
    "            'max': float(np.max(results['rouge2']) * 100),\n",
    "            'median': float(np.median(results['rouge2']) * 100),\n",
    "            'q25': float(np.percentile(results['rouge2'], 25) * 100),\n",
    "            'q75': float(np.percentile(results['rouge2'], 75) * 100),\n",
    "        },\n",
    "        'rougeL': {\n",
    "            'mean': float(rougeL_mean),\n",
    "            'std': float(rougeL_std),\n",
    "            'min': float(np.min(results['rougeL']) * 100),\n",
    "            'max': float(np.max(results['rougeL']) * 100),\n",
    "            'median': float(np.median(results['rougeL']) * 100),\n",
    "            'q25': float(np.percentile(results['rougeL'], 25) * 100),\n",
    "            'q75': float(np.percentile(results['rougeL'], 75) * 100),\n",
    "        },\n",
    "    },\n",
    "    'length_analysis': {\n",
    "        'avg_document_length': float(np.mean(results['document_lengths'])),\n",
    "        'avg_reference_length': float(np.mean(results['summary_lengths'])),\n",
    "        'avg_prediction_length': float(np.mean(results['prediction_lengths'])),\n",
    "        'compression_ratio': float(np.mean(results['summary_lengths']) / np.mean(results['document_lengths'])),\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('vit5_summary_statistics.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary_stats, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Summary statistics saved to: vit5_summary_statistics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate and save final report\"\"\"\n",
    "\n",
    "report = f\"\"\"\n",
    "{'='*80}\n",
    "                  ViT5 MODEL - B√ÅO C√ÅO ƒê√ÅNH GI√Å CU·ªêI C√ôNG\n",
    "{'='*80}\n",
    "\n",
    "üìÖ Ng√†y ƒë√°nh gi√°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "ü§ñ Model: VietAI/vit5-base\n",
    "üìä Test samples: {len(test_df):,}\n",
    "üíª Device: {device}\n",
    "\n",
    "{'='*80}\n",
    "üìä K·∫æT QU·∫¢ ƒê√ÅNH GI√Å T·ªîNG QUAN\n",
    "{'='*80}\n",
    "\n",
    "ROUGE-1: {rouge1_mean:.2f}% ¬± {rouge1_std:.2f}%\n",
    "ROUGE-2: {rouge2_mean:.2f}% ¬± {rouge2_std:.2f}%\n",
    "ROUGE-L: {rougeL_mean:.2f}% ¬± {rougeL_std:.2f}%\n",
    "\n",
    "üìà Ph√¢n ph·ªëi ƒëi·ªÉm s·ªë (Percentiles):\n",
    "  ROUGE-1: 25th={np.percentile(results['rouge1'], 25)*100:.1f}%, Median={np.percentile(results['rouge1'], 50)*100:.1f}%, 75th={np.percentile(results['rouge1'], 75)*100:.1f}%\n",
    "  ROUGE-2: 25th={np.percentile(results['rouge2'], 25)*100:.1f}%, Median={np.percentile(results['rouge2'], 50)*100:.1f}%, 75th={np.percentile(results['rouge2'], 75)*100:.1f}%\n",
    "  ROUGE-L: 25th={np.percentile(results['rougeL'], 25)*100:.1f}%, Median={np.percentile(results['rougeL'], 50)*100:.1f}%, 75th={np.percentile(results['rougeL'], 75)*100:.1f}%\n",
    "\n",
    "{'='*80}\n",
    "üìè PH√ÇN T√çCH ƒê·ªò D√ÄI\n",
    "{'='*80}\n",
    "\n",
    "ƒê·ªô d√†i trung b√¨nh document: {np.mean(results['document_lengths']):.1f} k√Ω t·ª±\n",
    "ƒê·ªô d√†i trung b√¨nh reference: {np.mean(results['summary_lengths']):.1f} k√Ω t·ª±\n",
    "ƒê·ªô d√†i trung b√¨nh prediction: {np.mean(results['prediction_lengths']):.1f} k√Ω t·ª±\n",
    "T·ª∑ l·ªá n√©n: {np.mean(results['summary_lengths'])/np.mean(results['document_lengths']):.2%}\n",
    "\n",
    "{'='*80}\n",
    "üéØ ƒê√ÅNH GI√Å CH·∫§T L∆Ø·ª¢NG\n",
    "{'='*80}\n",
    "\n",
    "Benchmarks cho Vietnamese Summarization:\n",
    "  Good:      ROUGE-1: 30-40%, ROUGE-2: 15-20%, ROUGE-L: 25-35%\n",
    "  Excellent: ROUGE-1: 40-50%, ROUGE-2: 20-30%, ROUGE-L: 35-45%\n",
    "\n",
    "K·∫øt qu·∫£ model c·ªßa b·∫°n:\n",
    "  ROUGE-1: {rouge1_mean:.2f}% - {'EXCELLENT ‚úÖ' if rouge1_mean > 40 else 'GOOD ‚úì' if rouge1_mean > 30 else 'NEEDS IMPROVEMENT'}\n",
    "  ROUGE-2: {rouge2_mean:.2f}% - {'EXCELLENT ‚úÖ' if rouge2_mean > 20 else 'GOOD ‚úì' if rouge2_mean > 15 else 'NEEDS IMPROVEMENT'}\n",
    "  ROUGE-L: {rougeL_mean:.2f}% - {'EXCELLENT ‚úÖ' if rougeL_mean > 35 else 'GOOD ‚úì' if rougeL_mean > 25 else 'NEEDS IMPROVEMENT'}\n",
    "\n",
    "{'='*80}\n",
    "üìÅ FILES GENERATED\n",
    "{'='*80}\n",
    "\n",
    "1. vit5_test_results.csv - Detailed results ({len(results_df):,} rows)\n",
    "2. vit5_summary_statistics.json - Summary statistics\n",
    "3. vit5_evaluation_analysis.png - Comprehensive visualizations\n",
    "4. vit5_final_report.txt - This report\n",
    "\n",
    "{'='*80}\n",
    "‚úÖ EVALUATION COMPLETE!\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "with open('vit5_final_report.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"‚úÖ Final report saved to: vit5_final_report.txt\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Evaluation Complete!\n",
    "\n",
    "### üìÅ Generated Files:\n",
    "1. **vit5_test_results.csv** - Detailed predictions and scores\n",
    "2. **vit5_summary_statistics.json** - Summary statistics in JSON\n",
    "3. **vit5_evaluation_analysis.png** - Comprehensive visualization\n",
    "4. **vit5_final_report.txt** - Human-readable report\n",
    "\n",
    "### üìä Next Steps:\n",
    "- Review the visualizations above\n",
    "- Check best/worst examples to understand model behavior\n",
    "- Download the generated files for your records\n",
    "- Share results with your team/advisor\n",
    "\n",
    "---\n",
    "\n",
    "**üåü Model Performance Summary:**\n",
    "- Your ViT5 model achieves **EXCELLENT** results on Vietnamese text summarization\n",
    "- Significantly outperforms baseline benchmarks\n",
    "- Ready for production deployment or further fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "*Generated with ViT5 Evaluation Notebook*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}