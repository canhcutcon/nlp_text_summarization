{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Vietnamese Text Summarization - Extractive & Abstractive\n",
    "\n",
    "## T·ªïng quan d·ª± √°n\n",
    "\n",
    "**M·ª•c ti√™u**: So s√°nh 2 ph∆∞∆°ng ph√°p t√≥m t·∫Øt vƒÉn b·∫£n ti·∫øng Vi·ªát\n",
    "- **Extractive**: Ch·ªçn l·ªçc c√¢u c√≥ s·∫µn (PhoBERT + TextRank)\n",
    "- **Abstractive**: Sinh c√¢u m·ªõi ho√†n to√†n (ViT5)\n",
    "\n",
    "**Dataset**: VLSP 2021 Summarization Task\n",
    "\n",
    "---\n",
    "\n",
    "## üìã N·ªôi dung\n",
    "\n",
    "1. **L√Ω thuy·∫øt Text Summarization**\n",
    "2. **Setup & Load Data**\n",
    "3. **Extractive Summarization**\n",
    "   - TextRank Algorithm\n",
    "   - PhoBERT Sentence Scoring\n",
    "4. **Abstractive Summarization**\n",
    "   - ViT5 Fine-tuning\n",
    "   - Generation Strategies\n",
    "5. **Evaluation & Comparison**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ L√ù THUY·∫æT: EXTRACTIVE vs ABSTRACTIVE\n",
    "\n",
    "### A. Extractive Summarization üìã\n",
    "\n",
    "**ƒê·ªãnh nghƒ©a**: Ch·ªçn l·ªçc c√°c c√¢u quan tr·ªçng t·ª´ vƒÉn b·∫£n g·ªëc\n",
    "\n",
    "```\n",
    "Input (3 c√¢u):\n",
    "  [1] \"Vi·ªát Nam l√† m·ªôt ƒë·∫•t n∆∞·ªõc ƒë√¥ng d√¢n ·ªü ƒê√¥ng Nam √Å.\"\n",
    "  [2] \"N·ªÅn kinh t·∫ø ph√°t tri·ªÉn nhanh v·ªõi t·ªëc ƒë·ªô tƒÉng tr∆∞·ªüng cao.\"\n",
    "  [3] \"VƒÉn h√≥a ƒëa d·∫°ng v·ªõi 54 d√¢n t·ªôc.\"\n",
    "\n",
    "Extractive Output:\n",
    "  ‚Üì Ch·ªçn c√¢u [1] v√† [2] (score cao nh·∫•t)\n",
    "  \"Vi·ªát Nam l√† m·ªôt ƒë·∫•t n∆∞·ªõc ƒë√¥ng d√¢n ·ªü ƒê√¥ng Nam √Å. N·ªÅn kinh t·∫ø ph√°t tri·ªÉn nhanh.\"\n",
    "```\n",
    "\n",
    "**Ph∆∞∆°ng ph√°p:**\n",
    "\n",
    "1. **TextRank** (Graph-based)\n",
    "   - X√¢y d·ª±ng graph v·ªõi nodes = c√¢u\n",
    "   - Edges = ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa c√°c c√¢u\n",
    "   - Ch·∫°y PageRank algorithm\n",
    "   - Ch·ªçn top-k c√¢u c√≥ score cao\n",
    "**∆Øu ƒëi·ªÉm:**\n",
    "- ‚úÖ Ng·ªØ ph√°p chu·∫©n (d√πng c√¢u g·ªëc)\n",
    "- ‚úÖ Kh√¥ng t·∫°o th√¥ng tin sai (no hallucination)\n",
    "- ‚úÖ Training ƒë∆°n gi·∫£n, √≠t resource\n",
    "- ‚úÖ Nhanh, d·ªÖ deploy\n",
    "\n",
    "**Nh∆∞·ª£c ƒëi·ªÉm:**\n",
    "- ‚ùå T√≠nh m·∫°ch l·∫°c kh√¥ng cao\n",
    "- ‚ùå Kh√¥ng th·ªÉ paraphrase\n",
    "- ‚ùå K√©m linh ho·∫°t v·ªÅ ƒë·ªô d√†i\n",
    "- ‚ùå C√≥ th·ªÉ ch·ª©a th√¥ng tin th·ª´a\n",
    "\n",
    "---\n",
    "\n",
    "### B. Abstractive Summarization ü§ñ\n",
    "\n",
    "**ƒê·ªãnh nghƒ©a**: Sinh c√¢u m·ªõi ho√†n to√†n, gi·ªëng c√°ch con ng∆∞·ªùi t√≥m t·∫Øt\n",
    "\n",
    "```\n",
    "Input (3 c√¢u):\n",
    "  \"Vi·ªát Nam l√† m·ªôt ƒë·∫•t n∆∞·ªõc ƒë√¥ng d√¢n ·ªü ƒê√¥ng Nam √Å.\"\n",
    "  \"N·ªÅn kinh t·∫ø ph√°t tri·ªÉn nhanh v·ªõi t·ªëc ƒë·ªô tƒÉng tr∆∞·ªüng cao.\"\n",
    "  \"VƒÉn h√≥a ƒëa d·∫°ng v·ªõi 54 d√¢n t·ªôc.\"\n",
    "\n",
    "Abstractive Output:\n",
    "  ‚Üì T·∫°o c√¢u m·ªõi, t·ªïng h·ª£p th√¥ng tin\n",
    "  \"Vi·ªát Nam c√≥ d√¢n s·ªë ƒë√¥ng, n·ªÅn kinh t·∫ø tƒÉng tr∆∞·ªüng nhanh v√† vƒÉn h√≥a ƒëa d·∫°ng.\"\n",
    "```\n",
    "\n",
    "**Ph∆∞∆°ng ph√°p:**\n",
    "\n",
    "**ViT5** (Vietnamese T5)\n",
    "- Encoder-Decoder Transformer\n",
    "- Pre-trained tr√™n Vietnamese corpus\n",
    "- Generation v·ªõi Beam Search/Sampling\n",
    "\n",
    "```\n",
    "Document ‚Üí [Encoder] ‚Üí Context ‚Üí [Decoder] ‚Üí Summary\n",
    "             ViT5-enc             ViT5-dec\n",
    "                        ‚Üì\n",
    "                 Attention weights\n",
    "```\n",
    "\n",
    "**∆Øu ƒëi·ªÉm:**\n",
    "- ‚úÖ T·ª± nhi√™n, m·∫°ch l·∫°c nh∆∞ ng∆∞·ªùi vi·∫øt\n",
    "- ‚úÖ C√≥ th·ªÉ paraphrase, t·ªïng h·ª£p th√¥ng tin\n",
    "- ‚úÖ Linh ho·∫°t v·ªÅ ƒë·ªô d√†i v√† style\n",
    "- ‚úÖ Lo·∫°i b·ªè th√¥ng tin th·ª´a t·ªët h∆°n\n",
    "\n",
    "**Nh∆∞·ª£c ƒëi·ªÉm:**\n",
    "- ‚ùå D·ªÖ g·∫∑p l·ªói logic, hallucination\n",
    "- ‚ùå C·∫ßn nhi·ªÅu t√†i nguy√™n training\n",
    "- ‚ùå Ph·ª©c t·∫°p, kh√≥ debug\n",
    "- ‚ùå Ng·ªØ ph√°p c√≥ th·ªÉ sai\n",
    "\n",
    "---\n",
    "\n",
    "### C. So s√°nh tr·ª±c quan\n",
    "\n",
    "| Ti√™u ch√≠ | Extractive | Abstractive |\n",
    "|----------|------------|-------------|\n",
    "| **Ng·ªØ ph√°p** | ‚úÖ Lu√¥n ƒë√∫ng | ‚ö†Ô∏è C√≥ th·ªÉ sai |\n",
    "| **M·∫°ch l·∫°c** | ‚ö†Ô∏è Trung b√¨nh | ‚úÖ T·ªët |\n",
    "| **T·ª± nhi√™n** | ‚ö†Ô∏è C·ª©ng nh·∫Øc | ‚úÖ Nh∆∞ ng∆∞·ªùi vi·∫øt |\n",
    "| **Hallucination** | ‚úÖ Kh√¥ng c√≥ | ‚ùå C√≥ th·ªÉ x·∫£y ra |\n",
    "| **Training** | ‚úÖ ƒê∆°n gi·∫£n | ‚ùå Ph·ª©c t·∫°p |\n",
    "| **Speed** | ‚úÖ Nhanh | ‚ö†Ô∏è Ch·∫≠m h∆°n |\n",
    "| **Flexibility** | ‚ùå Th·∫•p | ‚úÖ Cao |\n",
    "\n",
    "**K·∫øt lu·∫≠n:**\n",
    "- **Extractive**: T·ªët cho tin t·ª©c, b√°o c√°o, c·∫ßn ƒë·ªô ch√≠nh x√°c cao\n",
    "- **Abstractive**: T·ªët cho t√≥m t·∫Øt d√†i, content marketing, creative writing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ SETUP ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Install Required Packages\n",
    "# ============================================================================\n",
    "!pip install -q transformers>=4.50.0 accelerate>=0.26.0 datasets>=2.14.6\n",
    "!pip install -q sentencepiece>=0.1.99 rouge-score>=0.1.2 evaluate>=0.4.1\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q underthesea  # For Vietnamese text processing\n",
    "!pip install -q scikit-learn networkx  # For TextRank\n",
    "\n",
    "print(\"‚úÖ All packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Import Libraries\n",
    "# ============================================================================\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Text processing\n",
    "from underthesea import sent_tokenize\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "from rouge_score import rouge_scorer\n",
    "import evaluate\n",
    "\n",
    "# Graph algorithms\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ LOAD & EXPLORE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Load VLSP 2021 Dataset\n",
    "# ============================================================================\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading VLSP 2021 Summarization dataset...\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"Linh2703/VLSP2021_summarization\")\n",
    "\n",
    "# Convert to pandas for easier manipulation\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "val_df = pd.DataFrame(dataset['validation'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"  Train: {len(train_df):,} samples\")\n",
    "print(f\"  Validation: {len(val_df):,} samples\")\n",
    "print(f\"  Test: {len(test_df):,} samples\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nüìÑ Sample data:\")\n",
    "sample = train_df.iloc[0]\n",
    "print(f\"\\nDocument (first 200 chars):\\n{sample['document'][:200]}...\")\n",
    "print(f\"\\nSummary:\\n{sample['summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Data Statistics\n",
    "# ============================================================================\n",
    "def analyze_text_lengths(df: pd.DataFrame, name: str):\n",
    "    \"\"\"Analyze document and summary lengths\"\"\"\n",
    "    doc_words = df['document'].apply(lambda x: len(x.split()))\n",
    "    sum_words = df['summary'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    doc_sents = df['document'].apply(lambda x: len(sent_tokenize(x)))\n",
    "    sum_sents = df['summary'].apply(lambda x: len(sent_tokenize(x)))\n",
    "    \n",
    "    print(f\"\\n{name} Statistics:\")\n",
    "    print(f\"  Document words: mean={doc_words.mean():.1f}, median={doc_words.median():.1f}\")\n",
    "    print(f\"  Summary words: mean={sum_words.mean():.1f}, median={sum_words.median():.1f}\")\n",
    "    print(f\"  Document sentences: mean={doc_sents.mean():.1f}, median={doc_sents.median():.1f}\")\n",
    "    print(f\"  Summary sentences: mean={sum_sents.mean():.1f}, median={sum_sents.median():.1f}\")\n",
    "    print(f\"  Compression ratio: {(sum_words.mean() / doc_words.mean() * 100):.1f}%\")\n",
    "\n",
    "analyze_text_lengths(train_df, \"Train\")\n",
    "analyze_text_lengths(val_df, \"Validation\")\n",
    "analyze_text_lengths(test_df, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ EXTRACTIVE SUMMARIZATION\n",
    "\n",
    "### Method 1: TextRank (Graph-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TextRank Implementation\n",
    "# ============================================================================\n",
    "class TextRankSummarizer:\n",
    "    \"\"\"TextRank algorithm for extractive summarization\"\"\"\n",
    "    \n",
    "    def __init__(self, top_n: int = 3, damping: float = 0.85):\n",
    "        self.top_n = top_n\n",
    "        self.damping = damping\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base')\n",
    "        self.model = AutoModel.from_pretrained('vinai/phobert-base')\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def get_sentence_embedding(self, sentence: str) -> np.ndarray:\n",
    "        \"\"\"Get PhoBERT embedding for a sentence\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            sentence, \n",
    "            return_tensors='pt', \n",
    "            truncation=True, \n",
    "            max_length=256\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            # Use CLS token embedding\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        \n",
    "        return embedding[0]\n",
    "    \n",
    "    def build_similarity_matrix(self, sentences: List[str]) -> np.ndarray:\n",
    "        \"\"\"Build similarity matrix between sentences\"\"\"\n",
    "        print(f\"  Computing embeddings for {len(sentences)} sentences...\")\n",
    "        embeddings = []\n",
    "        \n",
    "        for sent in tqdm(sentences, desc=\"Encoding\"):\n",
    "            emb = self.get_sentence_embedding(sent)\n",
    "            embeddings.append(emb)\n",
    "        \n",
    "        embeddings = np.array(embeddings)\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def textrank(self, similarity_matrix: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Run TextRank algorithm (PageRank on sentence graph)\"\"\"\n",
    "        # Create graph from similarity matrix\n",
    "        nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "        \n",
    "        # Compute PageRank scores\n",
    "        scores = nx.pagerank(nx_graph, alpha=self.damping)\n",
    "        \n",
    "        return np.array(list(scores.values()))\n",
    "    \n",
    "    def summarize(self, document: str, num_sentences: int = None) -> str:\n",
    "        \"\"\"Generate extractive summary using TextRank\"\"\"\n",
    "        if num_sentences is None:\n",
    "            num_sentences = self.top_n\n",
    "        \n",
    "        # Split into sentences\n",
    "        sentences = sent_tokenize(document)\n",
    "        \n",
    "        if len(sentences) <= num_sentences:\n",
    "            return document\n",
    "        \n",
    "        # Build similarity matrix\n",
    "        similarity_matrix = self.build_similarity_matrix(sentences)\n",
    "        \n",
    "        # Run TextRank\n",
    "        scores = self.textrank(similarity_matrix)\n",
    "        \n",
    "        # Select top sentences\n",
    "        ranked_indices = np.argsort(scores)[::-1][:num_sentences]\n",
    "        \n",
    "        # Sort by original order to maintain coherence\n",
    "        ranked_indices = sorted(ranked_indices)\n",
    "        \n",
    "        # Extract summary\n",
    "        summary_sentences = [sentences[i] for i in ranked_indices]\n",
    "        summary = ' '.join(summary_sentences)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "print(\"‚úÖ TextRank Summarizer created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: PhoBERT Sentence Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PhoBERT Sentence Scorer\n",
    "# ============================================================================\n",
    "class PhoBERTExtractor:\n",
    "    \"\"\"Extractive summarization using PhoBERT sentence scoring\"\"\"\n",
    "    \n",
    "    def __init__(self, top_n: int = 3):\n",
    "        self.top_n = top_n\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base')\n",
    "        self.model = AutoModel.from_pretrained('vinai/phobert-base')\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Get PhoBERT embedding\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            return_tensors='pt', \n",
    "            truncation=True, \n",
    "            max_length=256\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        \n",
    "        return embedding[0]\n",
    "    \n",
    "    def score_sentences(self, document: str) -> List[Tuple[int, float, str]]:\n",
    "        \"\"\"Score sentences by similarity to document\"\"\"\n",
    "        # Split into sentences\n",
    "        sentences = sent_tokenize(document)\n",
    "        \n",
    "        # Get document embedding\n",
    "        doc_embedding = self.get_embedding(document)\n",
    "        \n",
    "        # Score each sentence\n",
    "        scores = []\n",
    "        for idx, sent in enumerate(sentences):\n",
    "            sent_embedding = self.get_embedding(sent)\n",
    "            \n",
    "            # Cosine similarity\n",
    "            similarity = cosine_similarity(\n",
    "                doc_embedding.reshape(1, -1),\n",
    "                sent_embedding.reshape(1, -1)\n",
    "            )[0][0]\n",
    "            \n",
    "            # Add position bias (earlier sentences are more important)\n",
    "            position_weight = 1.0 - (idx / len(sentences)) * 0.3\n",
    "            final_score = similarity * position_weight\n",
    "            \n",
    "            scores.append((idx, final_score, sent))\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def summarize(self, document: str, num_sentences: int = None) -> str:\n",
    "        \"\"\"Generate extractive summary\"\"\"\n",
    "        if num_sentences is None:\n",
    "            num_sentences = self.top_n\n",
    "        \n",
    "        sentences = sent_tokenize(document)\n",
    "        \n",
    "        if len(sentences) <= num_sentences:\n",
    "            return document\n",
    "        \n",
    "        # Score sentences\n",
    "        scores = self.score_sentences(document)\n",
    "        \n",
    "        # Sort by score and select top\n",
    "        top_sentences = sorted(scores, key=lambda x: x[1], reverse=True)[:num_sentences]\n",
    "        \n",
    "        # Sort by original position\n",
    "        top_sentences = sorted(top_sentences, key=lambda x: x[0])\n",
    "        \n",
    "        # Extract summary\n",
    "        summary = ' '.join([sent[2] for sent in top_sentences])\n",
    "        \n",
    "        return summary\n",
    "\n",
    "print(\"‚úÖ PhoBERT Extractor created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Extractive Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Test Extractive Summarization\n",
    "# ============================================================================\n",
    "# Sample document\n",
    "sample_doc = train_df.iloc[0]['document']\n",
    "sample_ref = train_df.iloc[0]['summary']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXTRACTIVE SUMMARIZATION TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìÑ Original Document:\")\n",
    "print(sample_doc[:500] + \"...\")\n",
    "\n",
    "print(\"\\nüìù Reference Summary:\")\n",
    "print(sample_ref)\n",
    "\n",
    "# Method 1: PhoBERT Extractor (faster)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Method 1: PhoBERT Sentence Scoring\")\n",
    "print(\"=\"*80)\n",
    "phobert_extractor = PhoBERTExtractor(top_n=3)\n",
    "phobert_summary = phobert_extractor.summarize(sample_doc)\n",
    "print(\"\\n‚úÖ PhoBERT Summary:\")\n",
    "print(phobert_summary)\n",
    "\n",
    "# Method 2: TextRank (more sophisticated but slower)\n",
    "# Uncomment if you want to test TextRank\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"Method 2: TextRank Algorithm\")\n",
    "# print(\"=\"*80)\n",
    "# textrank_summarizer = TextRankSummarizer(top_n=3)\n",
    "# textrank_summary = textrank_summarizer.summarize(sample_doc)\n",
    "# print(\"\\n‚úÖ TextRank Summary:\")\n",
    "# print(textrank_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ ABSTRACTIVE SUMMARIZATION WITH ViT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ViT5 Configuration\n",
    "# ============================================================================\n",
    "VIT5_MODEL_NAME = 'VietAI/vit5-base'\n",
    "MAX_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 128\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 5e-5\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ViT5 MODEL CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {VIT5_MODEL_NAME}\")\n",
    "print(f\"Max input length: {MAX_LENGTH}\")\n",
    "print(f\"Max output length: {MAX_TARGET_LENGTH}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Load ViT5 Model & Tokenizer\n",
    "# ============================================================================\n",
    "print(\"Loading ViT5 model...\")\n",
    "\n",
    "vit5_tokenizer = AutoTokenizer.from_pretrained(VIT5_MODEL_NAME)\n",
    "vit5_model = AutoModelForSeq2SeqLM.from_pretrained(VIT5_MODEL_NAME)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vit5_model.to(device)\n",
    "\n",
    "print(f\"\\n‚úÖ ViT5 Model loaded successfully!\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in vit5_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Prepare Dataset for ViT5\n",
    "# ============================================================================\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize documents and summaries\"\"\"\n",
    "    inputs = [\"summarize: \" + doc for doc in examples['document']]\n",
    "    \n",
    "    model_inputs = vit5_tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = vit5_tokenizer(\n",
    "        examples['summary'],\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "print(\"Preparing datasets...\")\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train = dataset['train'].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['document', 'summary']\n",
    ")\n",
    "\n",
    "tokenized_val = dataset['validation'].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['document', 'summary']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Datasets prepared!\")\n",
    "print(f\"  Train: {len(tokenized_train)} samples\")\n",
    "print(f\"  Validation: {len(tokenized_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Training Arguments & Trainer\n",
    "# ============================================================================\n",
    "rouge_metric = evaluate.load('rouge')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute ROUGE metrics\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Decode predictions\n",
    "    decoded_preds = vit5_tokenizer.batch_decode(\n",
    "        predictions, \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # Replace -100 in labels (padding)\n",
    "    labels = np.where(labels != -100, labels, vit5_tokenizer.pad_token_id)\n",
    "    decoded_labels = vit5_tokenizer.batch_decode(\n",
    "        labels, \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # Compute ROUGE\n",
    "    result = rouge_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'rouge1': round(result['rouge1'] * 100, 2),\n",
    "        'rouge2': round(result['rouge2'] * 100, 2),\n",
    "        'rougeL': round(result['rougeL'] * 100, 2)\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./vit5_summarization\",\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    \n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    fp16=torch.cuda.is_available(),\n",
    "    \n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    "    generation_num_beams=4,\n",
    "    \n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    vit5_tokenizer,\n",
    "    model=vit5_model,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=vit5_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=vit5_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ViT5 Trainer initialized!\")\n",
    "print(\"\\n‚ö†Ô∏è  To train: trainer.train()\")\n",
    "print(\"‚ö†Ô∏è  This will take several hours!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Train ViT5 Model (OPTIONAL - takes hours)\n",
    "# ============================================================================\n",
    "# Uncomment to train\n",
    "# print(\"Starting training...\")\n",
    "# trainer.train()\n",
    "# print(\"‚úÖ Training completed!\")\n",
    "\n",
    "# Save model\n",
    "# trainer.save_model(\"./vit5_final_model\")\n",
    "# vit5_tokenizer.save_pretrained(\"./vit5_final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ViT5 Inference Function\n",
    "# ============================================================================\n",
    "def generate_abstractive_summary(\n",
    "    document: str,\n",
    "    max_length: int = 128,\n",
    "    num_beams: int = 4,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.9\n",
    ") -> str:\n",
    "    \"\"\"Generate abstractive summary using ViT5\"\"\"\n",
    "    \n",
    "    # Prepare input\n",
    "    input_text = f\"summarize: {document}\"\n",
    "    inputs = vit5_tokenizer(\n",
    "        input_text,\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = vit5_model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,\n",
    "            length_penalty=1.0\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    summary = vit5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "print(\"‚úÖ ViT5 inference function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Test Abstractive Summarization\n",
    "# ============================================================================\n",
    "sample_doc = train_df.iloc[0]['document']\n",
    "sample_ref = train_df.iloc[0]['summary']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ABSTRACTIVE SUMMARIZATION TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìÑ Original Document:\")\n",
    "print(sample_doc[:500] + \"...\")\n",
    "\n",
    "print(\"\\nüìù Reference Summary:\")\n",
    "print(sample_ref)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ViT5 Generation Strategies\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Strategy 1: Beam Search (most common)\n",
    "print(\"\\n1Ô∏è‚É£ Beam Search (num_beams=4):\")\n",
    "beam_summary = generate_abstractive_summary(\n",
    "    sample_doc,\n",
    "    num_beams=4,\n",
    "    temperature=1.0\n",
    ")\n",
    "print(beam_summary)\n",
    "\n",
    "# Strategy 2: Sampling with temperature\n",
    "print(\"\\n2Ô∏è‚É£ Sampling (temperature=0.7, top_p=0.9):\")\n",
    "sampling_summary = generate_abstractive_summary(\n",
    "    sample_doc,\n",
    "    num_beams=1,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "print(sampling_summary)\n",
    "\n",
    "# Strategy 3: Greedy (deterministic)\n",
    "print(\"\\n3Ô∏è‚É£ Greedy Decoding (num_beams=1):\")\n",
    "greedy_summary = generate_abstractive_summary(\n",
    "    sample_doc,\n",
    "    num_beams=1,\n",
    "    temperature=1.0\n",
    ")\n",
    "print(greedy_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ EVALUATION & COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ROUGE Evaluation Function\n",
    "# ============================================================================\n",
    "def evaluate_rouge(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"Calculate ROUGE scores\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(\n",
    "        ['rouge1', 'rouge2', 'rougeL'],\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    \n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        scores = scorer.score(ref, pred)\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': np.mean(rouge1_scores) * 100,\n",
    "        'rouge2': np.mean(rouge2_scores) * 100,\n",
    "        'rougeL': np.mean(rougeL_scores) * 100\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Evaluation function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Compare Extractive vs Abstractive on Test Set\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use subset for quick evaluation\n",
    "test_subset = test_df.head(50)  # Adjust size as needed\n",
    "\n",
    "print(f\"\\nEvaluating {len(test_subset)} samples...\\n\")\n",
    "\n",
    "# Initialize summarizers\n",
    "phobert_extractor = PhoBERTExtractor(top_n=3)\n",
    "\n",
    "# Store results\n",
    "extractive_summaries = []\n",
    "abstractive_summaries = []\n",
    "references = test_subset['summary'].tolist()\n",
    "\n",
    "# Generate summaries\n",
    "for idx, row in tqdm(test_subset.iterrows(), total=len(test_subset), desc=\"Generating summaries\"):\n",
    "    doc = row['document']\n",
    "    \n",
    "    # Extractive\n",
    "    ext_summary = phobert_extractor.summarize(doc)\n",
    "    extractive_summaries.append(ext_summary)\n",
    "    \n",
    "    # Abstractive\n",
    "    abs_summary = generate_abstractive_summary(doc)\n",
    "    abstractive_summaries.append(abs_summary)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ROUGE SCORES COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "extractive_scores = evaluate_rouge(extractive_summaries, references)\n",
    "abstractive_scores = evaluate_rouge(abstractive_summaries, references)\n",
    "\n",
    "print(\"\\nüìä Extractive Summarization (PhoBERT):\")\n",
    "print(f\"  ROUGE-1: {extractive_scores['rouge1']:.2f}%\")\n",
    "print(f\"  ROUGE-2: {extractive_scores['rouge2']:.2f}%\")\n",
    "print(f\"  ROUGE-L: {extractive_scores['rougeL']:.2f}%\")\n",
    "\n",
    "print(\"\\nü§ñ Abstractive Summarization (ViT5):\")\n",
    "print(f\"  ROUGE-1: {abstractive_scores['rouge1']:.2f}%\")\n",
    "print(f\"  ROUGE-2: {abstractive_scores['rouge2']:.2f}%\")\n",
    "print(f\"  ROUGE-L: {abstractive_scores['rougeL']:.2f}%\")\n",
    "\n",
    "# Compare\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "r1_diff = abstractive_scores['rouge1'] - extractive_scores['rouge1']\n",
    "r2_diff = abstractive_scores['rouge2'] - extractive_scores['rouge2']\n",
    "rL_diff = abstractive_scores['rougeL'] - extractive_scores['rougeL']\n",
    "\n",
    "print(f\"\\nROUGE-1 difference: {r1_diff:+.2f}% {'(Abstractive wins)' if r1_diff > 0 else '(Extractive wins)'}\")\n",
    "print(f\"ROUGE-2 difference: {r2_diff:+.2f}% {'(Abstractive wins)' if r2_diff > 0 else '(Extractive wins)'}\")\n",
    "print(f\"ROUGE-L difference: {rL_diff:+.2f}% {'(Abstractive wins)' if rL_diff > 0 else '(Extractive wins)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Qualitative Comparison\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"QUALITATIVE ANALYSIS - Side by Side Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show 3 examples\n",
    "for i in range(min(3, len(test_subset))):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    doc = test_subset.iloc[i]['document']\n",
    "    ref = test_subset.iloc[i]['summary']\n",
    "    ext = extractive_summaries[i]\n",
    "    abs_ = abstractive_summaries[i]\n",
    "    \n",
    "    print(f\"\\nüìÑ Document (first 300 chars):\\n{doc[:300]}...\\n\")\n",
    "    \n",
    "    print(f\"üìù Reference Summary:\\n{ref}\\n\")\n",
    "    \n",
    "    print(f\"üìã Extractive Summary (PhoBERT):\\n{ext}\\n\")\n",
    "    \n",
    "    print(f\"ü§ñ Abstractive Summary (ViT5):\\n{abs_}\\n\")\n",
    "    \n",
    "    # Compare characteristics\n",
    "    print(f\"üìä Statistics:\")\n",
    "    print(f\"  Reference: {len(ref.split())} words, {len(sent_tokenize(ref))} sentences\")\n",
    "    print(f\"  Extractive: {len(ext.split())} words, {len(sent_tokenize(ext))} sentences\")\n",
    "    print(f\"  Abstractive: {len(abs_.split())} words, {len(sent_tokenize(abs_))} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ FINAL ANALYSIS & INSIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Summary Statistics\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL ANALYSIS: EXTRACTIVE vs ABSTRACTIVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Length analysis\n",
    "ext_lengths = [len(s.split()) for s in extractive_summaries]\n",
    "abs_lengths = [len(s.split()) for s in abstractive_summaries]\n",
    "ref_lengths = [len(s.split()) for s in references]\n",
    "\n",
    "print(\"\\nüìè Summary Length Analysis:\")\n",
    "print(f\"  Reference: {np.mean(ref_lengths):.1f} ¬± {np.std(ref_lengths):.1f} words\")\n",
    "print(f\"  Extractive: {np.mean(ext_lengths):.1f} ¬± {np.std(ext_lengths):.1f} words\")\n",
    "print(f\"  Abstractive: {np.mean(abs_lengths):.1f} ¬± {np.std(abs_lengths):.1f} words\")\n",
    "\n",
    "# Sentence count\n",
    "ext_sents = [len(sent_tokenize(s)) for s in extractive_summaries]\n",
    "abs_sents = [len(sent_tokenize(s)) for s in abstractive_summaries]\n",
    "ref_sents = [len(sent_tokenize(s)) for s in references]\n",
    "\n",
    "print(\"\\nüìù Sentence Count:\")\n",
    "print(f\"  Reference: {np.mean(ref_sents):.1f} sentences\")\n",
    "print(f\"  Extractive: {np.mean(ext_sents):.1f} sentences\")\n",
    "print(f\"  Abstractive: {np.mean(abs_sents):.1f} sentences\")\n",
    "\n",
    "# Final recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "1Ô∏è‚É£ EXTRACTIVE SUMMARIZATION:\n",
    "   ‚úÖ ∆Øu ƒëi·ªÉm:\n",
    "      - Ng·ªØ ph√°p lu√¥n ch√≠nh x√°c (d√πng c√¢u g·ªëc)\n",
    "      - Kh√¥ng t·∫°o th√¥ng tin sai\n",
    "      - Training ƒë∆°n gi·∫£n, deploy d·ªÖ d√†ng\n",
    "      - T·ªëc ƒë·ªô nhanh\n",
    "   \n",
    "   ‚ùå Nh∆∞·ª£c ƒëi·ªÉm:\n",
    "      - K√©m t·ª± nhi√™n, c√≥ th·ªÉ thi·∫øu m·∫°ch l·∫°c\n",
    "      - Kh√¥ng th·ªÉ t·ªïng h·ª£p th√¥ng tin\n",
    "      - ƒê·ªô d√†i kh√≥ ki·ªÉm so√°t\n",
    "   \n",
    "   üí° Ph√π h·ª£p cho:\n",
    "      - Tin t·ª©c, b√°o c√°o ch√≠nh th·ª©c\n",
    "      - T√†i li·ªáu k·ªπ thu·∫≠t\n",
    "      - Khi c·∫ßn ƒë·ªô ch√≠nh x√°c cao\n",
    "\n",
    "2Ô∏è‚É£ ABSTRACTIVE SUMMARIZATION:\n",
    "   ‚úÖ ∆Øu ƒëi·ªÉm:\n",
    "      - T·ª± nhi√™n, m·∫°ch l·∫°c\n",
    "      - C√≥ th·ªÉ t·ªïng h·ª£p v√† paraphrase\n",
    "      - Linh ho·∫°t v·ªÅ ƒë·ªô d√†i\n",
    "      - Quality cao h∆°n khi fine-tuned t·ªët\n",
    "   \n",
    "   ‚ùå Nh∆∞·ª£c ƒëi·ªÉm:\n",
    "      - C√≥ th·ªÉ t·∫°o th√¥ng tin sai (hallucination)\n",
    "      - C·∫ßn nhi·ªÅu resource ƒë·ªÉ train\n",
    "      - Ph·ª©c t·∫°p h∆°n\n",
    "   \n",
    "   üí° Ph√π h·ª£p cho:\n",
    "      - Content marketing\n",
    "      - T√≥m t·∫Øt d√†i, creative writing\n",
    "      - Khi c·∫ßn summary t·ª± nhi√™n\n",
    "\n",
    "3Ô∏è‚É£ HYBRID APPROACH (Khuy·∫øn ngh·ªã):\n",
    "   üí° K·∫øt h·ª£p 2 ph∆∞∆°ng ph√°p:\n",
    "      - D√πng Extractive ƒë·ªÉ ch·ªçn th√¥ng tin quan tr·ªçng\n",
    "      - D√πng Abstractive ƒë·ªÉ paraphrase v√† l√†m m∆∞·ª£t\n",
    "      - K·∫øt qu·∫£: V·ª´a ch√≠nh x√°c v·ª´a t·ª± nhi√™n\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT COMPLETED! üéâ\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
