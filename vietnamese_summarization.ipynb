{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“š Vietnamese Text Summarization vá»›i mT5, ViT5\n",
    "\n",
    "## Tá»•ng quan dá»± Ã¡n\n",
    "\n",
    "**Má»¥c tiÃªu**: Fine-tune models transformer cho tÃ³m táº¯t vÄƒn báº£n tiáº¿ng Viá»‡t  \n",
    "**Dataset**: VLSP 2021 Summarization Task  \n",
    "**Models**: PhoBERT (extractive), mT5 (abstractive), ViT5 (abstractive)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Ná»™i dung\n",
    "\n",
    "1. **LÃ½ thuyáº¿t Text Summarization**\n",
    "2. **Setup Environment & Load Data**\n",
    "3. **Data Preprocessing & EDA**\n",
    "4. **Model Implementation**\n",
    "   - PhoBERT (Extractive)\n",
    "   - mT5 (Abstractive)\n",
    "   - ViT5 (Abstractive)\n",
    "5. **Training & Fine-tuning**\n",
    "6. **Evaluation vá»›i ROUGE metrics**\n",
    "7. **Model Comparison & Analysis**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ LÃ THUYáº¾T TEXT SUMMARIZATION\n",
    "\n",
    "### A. Hai loáº¡i Text Summarization\n",
    "\n",
    "#### **Extractive Summarization** (PhoBERT)\n",
    "\n",
    "```\n",
    "Input: \"Viá»‡t Nam lÃ  má»™t Ä‘áº¥t nÆ°á»›c Ä‘Ã´ng dÃ¢n. Ná»n kinh táº¿ phÃ¡t triá»ƒn nhanh. \n",
    "        VÄƒn hÃ³a Ä‘a dáº¡ng vÃ  phong phÃº.\"\n",
    "\n",
    "Extractive: \"Viá»‡t Nam lÃ  má»™t Ä‘áº¥t nÆ°á»›c Ä‘Ã´ng dÃ¢n. Ná»n kinh táº¿ phÃ¡t triá»ƒn nhanh.\"\n",
    "             â†‘ Chá»n cÃ¢u quan trá»ng tá»« vÄƒn báº£n gá»‘c\n",
    "```\n",
    "\n",
    "**CÃ¡ch hoáº¡t Ä‘á»™ng:**\n",
    "- Cháº¥m Ä‘iá»ƒm tá»«ng cÃ¢u trong vÄƒn báº£n (sentence scoring)\n",
    "- Chá»n top-k cÃ¢u cÃ³ Ä‘iá»ƒm cao nháº¥t\n",
    "- Sáº¯p xáº¿p láº¡i theo thá»© tá»± xuáº¥t hiá»‡n\n",
    "\n",
    "**Æ¯u Ä‘iá»ƒm:**\n",
    "- âœ… Äáº£m báº£o ngá»¯ phÃ¡p chÃ­nh xÃ¡c (vÃ¬ dÃ¹ng cÃ¢u gá»‘c)\n",
    "- âœ… KhÃ´ng táº¡o thÃ´ng tin sai (no hallucination)\n",
    "- âœ… Training nhanh, Ã­t resource\n",
    "\n",
    "**NhÆ°á»£c Ä‘iá»ƒm:**\n",
    "- âŒ Thiáº¿u tÃ­nh linh hoáº¡t\n",
    "- âŒ CÃ³ thá»ƒ khÃ´ng máº¡ch láº¡c\n",
    "- âŒ KhÃ´ng thá»ƒ paraphrase\n",
    "\n",
    "---\n",
    "\n",
    "#### **Abstractive Summarization** (mT5, ViT5)\n",
    "\n",
    "```\n",
    "Input: \"Viá»‡t Nam lÃ  má»™t Ä‘áº¥t nÆ°á»›c Ä‘Ã´ng dÃ¢n. Ná»n kinh táº¿ phÃ¡t triá»ƒn nhanh. \n",
    "        VÄƒn hÃ³a Ä‘a dáº¡ng vÃ  phong phÃº.\"\n",
    "\n",
    "Abstractive: \"Viá»‡t Nam cÃ³ dÃ¢n sá»‘ Ä‘Ã´ng vá»›i ná»n kinh táº¿ vÃ  vÄƒn hÃ³a phÃ¡t triá»ƒn.\"\n",
    "              â†‘ Táº¡o cÃ¢u má»›i, tá»•ng há»£p thÃ´ng tin\n",
    "```\n",
    "\n",
    "**CÃ¡ch hoáº¡t Ä‘á»™ng:**\n",
    "- Encoder Ä‘á»c vÃ  hiá»ƒu vÄƒn báº£n\n",
    "- Decoder sinh ra vÄƒn báº£n tÃ³m táº¯t tá»«ng token\n",
    "- Attention mechanism táº­p trung vÃ o pháº§n quan trá»ng\n",
    "\n",
    "**Æ¯u Ä‘iá»ƒm:**\n",
    "- âœ… TÃ³m táº¯t tá»± nhiÃªn, máº¡ch láº¡c\n",
    "- âœ… CÃ³ thá»ƒ paraphrase vÃ  tá»•ng há»£p\n",
    "- âœ… Linh hoáº¡t vá» Ä‘á»™ dÃ i\n",
    "\n",
    "**NhÆ°á»£c Ä‘iá»ƒm:**\n",
    "- âŒ CÃ³ thá»ƒ táº¡o thÃ´ng tin sai (hallucination)\n",
    "- âŒ Cáº§n nhiá»u tÃ i nguyÃªn training\n",
    "- âŒ Phá»©c táº¡p hÆ¡n\n",
    "\n",
    "---\n",
    "\n",
    "### B. Model Architecture\n",
    "\n",
    "#### **Transformer Encoder-Decoder**\n",
    "\n",
    "```\n",
    "Input Text â†’ [Tokenizer] â†’ Input IDs\n",
    "                                |\n",
    "                                v\n",
    "                          [ENCODER]\n",
    "                           (PhoBERT)\n",
    "                           (mT5-enc)\n",
    "                           (ViT5-enc)\n",
    "                                |\n",
    "                                v\n",
    "                       Context Vectors\n",
    "                                |\n",
    "                                v\n",
    "                          [DECODER]\n",
    "                           (mT5-dec)\n",
    "                           (ViT5-dec)\n",
    "                                |\n",
    "                                v\n",
    "                       Generated Summary\n",
    "```\n",
    "\n",
    "#### **Attention Mechanism**\n",
    "\n",
    "```python\n",
    "# Simplified attention calculation\n",
    "Q = Query vectors    # What decoder wants to know\n",
    "K = Key vectors      # What encoder has\n",
    "V = Value vectors    # Actual information\n",
    "\n",
    "Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V\n",
    "                      â†‘ Attention weights â†‘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### C. Evaluation Metrics\n",
    "\n",
    "#### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
    "\n",
    "```python\n",
    "Reference: \"Viá»‡t Nam phÃ¡t triá»ƒn kinh táº¿ nhanh\"\n",
    "Generated: \"Kinh táº¿ Viá»‡t Nam phÃ¡t triá»ƒn\"\n",
    "\n",
    "ROUGE-1 (Unigram):\n",
    "  Overlap = {Viá»‡t, Nam, phÃ¡t, triá»ƒn, kinh, táº¿} = 5 words\n",
    "  Recall = 5/5 = 1.0  âœ“\n",
    "  Precision = 5/5 = 1.0  âœ“\n",
    "  F1 = 2 * (1.0 * 1.0) / (1.0 + 1.0) = 1.0\n",
    "\n",
    "ROUGE-2 (Bigram):\n",
    "  Reference bigrams: {(Viá»‡t,Nam), (Nam,phÃ¡t), (phÃ¡t,triá»ƒn), (triá»ƒn,kinh), (kinh,táº¿)}\n",
    "  Generated bigrams: {(Kinh,táº¿), (táº¿,Viá»‡t), (Viá»‡t,Nam), (Nam,phÃ¡t), (phÃ¡t,triá»ƒn)}\n",
    "  Overlap = 3\n",
    "  F1 â‰ˆ 0.6\n",
    "\n",
    "ROUGE-L (Longest Common Subsequence):\n",
    "  LCS = \"Viá»‡t Nam phÃ¡t triá»ƒn\" (4 words)\n",
    "  F1 â‰ˆ 0.8\n",
    "```\n",
    "\n",
    "#### **Interpretation**\n",
    "\n",
    "- **ROUGE-1**: Measures word overlap (content preservation)\n",
    "- **ROUGE-2**: Measures bigram overlap (fluency)\n",
    "- **ROUGE-L**: Measures sentence coherence\n",
    "\n",
    "**Good scores:**\n",
    "- ROUGE-1: 0.40-0.50 (excellent), 0.30-0.40 (good)\n",
    "- ROUGE-2: 0.20-0.30 (excellent), 0.15-0.20 (good)\n",
    "- ROUGE-L: 0.35-0.45 (excellent), 0.25-0.35 (good)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ SETUP ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install transformers==4.35.0 datasets==2.14.6 -q\n",
    "!pip install rouge-score==0.1.2 sentencepiece==0.1.99 -q\n",
    "!pip install accelerate==0.24.1 evaluate==0.4.1 -q\n",
    "\n",
    "print(\"âœ… All packages installed successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ LOAD & EXPLORE DATA\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "VLSP 2021 Summarization dataset bao gá»“m:\n",
    "- **article**: VÄƒn báº£n tin tá»©c gá»‘c (dÃ i)\n",
    "- **summary**: TÃ³m táº¯t reference (ngáº¯n, do con ngÆ°á»i viáº¿t)\n",
    "\n",
    "Format cÃ³ thá»ƒ lÃ :\n",
    "- CSV: `train.csv`, `test.csv`\n",
    "- JSON: `train.json`, `test.json`\n",
    "- Text files: Folder chá»©a cÃ¡c file `.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load data - Äiá»u chá»‰nh path theo dataset cá»§a báº¡n\n",
    "# VÃ­ dá»¥: náº¿u upload dataset vÃ o Kaggle vá»›i tÃªn 'vlsp2021-summarization'\n",
    "\n",
    "data_path = '/kaggle/input/vlsp2021-summarization/train.csv'\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "print(\"=\"*60)\n",
    "print(\"DATA STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nðŸ“Š Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Length statistics\n",
    "df['article_len'] = df['article'].str.len()\n",
    "df['summary_len'] = df['summary'].str.len()\n",
    "df['compression_ratio'] = df['summary_len'] / df['article_len']\n",
    "\n",
    "print(f\"\\nðŸ“ Article statistics:\")\n",
    "print(df['article_len'].describe())\n",
    "\n",
    "print(f\"\\nðŸ“„ Summary statistics:\")\n",
    "print(df['summary_len'].describe())\n",
    "\n",
    "print(f\"\\nðŸ”„ Compression ratio:\")\n",
    "print(f\"Mean: {df['compression_ratio'].mean():.2%}\")\n",
    "print(f\"Median: {df['compression_ratio'].median():.2%}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize data distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Article length distribution\n",
    "axes[0,0].hist(df['article_len'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0,0].axvline(df['article_len'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[\"article_len\"].mean():.0f}')\n",
    "axes[0,0].set_xlabel('Length (characters)', fontsize=12)\n",
    "axes[0,0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0,0].set_title('Article Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Summary length distribution\n",
    "axes[0,1].hist(df['summary_len'], bins=50, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[0,1].axvline(df['summary_len'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[\"summary_len\"].mean():.0f}')\n",
    "axes[0,1].set_xlabel('Length (characters)', fontsize=12)\n",
    "axes[0,1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0,1].set_title('Summary Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Compression ratio\n",
    "axes[1,0].hist(df['compression_ratio'], bins=50, color='mediumseagreen', alpha=0.7, edgecolor='black')\n",
    "axes[1,0].axvline(df['compression_ratio'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[\"compression_ratio\"].mean():.2%}')\n",
    "axes[1,0].set_xlabel('Ratio', fontsize=12)\n",
    "axes[1,0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1,0].set_title('Compression Ratio Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot\n",
    "axes[1,1].scatter(df['article_len'], df['summary_len'], alpha=0.3, s=10)\n",
    "axes[1,1].set_xlabel('Article Length', fontsize=12)\n",
    "axes[1,1].set_ylabel('Summary Length', fontsize=12)\n",
    "axes[1,1].set_title('Article vs Summary Length', fontsize=14, fontweight='bold')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean Vietnamese text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Clean data\n",
    "print(\"Cleaning data...\")\n",
    "df['article'] = df['article'].apply(clean_text)\n",
    "df['summary'] = df['summary'].apply(clean_text)\n",
    "\n",
    "# Remove invalid samples\n",
    "df = df[df['article'].str.len() > 50]\n",
    "df = df[df['summary'].str.len() > 10]\n",
    "df = df[df['article'].str.len() < 5000]\n",
    "\n",
    "print(f\"âœ“ Clean dataset: {len(df)} samples\")\n",
    "print(f\"âœ“ Removed {len(pd.read_csv(data_path)) - len(df)} invalid samples\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/val/test split: 70/15/15\n",
    "train_val, test = train_test_split(df, test_size=0.15, random_state=42)\n",
    "train, val = train_test_split(train_val, test_size=0.15/(1-0.15), random_state=42)\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset split:\")\n",
    "print(f\"Train: {len(train):,} samples ({len(train)/len(df):.1%})\")\n",
    "print(f\"Val: {len(val):,} samples ({len(val)/len(df):.1%})\")\n",
    "print(f\"Test: {len(test):,} samples ({len(test)/len(df):.1%})\")\n",
    "\n",
    "# Reset index\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ DATASET & DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    \"\"\"Dataset for text summarization\"\"\"\n",
    "    \n",
    "    def __init__(self, articles, summaries, tokenizer, max_length=512, max_target_length=128):\n",
    "        self.articles = articles\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.max_target_length = max_target_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        article = \"summarize: \" + str(self.articles.iloc[idx])\n",
    "        summary = str(self.summaries.iloc[idx])\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            article,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize target\n",
    "        targets = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_target_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': targets['input_ids'].squeeze()\n",
    "        }\n",
    "\n",
    "print(\"âœ“ Dataset class defined\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ MODEL TRAINING - ViT5\n",
    "\n",
    "### ViT5: Vietnamese T5\n",
    "\n",
    "**ViT5 lÃ  model T5 Ä‘Æ°á»£c VietAI pre-train trÃªn corpus tiáº¿ng Viá»‡t lá»›n:**\n",
    "- 12 layers encoder + 12 layers decoder\n",
    "- 768 hidden dimensions\n",
    "- Optimized cho tiáº¿ng Viá»‡t\n",
    "- Performance tá»‘t hÆ¡n mT5 cho Vietnamese tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuration\n",
    "MODEL_NAME = 'VietAI/vit5-base'\n",
    "BATCH_SIZE = 4  # Giáº£m náº¿u out of memory\n",
    "LEARNING_RATE = 5e-5\n",
    "NUM_EPOCHS = 3\n",
    "MAX_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 128\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load model vÃ  tokenizer\n",
    "print(\"Loading ViT5 model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"âœ“ Model loaded on {device}\")\n",
    "print(f\"âœ“ Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create datasets\n",
    "train_dataset = SummarizationDataset(\n",
    "    train['article'],\n",
    "    train['summary'],\n",
    "    tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_target_length=MAX_TARGET_LENGTH\n",
    ")\n",
    "\n",
    "val_dataset = SummarizationDataset(\n",
    "    val['article'],\n",
    "    val['summary'],\n",
    "    tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_target_length=MAX_TARGET_LENGTH\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"âœ“ Val dataset: {len(val_dataset)} samples\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "print(\"âœ“ Data collator created\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Metric computation\n",
    "rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute ROUGE\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        scores = rouge_scorer_obj.score(label, pred)\n",
    "        rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "        rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "        rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': np.mean(rouge_scores['rouge1']),\n",
    "        'rouge2': np.mean(rouge_scores['rouge2']),\n",
    "        'rougeL': np.mean(rouge_scores['rougeL'])\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Metric function defined\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./vit5_summarization',\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='rouge1',\n",
    "    greater_is_better=True,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "print(\"âœ“ Training arguments configured\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"âœ“ Trainer initialized\")\n",
    "print(\"\\nðŸš€ Ready to train!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Start training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ViT5 MODEL\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nâœ… Training completed!\")\n",
    "print(f\"Final training loss: {train_result.metrics['train_loss']:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save model\n",
    "trainer.save_model('./vit5_final')\n",
    "tokenizer.save_pretrained('./vit5_final')\n",
    "\n",
    "print(\"âœ“ Model saved to ./vit5_final\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "results = {\n",
    "    'rouge1': [],\n",
    "    'rouge2': [],\n",
    "    'rougeL': [],\n",
    "    'predictions': [],\n",
    "    'references': []\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in tqdm(range(len(test)), desc=\"Evaluating\"):\n",
    "        article = \"summarize: \" + str(test.iloc[idx]['article'])\n",
    "        reference = str(test.iloc[idx]['summary'])\n",
    "        \n",
    "        # Generate\n",
    "        inputs = tokenizer(\n",
    "            article,\n",
    "            max_length=MAX_LENGTH,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            num_beams=4,\n",
    "            length_penalty=0.6,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Compute ROUGE\n",
    "        scores = rouge_scorer_obj.score(reference, prediction)\n",
    "        results['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "        results['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "        results['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "        results['predictions'].append(prediction)\n",
    "        results['references'].append(reference)\n",
    "\n",
    "# Final results\n",
    "print(f\"\\nðŸ“Š Test Results:\")\n",
    "print(f\"ROUGE-1: {np.mean(results['rouge1']):.4f} Â± {np.std(results['rouge1']):.4f}\")\n",
    "print(f\"ROUGE-2: {np.mean(results['rouge2']):.4f} Â± {np.std(results['rouge2']):.4f}\")\n",
    "print(f\"ROUGE-L: {np.mean(results['rougeL']):.4f} Â± {np.std(results['rougeL']):.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ ANALYSIS & VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot ROUGE distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "scores = ['rouge1', 'rouge2', 'rougeL']\n",
    "titles = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "colors = ['steelblue', 'coral', 'mediumseagreen']\n",
    "\n",
    "for idx, (score, title, color) in enumerate(zip(scores, titles, colors)):\n",
    "    axes[idx].hist(results[score], bins=30, alpha=0.7, color=color, edgecolor='black')\n",
    "    axes[idx].axvline(np.mean(results[score]), color='red', linestyle='--', \n",
    "                    linewidth=2, label=f'Mean: {np.mean(results[score]):.3f}')\n",
    "    axes[idx].set_xlabel('Score', fontsize=12)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[idx].set_title(f'{title} Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[idx].legend(fontsize=11)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rouge_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "n_samples = 5\n",
    "indices = np.random.choice(len(results['predictions']), n_samples, replace=False)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    print(f\"\\n{'â”€'*80}\")\n",
    "    print(f\"Sample {i+1}\")\n",
    "    print(f\"{'â”€'*80}\")\n",
    "    print(f\"\\nðŸ“ Reference:\")\n",
    "    print(f\"{results['references'][idx]}\")\n",
    "    print(f\"\\nðŸ¤– Generated:\")\n",
    "    print(f\"{results['predictions'][idx]}\")\n",
    "    print(f\"\\nðŸ“Š Scores:\")\n",
    "    print(f\"ROUGE-1: {results['rouge1'][idx]:.3f} | \"\n",
    "          f\"ROUGE-2: {results['rouge2'][idx]:.3f} | \"\n",
    "          f\"ROUGE-L: {results['rougeL'][idx]:.3f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ SAVE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save detailed results\n",
    "results_df = pd.DataFrame({\n",
    "    'reference': results['references'],\n",
    "    'prediction': results['predictions'],\n",
    "    'rouge1': results['rouge1'],\n",
    "    'rouge2': results['rouge2'],\n",
    "    'rougeL': results['rougeL']\n",
    "})\n",
    "\n",
    "results_df.to_csv('test_results.csv', index=False)\n",
    "print(\"âœ“ Results saved to test_results.csv\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary = {\n",
    "    'model': MODEL_NAME,\n",
    "    'rouge1_mean': float(np.mean(results['rouge1'])),\n",
    "    'rouge1_std': float(np.std(results['rouge1'])),\n",
    "    'rouge2_mean': float(np.mean(results['rouge2'])),\n",
    "    'rouge2_std': float(np.std(results['rouge2'])),\n",
    "    'rougeL_mean': float(np.mean(results['rougeL'])),\n",
    "    'rougeL_std': float(np.std(results['rougeL'])),\n",
    "    'test_samples': len(test)\n",
    "}\n",
    "\n",
    "with open('summary_statistics.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"âœ“ Summary saved to summary_statistics.json\")\n",
    "\n",
    "print(\"\\nâœ… ALL DONE!\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
