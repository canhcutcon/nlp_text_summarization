{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "---\n\n# 4. Abstractive Summarization\n\n## 4.1 Sequence-to-Sequence Models - Theory\n\n### Nguy√™n l√Ω / Principle:\n\n**Abstractive Summarization** s·ª≠ d·ª•ng m√¥ h√¨nh **Sequence-to-Sequence (Seq2Seq)** ƒë·ªÉ t·∫°o ra b·∫£n t√≥m t·∫Øt m·ªõi thay v√¨ ch·ªçn c√¢u c√≥ s·∫µn.\n\n### Ki·∫øn tr√∫c Encoder-Decoder:\n\n**Encoder:**\n- Nh·∫≠n vƒÉn b·∫£n ƒë·∫ßu v√†o\n- Chuy·ªÉn th√†nh vector bi·ªÉu di·ªÖn (context vector)\n- Capture to√†n b·ªô ng·ªØ nghƒ©a c·ªßa vƒÉn b·∫£n\n\n**Decoder:**\n- Nh·∫≠n context vector t·ª´ encoder\n- Sinh ra t·ª´ng t·ª´ m·ªôt c·ªßa b·∫£n t√≥m t·∫Øt\n- S·ª≠ d·ª•ng th√¥ng tin t·ª´ encoder v√† t·ª´ ƒë√£ sinh tr∆∞·ªõc ƒë√≥\n\n### Attention Mechanism:\n\n**V·∫•n ƒë·ªÅ / Problem:**\n- Context vector c·ªë ƒë·ªãnh kh√¥ng th·ªÉ ch·ª©a to√†n b·ªô th√¥ng tin\n- V·ªõi vƒÉn b·∫£n d√†i, th√¥ng tin b·ªã m·∫•t\n\n**Gi·∫£i ph√°p / Solution:**\n- **Attention** cho ph√©p decoder \"t·∫≠p trung\" v√†o ph·∫ßn kh√°c nhau c·ªßa input\n- T·∫°i m·ªói b∆∞·ªõc sinh t·ª´, decoder xem l·∫°i to√†n b·ªô encoder outputs\n- T√≠nh tr·ªçng s·ªë quan tr·ªçng cho t·ª´ng v·ªã tr√≠ trong input\n\n### Generation Strategies:\n\n#### 1. Greedy Decoding\n- Ch·ªçn t·ª´ c√≥ x√°c su·∫•t cao nh·∫•t t·∫°i m·ªói b∆∞·ªõc\n- **∆Øu ƒëi·ªÉm**: Nhanh\n- **Nh∆∞·ª£c ƒëi·ªÉm**: Kh√¥ng t·ªëi ∆∞u to√†n c·ª•c\n\n#### 2. Beam Search\n- Gi·ªØ K candidates t·ªët nh·∫•t (beam width = K)\n- **∆Øu ƒëi·ªÉm**: Ch·∫•t l∆∞·ª£ng cao h∆°n greedy\n- **Nh∆∞·ª£c ƒëi·ªÉm**: Ch·∫≠m h∆°n (K l·∫ßn)\n\n#### 3. Top-k Sampling\n- Ch·ªçn ng·∫´u nhi√™n t·ª´ K t·ª´ c√≥ x√°c su·∫•t cao nh·∫•t\n- **∆Øu ƒëi·ªÉm**: ƒêa d·∫°ng h∆°n\n- **Nh∆∞·ª£c ƒëi·ªÉm**: C√≥ th·ªÉ kh√¥ng ·ªïn ƒë·ªãnh\n\n#### 4. Top-p (Nucleus) Sampling\n- Ch·ªçn ng·∫´u nhi√™n t·ª´ c√°c t·ª´ c√≥ t·ªïng x√°c su·∫•t >= p\n- **∆Øu ƒëi·ªÉm**: C√¢n b·∫±ng gi·ªØa ƒëa d·∫°ng v√† ch·∫•t l∆∞·ª£ng\n- **Nh∆∞·ª£c ƒëi·ªÉm**: C·∫ßn ƒëi·ªÅu ch·ªânh p c·∫©n th·∫≠n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vietnamese Text Summarization - Complete Educational Notebook\n",
        "\n",
        "**Comprehensive Guide to Text Summarization with mT5-small and ViT5**\n",
        "\n",
        "---\n",
        "\n",
        "## Notebook Overview\n",
        "\n",
        "This notebook provides a complete educational guide to Vietnamese text summarization, covering:\n",
        "\n",
        "1. **L√Ω thuy·∫øt Text Summarization** - Theory (Vietnamese + English)\n",
        "2. **Setup & Load Data** - Data preparation and exploration\n",
        "3. **Extractive Summarization** - TextRank algorithm with PhoBERT\n",
        "4. **Abstractive Summarization** - mT5-small and ViT5 models\n",
        "5. **Evaluation & Comparison** - ROUGE metrics and model comparison\n",
        "6. **Visualizations** - 8 comprehensive charts\n",
        "7. **Applications** - Real-world use cases\n",
        "\n",
        "---\n",
        "\n",
        "## Technical Specifications\n",
        "\n",
        "- **Models**: mT5-small (HuggingFace), ViT5 (YangYang0203/vi5_summarize)\n",
        "- **GPU**: Optimized for RTX 4070 SUPER 12GB\n",
        "- **Prefix**: \"t√≥m t·∫Øt:\" for both models\n",
        "- **Mode**: Inference only (no training)\n",
        "- **Execution Time**: ~20-30 minutes for full notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 1. L√Ω thuy·∫øt Text Summarization (Theory)\n",
        "\n",
        "## 1.1 Text Summarization l√† g√¨? / What is Text Summarization?\n",
        "\n",
        "**Ti·∫øng Vi·ªát:**\n",
        "\n",
        "T√≥m t·∫Øt vƒÉn b·∫£n (Text Summarization) l√† qu√° tr√¨nh r√∫t g·ªçn m·ªôt vƒÉn b·∫£n d√†i th√†nh phi√™n b·∫£n ng·∫Øn h∆°n trong khi v·∫´n gi·ªØ ƒë∆∞·ª£c nh·ªØng th√¥ng tin quan tr·ªçng nh·∫•t. ƒê√¢y l√† m·ªôt trong nh·ªØng nhi·ªám v·ª• quan tr·ªçng nh·∫•t trong X·ª≠ l√Ω Ng√¥n ng·ªØ T·ª± nhi√™n (NLP).\n",
        "\n",
        "**English:**\n",
        "\n",
        "Text Summarization is the process of condensing a long text into a shorter version while preserving the most important information. It is one of the most important tasks in Natural Language Processing (NLP).\n",
        "\n",
        "### ·ª®ng d·ª•ng th·ª±c t·∫ø / Real-world Applications:\n",
        "\n",
        "- **T√≥m t·∫Øt tin t·ª©c / News Summarization**: T·ª± ƒë·ªông t·∫°o ti√™u ƒë·ªÅ v√† t√≥m t·∫Øt b√†i b√°o\n",
        "- **T√≥m t·∫Øt t√†i li·ªáu / Document Summarization**: R√∫t g·ªçn b√°o c√°o, nghi√™n c·ª©u khoa h·ªçc\n",
        "- **T√≥m t·∫Øt cu·ªôc h·ªçp / Meeting Notes**: Ghi ch√∫ t·ª± ƒë·ªông t·ª´ cu·ªôc h·ªçp\n",
        "- **T√≥m t·∫Øt ph√°p l√Ω / Legal Summaries**: R√∫t g·ªçn h·ª£p ƒë·ªìng, vƒÉn b·∫£n ph√°p lu·∫≠t\n",
        "- **T√≥m t·∫Øt y t·∫ø / Medical Records**: T√≥m t·∫Øt h·ªì s∆° b·ªánh √°n\n",
        "\n",
        "### Th√°ch th·ª©c v·ªõi ti·∫øng Vi·ªát / Challenges in Vietnamese:\n",
        "\n",
        "- Kh√¥ng c√≥ kho·∫£ng tr·∫Øng gi·ªØa c√°c t·ª´ (trong m·ªôt s·ªë tr∆∞·ªùng h·ª£p)\n",
        "- Ng·ªØ ph√°p ph·ª©c t·∫°p v·ªõi nhi·ªÅu t·ª´ ƒëa nghƒ©a\n",
        "- Thi·∫øu d·ªØ li·ªáu hu·∫•n luy·ªán ch·∫•t l∆∞·ª£ng cao\n",
        "- C·∫ßn m√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·∫∑c bi·ªát cho ti·∫øng Vi·ªát\n",
        "\n",
        "---\n",
        "\n",
        "## 1.2 Hai ph∆∞∆°ng ph√°p: Extractive vs Abstractive\n",
        "\n",
        "### A. Extractive Summarization (T√≥m t·∫Øt tr√≠ch xu·∫•t)\n",
        "\n",
        "**Nguy√™n l√Ω / Principle:**\n",
        "- Ch·ªçn c√°c c√¢u quan tr·ªçng nh·∫•t t·ª´ vƒÉn b·∫£n g·ªëc\n",
        "- Kh√¥ng t·∫°o ra c√¢u m·ªõi, ch·ªâ sao ch√©p c√¢u hi·ªán c√≥\n",
        "\n",
        "**∆Øu ƒëi·ªÉm / Advantages:**\n",
        "- ‚úÖ Ng·ªØ ph√°p lu√¥n ch√≠nh x√°c (v√¨ sao ch√©p t·ª´ g·ªëc)\n",
        "- ‚úÖ Kh√¥ng c√≥ \"hallucination\" (t·∫°o th√¥ng tin sai)\n",
        "- ‚úÖ T·ªëc ƒë·ªô nhanh, t√†i nguy√™n th·∫•p\n",
        "\n",
        "**Nh∆∞·ª£c ƒëi·ªÉm / Disadvantages:**\n",
        "- ‚ùå K√©m m·∫°ch l·∫°c (c√¢u r·ªùi r·∫°c)\n",
        "- ‚ùå T·ª∑ l·ªá n√©n h·∫°n ch·∫ø\n",
        "- ‚ùå Kh√¥ng linh ho·∫°t\n",
        "\n",
        "**Ph∆∞∆°ng ph√°p / Methods:**\n",
        "- TextRank algorithm (PageRank cho c√¢u)\n",
        "- TF-IDF scoring\n",
        "- BERT/PhoBERT-based sentence scoring\n",
        "\n",
        "### B. Abstractive Summarization (T√≥m t·∫Øt sinh t·∫°o)\n",
        "\n",
        "**Nguy√™n l√Ω / Principle:**\n",
        "- T·∫°o ra c√¢u m·ªõi d·ª±a tr√™n hi·ªÉu nghƒ©a vƒÉn b·∫£n g·ªëc\n",
        "- Gi·ªëng c√°ch con ng∆∞·ªùi t√≥m t·∫Øt\n",
        "\n",
        "**∆Øu ƒëi·ªÉm / Advantages:**\n",
        "- ‚úÖ M·∫°ch l·∫°c, t·ª± nhi√™n h∆°n\n",
        "- ‚úÖ T·ª∑ l·ªá n√©n cao h∆°n\n",
        "- ‚úÖ Linh ho·∫°t, c√≥ th·ªÉ paraphrase\n",
        "\n",
        "**Nh∆∞·ª£c ƒëi·ªÉm / Disadvantages:**\n",
        "- ‚ùå C√≥ th·ªÉ t·∫°o th√¥ng tin sai (hallucination)\n",
        "- ‚ùå C·∫ßn m√¥ h√¨nh l·ªõn, ph·ª©c t·∫°p\n",
        "- ‚ùå T·ªëc ƒë·ªô ch·∫≠m h∆°n, t√†i nguy√™n cao\n",
        "\n",
        "**Ph∆∞∆°ng ph√°p / Methods:**\n",
        "- Seq2seq models\n",
        "- Transformer models (T5, BART, PEGASUS)\n",
        "- mT5, ViT5 (for Vietnamese)\n",
        "\n",
        "---\n",
        "\n",
        "## 1.3 Ki·∫øn tr√∫c m√¥ h√¨nh / Model Architectures\n",
        "\n",
        "### A. T5 (Text-to-Text Transfer Transformer)\n",
        "\n",
        "**Nguy√™n l√Ω / Principle:**\n",
        "- M·ªçi t√°c v·ª• NLP ƒë·ªÅu ƒë∆∞·ª£c chuy·ªÉn th√†nh d·∫°ng \"text-to-text\"\n",
        "- Input: \"t√≥m t·∫Øt: [vƒÉn b·∫£n]\"\n",
        "- Output: \"[b·∫£n t√≥m t·∫Øt]\"\n",
        "\n",
        "**Ki·∫øn tr√∫c / Architecture:**\n",
        "- Encoder-Decoder structure (Transformer)\n",
        "- Encoder: Hi·ªÉu vƒÉn b·∫£n ƒë·∫ßu v√†o\n",
        "- Decoder: Sinh ra vƒÉn b·∫£n ƒë·∫ßu ra\n",
        "- Self-attention mechanism\n",
        "\n",
        "**Hu·∫•n luy·ªán / Training:**\n",
        "- Pre-trained on C4 corpus (750GB text)\n",
        "- Multi-task learning (translation, QA, summarization, etc.)\n",
        "\n",
        "### B. mT5 (Multilingual T5)\n",
        "\n",
        "**ƒê·∫∑c ƒëi·ªÉm / Characteristics:**\n",
        "- M·ªü r·ªông T5 cho 101 ng√¥n ng·ªØ (bao g·ªìm ti·∫øng Vi·ªát)\n",
        "- Pre-trained on mC4 corpus (multilingual C4)\n",
        "- Shared vocabulary across languages\n",
        "\n",
        "**Model Sizes:**\n",
        "- **mT5-small**: 300M parameters\n",
        "- **mT5-base**: 580M parameters  \n",
        "- **mT5-large**: 1.2B parameters\n",
        "- **mT5-xl**: 3.7B parameters\n",
        "\n",
        "**∆Øu ƒëi·ªÉm cho ti·∫øng Vi·ªát / Advantages for Vietnamese:**\n",
        "- ‚úÖ Pre-trained v·ªõi d·ªØ li·ªáu ti·∫øng Vi·ªát\n",
        "- ‚úÖ Transfer learning t·ª´ c√°c ng√¥n ng·ªØ kh√°c\n",
        "- ‚úÖ Hi·ªáu qu·∫£ v·ªõi √≠t d·ªØ li·ªáu fine-tuning\n",
        "\n",
        "### C. ViT5 (Vietnamese T5)\n",
        "\n",
        "**ƒê·∫∑c ƒëi·ªÉm / Characteristics:**\n",
        "- T5 ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·∫∑c bi·ªát cho ti·∫øng Vi·ªát\n",
        "- Pre-trained on Vietnamese corpus\n",
        "- Better understanding of Vietnamese grammar and semantics\n",
        "\n",
        "**∆Øu ƒëi·ªÉm / Advantages:**\n",
        "- ‚úÖ Hi·ªÉu ti·∫øng Vi·ªát t·ªët h∆°n mT5\n",
        "- ‚úÖ K·∫øt qu·∫£ t·ª± nhi√™n h∆°n v·ªõi ng·ªØ ph√°p ti·∫øng Vi·ªát\n",
        "- ‚úÖ X·ª≠ l√Ω t·ªët c√°c th√†nh ng·ªØ, t·ª•c ng·ªØ Vi·ªát Nam\n",
        "\n",
        "---\n",
        "\n",
        "## 1.4 ƒê·ªô ƒëo ƒë√°nh gi√° / Evaluation Metrics\n",
        "\n",
        "### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
        "\n",
        "**ROUGE-1: Unigram Overlap**\n",
        "- ƒêo l∆∞·ªùng s·ª± tr√πng kh·ªõp t·ª´ng t·ª´ ƒë∆°n\n",
        "- Measures word-level overlap\n",
        "- Formula: (Number of overlapping words) / (Total words in reference)\n",
        "\n",
        "**ROUGE-2: Bigram Overlap**\n",
        "- ƒêo l∆∞·ªùng s·ª± tr√πng kh·ªõp c·∫∑p t·ª´ li√™n ti·∫øp\n",
        "- Measures phrase-level overlap\n",
        "- Better reflects fluency\n",
        "\n",
        "**ROUGE-L: Longest Common Subsequence**\n",
        "- ƒêo l∆∞·ªùng chu·ªói con chung d√†i nh·∫•t\n",
        "- Captures sentence-level structure\n",
        "- Considers word order\n",
        "\n",
        "**C√°ch hi·ªÉu ƒëi·ªÉm s·ªë / Score Interpretation:**\n",
        "- **< 0.2**: Poor quality\n",
        "- **0.2 - 0.3**: Fair quality\n",
        "- **0.3 - 0.4**: Good quality\n",
        "- **0.4 - 0.5**: Very good quality\n",
        "- **> 0.5**: Excellent quality\n",
        "\n",
        "**Precision vs Recall vs F1:**\n",
        "- **Precision**: T·ª∑ l·ªá t·ª´ ƒë√∫ng trong b·∫£n t√≥m t·∫Øt sinh ra\n",
        "- **Recall**: T·ª∑ l·ªá t·ª´ trong reference ƒë∆∞·ª£c bao ph·ªß\n",
        "- **F1**: Trung b√¨nh ƒëi·ªÅu h√≤a c·ªßa Precision v√† Recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 2. Setup & Load Data\n",
        "\n",
        "## 2.1 Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core dependencies (protobuf required for T5 tokenizers)\n",
        "!pip install -q protobuf sentencepiece\n",
        "\n",
        "# Core libraries\n",
        "!pip install -q transformers datasets torch\n",
        "\n",
        "# Evaluation and metrics\n",
        "!pip install -q rouge-score py-rouge evaluate scikit-learn\n",
        "\n",
        "# Vietnamese NLP and graph algorithms\n",
        "!pip install -q underthesea networkx\n",
        "\n",
        "# Visualization\n",
        "!pip install -q matplotlib seaborn pandas numpy\n",
        "\n",
        "# Progress bars\n",
        "!pip install -q tqdm\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")\n",
        "print(\"‚ö†Ô∏è  If you encounter 'protobuf' errors, please RESTART THE KERNEL and run cells again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Import Libraries and Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# HuggingFace\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModel, \n",
        "    AutoModelForSeq2SeqLM\n",
        ")\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Evaluation\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Graph algorithms for TextRank\n",
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Check CUDA availability\n",
        "print(\"=\"*60)\n",
        "print(\"SYSTEM INFORMATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"‚úì CUDA is available\")\n",
        "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚úì Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"‚ö† CUDA not available, using CPU\")\n",
        "    print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
        "\n",
        "print(\"\\n‚úÖ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Load Data from CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"LOADING VIETNAMESE TEXT SUMMARIZATION DATASET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load CSV files\n",
        "data_path = \"data\"\n",
        "train_df = pd.read_csv(f\"{data_path}/train.csv\")\n",
        "val_df = pd.read_csv(f\"{data_path}/validation.csv\")\n",
        "test_df = pd.read_csv(f\"{data_path}/test.csv\")\n",
        "\n",
        "print(f\"\\nüìä Dataset loaded successfully!\")\n",
        "print(f\"  Train: {len(train_df):,} samples\")\n",
        "print(f\"  Validation: {len(val_df):,} samples\")\n",
        "print(f\"  Test: {len(test_df):,} samples\")\n",
        "print(f\"  Total: {len(train_df) + len(val_df) + len(test_df):,} samples\")\n",
        "\n",
        "# Check columns\n",
        "print(f\"\\nüìã Columns: {list(train_df.columns)}\")\n",
        "\n",
        "# Keep only document and summary columns\n",
        "train_df = train_df[['document', 'summary']].dropna()\n",
        "val_df = val_df[['document', 'summary']].dropna()\n",
        "test_df = test_df[['document', 'summary']].dropna()\n",
        "\n",
        "print(f\"\\n‚úì After removing NaN: Train={len(train_df):,}, Val={len(val_df):,}, Test={len(test_df):,}\")\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "dataset = DatasetDict({\n",
        "    'train': Dataset.from_pandas(train_df, preserve_index=False),\n",
        "    'validation': Dataset.from_pandas(val_df, preserve_index=False),\n",
        "    'test': Dataset.from_pandas(test_df, preserve_index=False)\n",
        "})\n",
        "\n",
        "print(f\"\\n{dataset}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Data Exploration and Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample\n",
        "print(\"=\"*60)\n",
        "print(\"SAMPLE DOCUMENT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sample = dataset['train'][0]\n",
        "print(f\"\\nüìÑ Document ({len(sample['document'])} characters):\")\n",
        "print(sample['document'][:500] + \"...\\n\")\n",
        "\n",
        "print(f\"üìù Summary ({len(sample['summary'])} characters):\")\n",
        "print(sample['summary'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vietnamese sentence tokenizer\n",
        "def sent_tokenize(text):\n",
        "    \"\"\"Simple Vietnamese sentence tokenizer\"\"\"\n",
        "    pattern = r'(?<=[.!?])\\s+(?=[A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê])'\n",
        "    sentences = re.split(pattern, text)\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "# Compute statistics\n",
        "def compute_statistics(df, name):\n",
        "    \"\"\"Compute document and summary statistics\"\"\"\n",
        "    doc_words = df['document'].apply(lambda x: len(x.split()))\n",
        "    sum_words = df['summary'].apply(lambda x: len(x.split()))\n",
        "    doc_chars = df['document'].apply(len)\n",
        "    sum_chars = df['summary'].apply(len)\n",
        "    doc_sents = df['document'].apply(lambda x: len(sent_tokenize(x)))\n",
        "    sum_sents = df['summary'].apply(lambda x: len(sent_tokenize(x)))\n",
        "    \n",
        "    compression_ratio = (sum_words.mean() / doc_words.mean() * 100)\n",
        "    \n",
        "    print(f\"\\n{name} Statistics:\")\n",
        "    print(f\"  Document length:\")\n",
        "    print(f\"    Words: mean={doc_words.mean():.1f}, median={doc_words.median():.1f}, std={doc_words.std():.1f}\")\n",
        "    print(f\"    Characters: mean={doc_chars.mean():.1f}, median={doc_chars.median():.1f}\")\n",
        "    print(f\"    Sentences: mean={doc_sents.mean():.1f}, median={doc_sents.median():.1f}\")\n",
        "    print(f\"  Summary length:\")\n",
        "    print(f\"    Words: mean={sum_words.mean():.1f}, median={sum_words.median():.1f}, std={sum_words.std():.1f}\")\n",
        "    print(f\"    Characters: mean={sum_chars.mean():.1f}, median={sum_chars.median():.1f}\")\n",
        "    print(f\"    Sentences: mean={sum_sents.mean():.1f}, median={sum_sents.median():.1f}\")\n",
        "    print(f\"  Compression ratio: {compression_ratio:.1f}%\")\n",
        "    \n",
        "    return {\n",
        "        'doc_words': doc_words,\n",
        "        'sum_words': sum_words,\n",
        "        'doc_chars': doc_chars,\n",
        "        'sum_chars': sum_chars,\n",
        "        'compression': compression_ratio\n",
        "    }\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DATASET STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "train_stats = compute_statistics(train_df, \"TRAIN\")\n",
        "val_stats = compute_statistics(val_df, \"VALIDATION\")\n",
        "test_stats = compute_statistics(test_df, \"TEST\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize data statistics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# Document length distribution\n",
        "axes[0, 0].hist(train_stats['doc_words'], bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "axes[0, 0].axvline(train_stats['doc_words'].mean(), color='red', linestyle='--', \n",
        "                   linewidth=2, label=f\"Mean: {train_stats['doc_words'].mean():.0f}\")\n",
        "axes[0, 0].set_xlabel('Document Length (words)', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
        "axes[0, 0].set_title('Document Length Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Summary length distribution\n",
        "axes[0, 1].hist(train_stats['sum_words'], bins=50, alpha=0.7, color='coral', edgecolor='black')\n",
        "axes[0, 1].axvline(train_stats['sum_words'].mean(), color='red', linestyle='--',\n",
        "                   linewidth=2, label=f\"Mean: {train_stats['sum_words'].mean():.0f}\")\n",
        "axes[0, 1].set_xlabel('Summary Length (words)', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Frequency', fontsize=12)\n",
        "axes[0, 1].set_title('Summary Length Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Comparison by split\n",
        "splits = ['Train', 'Validation', 'Test']\n",
        "doc_means = [train_stats['doc_words'].mean(), val_stats['doc_words'].mean(), test_stats['doc_words'].mean()]\n",
        "sum_means = [train_stats['sum_words'].mean(), val_stats['sum_words'].mean(), test_stats['sum_words'].mean()]\n",
        "\n",
        "x = np.arange(len(splits))\n",
        "width = 0.35\n",
        "\n",
        "axes[1, 0].bar(x - width/2, doc_means, width, label='Document', color='steelblue', alpha=0.8)\n",
        "axes[1, 0].bar(x + width/2, sum_means, width, label='Summary', color='coral', alpha=0.8)\n",
        "axes[1, 0].set_xlabel('Dataset Split', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Average Length (words)', fontsize=12)\n",
        "axes[1, 0].set_title('Average Length by Dataset Split', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_xticks(x)\n",
        "axes[1, 0].set_xticklabels(splits)\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Compression ratio\n",
        "compression_ratios = [train_stats['compression'], val_stats['compression'], test_stats['compression']]\n",
        "axes[1, 1].bar(splits, compression_ratios, color=['steelblue', 'coral', 'lightgreen'], alpha=0.8)\n",
        "axes[1, 1].set_xlabel('Dataset Split', fontsize=12)\n",
        "axes[1, 1].set_ylabel('Compression Ratio (%)', fontsize=12)\n",
        "axes[1, 1].set_title('Compression Ratio by Dataset Split', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for i, v in enumerate(compression_ratios):\n",
        "    axes[1, 1].text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Data exploration complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 3. Extractive Summarization\n",
        "\n",
        "## 3.1 TextRank Algorithm - Theory\n",
        "\n",
        "### Nguy√™n l√Ω / Principle:\n",
        "\n",
        "TextRank l√† thu·∫≠t to√°n extractive summarization d·ª±a tr√™n **PageRank** (thu·∫≠t to√°n x·∫øp h·∫°ng trang web c·ªßa Google).\n",
        "\n",
        "**C√°c b∆∞·ªõc / Steps:**\n",
        "\n",
        "1. **T√°ch c√¢u / Sentence Splitting**: Chia vƒÉn b·∫£n th√†nh c√°c c√¢u ri√™ng bi·ªát\n",
        "2. **Embedding**: Chuy·ªÉn m·ªói c√¢u th√†nh vector s·ªë (s·ª≠ d·ª•ng PhoBERT)\n",
        "3. **Similarity Matrix**: T√≠nh ƒë·ªô t∆∞∆°ng t·ª± gi·ªØa m·ªçi c·∫∑p c√¢u (cosine similarity)\n",
        "4. **Graph Construction**: X√¢y d·ª±ng ƒë·ªì th·ªã v·ªõi:\n",
        "   - Nodes: C√°c c√¢u\n",
        "   - Edges: ƒê·ªô t∆∞∆°ng t·ª± gi·ªØa c√°c c√¢u\n",
        "5. **PageRank**: Ch·∫°y thu·∫≠t to√°n PageRank ƒë·ªÉ t√≠nh ƒëi·ªÉm cho m·ªói c√¢u\n",
        "6. **Selection**: Ch·ªçn top N c√¢u c√≥ ƒëi·ªÉm cao nh·∫•t\n",
        "\n",
        "### PhoBERT for Vietnamese:\n",
        "\n",
        "- **PhoBERT**: BERT model ƒë∆∞·ª£c hu·∫•n luy·ªán cho ti·∫øng Vi·ªát\n",
        "- T·∫°o ra embeddings ch·∫•t l∆∞·ª£ng cao cho c√¢u ti·∫øng Vi·ªát\n",
        "- Hi·ªÉu ƒë∆∞·ª£c ng·ªØ nghƒ©a v√† ng·ªØ c·∫£nh\n",
        "\n",
        "### PageRank Formula:\n",
        "\n",
        "```\n",
        "PR(Vi) = (1-d) + d * Œ£(PR(Vj) / |Out(Vj)|)\n",
        "```\n",
        "\n",
        "Trong ƒë√≥:\n",
        "- `PR(Vi)`: PageRank score c·ªßa c√¢u i\n",
        "- `d`: Damping factor (th∆∞·ªùng l√† 0.85)\n",
        "- `Vj`: C√°c c√¢u c√≥ li√™n k·∫øt ƒë·∫øn Vi\n",
        "- `Out(Vj)`: S·ªë c√¢u m√† Vj li√™n k·∫øt t·ªõi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 TextRank Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextRankSummarizer:\n",
        "    \"\"\"\n",
        "    TextRank algorithm for extractive summarization using PhoBERT embeddings\n",
        "    \n",
        "    Args:\n",
        "        top_n (int): Number of sentences to extract\n",
        "        damping (float): Damping factor for PageRank (default: 0.85)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, top_n=3, damping=0.85):\n",
        "        self.top_n = top_n\n",
        "        self.damping = damping\n",
        "        \n",
        "        print(\"Loading PhoBERT model for Vietnamese sentence embeddings...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base')\n",
        "        self.model = AutoModel.from_pretrained('vinai/phobert-base')\n",
        "        self.device = device\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        print(\"‚úì PhoBERT loaded successfully!\")\n",
        "    \n",
        "    def get_sentence_embedding(self, sentence):\n",
        "        \"\"\"\n",
        "        Get PhoBERT embedding for a sentence\n",
        "        \n",
        "        Args:\n",
        "            sentence (str): Input sentence\n",
        "            \n",
        "        Returns:\n",
        "            np.ndarray: Sentence embedding vector\n",
        "        \"\"\"\n",
        "        inputs = self.tokenizer(\n",
        "            sentence, \n",
        "            return_tensors='pt', \n",
        "            truncation=True, \n",
        "            max_length=256\n",
        "        ).to(self.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            # Use CLS token embedding\n",
        "            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "        \n",
        "        return embedding[0]\n",
        "    \n",
        "    def build_similarity_matrix(self, sentences):\n",
        "        \"\"\"\n",
        "        Build similarity matrix between sentences using cosine similarity\n",
        "        \n",
        "        Args:\n",
        "            sentences (list): List of sentences\n",
        "            \n",
        "        Returns:\n",
        "            np.ndarray: Similarity matrix\n",
        "        \"\"\"\n",
        "        embeddings = []\n",
        "        \n",
        "        for sent in tqdm(sentences, desc=\"Computing sentence embeddings\"):\n",
        "            emb = self.get_sentence_embedding(sent)\n",
        "            embeddings.append(emb)\n",
        "        \n",
        "        embeddings = np.array(embeddings)\n",
        "        similarity_matrix = cosine_similarity(embeddings)\n",
        "        \n",
        "        return similarity_matrix\n",
        "    \n",
        "    def textrank(self, similarity_matrix):\n",
        "        \"\"\"\n",
        "        Run TextRank algorithm (PageRank on sentence graph)\n",
        "        \n",
        "        Args:\n",
        "            similarity_matrix (np.ndarray): Sentence similarity matrix\n",
        "            \n",
        "        Returns:\n",
        "            np.ndarray: PageRank scores for each sentence\n",
        "        \"\"\"\n",
        "        # Create graph from similarity matrix\n",
        "        nx_graph = nx.from_numpy_array(similarity_matrix)\n",
        "        \n",
        "        # Compute PageRank scores\n",
        "        scores = nx.pagerank(nx_graph, alpha=self.damping)\n",
        "        \n",
        "        return np.array(list(scores.values()))\n",
        "    \n",
        "    def summarize(self, document, num_sentences=None):\n",
        "        \"\"\"\n",
        "        Generate extractive summary using TextRank\n",
        "        \n",
        "        Args:\n",
        "            document (str): Input document\n",
        "            num_sentences (int): Number of sentences to extract (default: self.top_n)\n",
        "            \n",
        "        Returns:\n",
        "            str: Extractive summary\n",
        "        \"\"\"\n",
        "        if num_sentences is None:\n",
        "            num_sentences = self.top_n\n",
        "        \n",
        "        # Split into sentences\n",
        "        sentences = sent_tokenize(document)\n",
        "        \n",
        "        if len(sentences) <= num_sentences:\n",
        "            return document\n",
        "        \n",
        "        # Build similarity matrix\n",
        "        similarity_matrix = self.build_similarity_matrix(sentences)\n",
        "        \n",
        "        # Run TextRank\n",
        "        scores = self.textrank(similarity_matrix)\n",
        "        \n",
        "        # Select top sentences\n",
        "        ranked_indices = np.argsort(scores)[::-1][:num_sentences]\n",
        "        \n",
        "        # Sort by original order to maintain coherence\n",
        "        ranked_indices = sorted(ranked_indices)\n",
        "        \n",
        "        # Extract summary\n",
        "        summary_sentences = [sentences[i] for i in ranked_indices]\n",
        "        summary = ' '.join(summary_sentences)\n",
        "        \n",
        "        return summary\n",
        "\n",
        "print(\"‚úÖ TextRank Summarizer class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Initialize TextRank Summarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize TextRank summarizer\n",
        "print(\"=\"*60)\n",
        "print(\"INITIALIZING TEXTRANK SUMMARIZER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "textrank = TextRankSummarizer(top_n=3, damping=0.85)\n",
        "\n",
        "print(\"\\n‚úÖ TextRank Summarizer initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.4 Test Extractive Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on a few examples\n",
        "print(\"=\"*60)\n",
        "print(\"EXTRACTIVE SUMMARIZATION EXAMPLES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "num_examples = 3\n",
        "\n",
        "for i in range(num_examples):\n",
        "    test_doc = dataset['test'][i]['document']\n",
        "    test_ref = dataset['test'][i]['summary']\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EXAMPLE {i+1}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    print(f\"\\nüìÑ Original Document ({len(test_doc.split())} words):\")\n",
        "    print(test_doc[:300] + \"...\")\n",
        "    \n",
        "    print(f\"\\nü§ñ Extractive Summary (TextRank):\")\n",
        "    extractive_summary = textrank.summarize(test_doc, num_sentences=3)\n",
        "    print(extractive_summary)\n",
        "    \n",
        "    print(f\"\\nüìù Reference Summary:\")\n",
        "    print(test_ref)\n",
        "    \n",
        "    # Calculate statistics\n",
        "    doc_words = len(test_doc.split())\n",
        "    ext_words = len(extractive_summary.split())\n",
        "    compression = (ext_words / doc_words * 100)\n",
        "    \n",
        "    print(f\"\\nüìä Statistics:\")\n",
        "    print(f\"  Original: {doc_words} words\")\n",
        "    print(f\"  Extractive: {ext_words} words\")\n",
        "    print(f\"  Compression: {compression:.1f}%\")\n",
        "\n",
        "print(\"\\n‚úÖ Extractive summarization demo complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 4. Abstractive Summarization\n",
        "\n",
        "## 4.1 Sequence-to-Sequence Models - Theory\n",
        "\n",
        "### Nguy√™n l√Ω / Principle:\n",
        "\n",
        "**Abstractive Summarization** s·ª≠ d·ª•ng m√¥ h√¨nh **Sequence-to-Sequence (Seq2Seq)** ƒë·ªÉ t·∫°o ra b·∫£n t√≥m t·∫Øt m·ªõi thay v√¨ ch·ªçn c√¢u c√≥ s·∫µn.\n",
        "\n",
        "### Ki·∫øn tr√∫c Encoder-Decoder:\n",
        "\n",
        "**Encoder:**\n",
        "- Nh·∫≠n vƒÉn b·∫£n ƒë·∫ßu v√†o\n",
        "- Chuy·ªÉn th√†nh vector bi·ªÉu di·ªÖn (context vector)\n",
        "- Capture to√†n b·ªô ng·ªØ nghƒ©a c·ªßa vƒÉn b·∫£n\n",
        "\n",
        "**Decoder:**\n",
        "- Nh·∫≠n context vector t·ª´ encoder\n",
        "- Sinh ra t·ª´ng t·ª´ m·ªôt c·ªßa b·∫£n t√≥m t·∫Øt\n",
        "- S·ª≠ d·ª•ng th√¥ng tin t·ª´ encoder v√† t·ª´ ƒë√£ sinh tr∆∞·ªõc ƒë√≥\n",
        "\n",
        "### Attention Mechanism:\n",
        "\n",
        "**V·∫•n ƒë·ªÅ / Problem:**\n",
        "- Context vector c·ªë ƒë·ªãnh kh√¥ng th·ªÉ ch·ª©a to√†n b·ªô th√¥ng tin\n",
        "- V·ªõi vƒÉn b·∫£n d√†i, th√¥ng tin b·ªã m·∫•t\n",
        "\n",
        "**Gi·∫£i ph√°p / Solution:**\n",
        "- **Attention** cho ph√©p decoder \"t·∫≠p trung\" v√†o ph·∫ßn kh√°c nhau c·ªßa input\n",
        "- T·∫°i m·ªói b∆∞·ªõc sinh t·ª´, decoder xem l·∫°i to√†n b·ªô encoder outputs\n",
        "- T√≠nh tr·ªçng s·ªë quan tr·ªçng cho t·ª´ng v·ªã tr√≠ trong input\n",
        "\n",
        "### Generation Strategies:\n",
        "\n",
        "#### 1. Greedy Decoding\n",
        "- Ch·ªçn t·ª´ c√≥ x√°c su·∫•t cao nh·∫•t t·∫°i m·ªói b∆∞·ªõc\n",
        "- **∆Øu ƒëi·ªÉm**: Nhanh\n",
        "- **Nh∆∞·ª£c ƒëi·ªÉm**: Kh√¥ng t·ªëi ∆∞u to√†n c·ª•c\n",
        "\n",
        "#### 2. Beam Search\n",
        "- Gi·ªØ K candidates t·ªët nh·∫•t (beam width = K)\n",
        "- **∆Øu ƒëi·ªÉm**: Ch·∫•t l∆∞·ª£ng cao h∆°n greedy\n",
        "- **Nh∆∞·ª£c ƒëi·ªÉm**: Ch·∫≠m h∆°n (K l·∫ßn)\n",
        "\n",
        "#### 3. Top-k Sampling\n",
        "- Ch·ªçn ng·∫´u nhi√™n t·ª´ K t·ª´ c√≥ x√°c su·∫•t cao nh·∫•t\n",
        "- **∆Øu ƒëi·ªÉm**: ƒêa d·∫°ng h∆°n\n",
        "- **Nh∆∞·ª£c ƒëi·ªÉm**: C√≥ th·ªÉ kh√¥ng ·ªïn ƒë·ªãnh\n",
        "\n",
        "#### 4. Top-p (Nucleus) Sampling\n",
        "- Ch·ªçn ng·∫´u nhi√™n t·ª´ c√°c t·ª´ c√≥ t·ªïng x√°c su·∫•t >= p\n",
        "- **∆Øu ƒëi·ªÉm**: C√¢n b·∫±ng gi·ªØa ƒëa d·∫°ng v√† ch·∫•t l∆∞·ª£ng\n",
        "- **Nh∆∞·ª£c ƒëi·ªÉm**: C·∫ßn ƒëi·ªÅu ch·ªânh p c·∫©n th·∫≠n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Load mT5-small Model (Inference Only)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear GPU memory\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"LOADING MT5-SMALL MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load mT5-small model\n",
        "print(\"\\nLoading mT5-small model from HuggingFace...\")\n",
        "mt5_tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
        "mt5_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    \"google/mt5-small\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "mt5_model.to(device)\n",
        "mt5_model.eval()\n",
        "\n",
        "print(f\"‚úì mT5-small loaded on {device}\")\n",
        "print(f\"‚úì Model size: ~300M parameters\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úì GPU memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Load ViT5 Model from HuggingFace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"LOADING VIT5 MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load ViT5 model from HuggingFace\n",
        "print(\"\\nLoading ViT5 model from HuggingFace (YangYang0203/vi5_summarize)...\")\n",
        "vit5_tokenizer = AutoTokenizer.from_pretrained(\"YangYang0203/vi5_summarize\")\n",
        "vit5_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    \"YangYang0203/vi5_summarize\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "vit5_model.to(device)\n",
        "vit5_model.eval()\n",
        "\n",
        "print(f\"‚úì ViT5 loaded on {device}\")\n",
        "print(f\"‚úì Model: Vietnamese-specific T5 (YangYang0203/vi5_summarize)\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úì GPU memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "\n",
        "print(f\"\\n‚úÖ Both models loaded successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4 Inference Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_summary_mt5(text, max_length=128, min_length=30, num_beams=4, strategy=\"beam_search\"):\n",
        "    input_text = f\"t√≥m t·∫Øt: {text}\"\n",
        "    inputs = mt5_tokenizer(\n",
        "        input_text,\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if strategy == \"beam_search\":\n",
        "            outputs = mt5_model.generate(\n",
        "                **inputs,\n",
        "                max_length=max_length,\n",
        "                min_length=min_length,\n",
        "                num_beams=num_beams,\n",
        "                early_stopping=True,\n",
        "                no_repeat_ngram_size=3,\n",
        "                length_penalty=1.0\n",
        "            )\n",
        "        elif strategy == \"sampling\":\n",
        "            outputs = mt5_model.generate(\n",
        "                **inputs,\n",
        "                max_length=max_length,\n",
        "                min_length=min_length,\n",
        "                do_sample=True,\n",
        "                temperature=0.7\n",
        "            )\n",
        "        elif strategy == \"top_k\":\n",
        "            outputs = mt5_model.generate(\n",
        "                **inputs,\n",
        "                max_length=max_length,\n",
        "                min_length=min_length,\n",
        "                do_sample=True,\n",
        "                top_k=50,\n",
        "                temperature=0.8\n",
        "            )\n",
        "        elif strategy == \"top_p\":\n",
        "            outputs = mt5_model.generate(\n",
        "                **inputs,\n",
        "                max_length=max_length,\n",
        "                min_length=min_length,\n",
        "                do_sample=True,\n",
        "                top_p=0.95,\n",
        "                temperature=0.7\n",
        "            )\n",
        "        else:\n",
        "            outputs = mt5_model.generate(**inputs, max_length=max_length, min_length=min_length)\n",
        "\n",
        "    return mt5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def generate_summary_vit5(text, max_length=256, min_length=50, num_beams=4):\n",
        "    input_text = f\"t√≥m t·∫Øt: {text}\"\n",
        "    inputs = vit5_tokenizer(\n",
        "        input_text,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = vit5_model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            min_length=min_length,\n",
        "            num_beams=num_beams,\n",
        "            length_penalty=2.0,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=3\n",
        "        )\n",
        "\n",
        "    return vit5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"‚úÖ Inference functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.5 Test Abstractive Summarization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test both models on examples\n",
        "print(\"=\"*60)\n",
        "print(\"ABSTRACTIVE SUMMARIZATION EXAMPLES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "num_examples = 3\n",
        "\n",
        "for i in range(num_examples):\n",
        "    test_doc = dataset['test'][i]['document']\n",
        "    test_ref = dataset['test'][i]['summary']\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EXAMPLE {i+1}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    print(f\"\\nüìÑ Original Document ({len(test_doc.split())} words):\")\n",
        "    print(test_doc[:300] + \"...\")\n",
        "\n",
        "    print(f\"\\nü§ñ mT5-small Summary:\")\n",
        "    mt5_summary = generate_summary_mt5(test_doc)\n",
        "    print(mt5_summary)\n",
        "\n",
        "    print(f\"\\nü§ñ ViT5 Summary:\")\n",
        "    vit5_summary = generate_summary_vit5(test_doc)\n",
        "    print(vit5_summary)\n",
        "\n",
        "    print(f\"\\nüìù Reference Summary:\")\n",
        "    print(test_ref)\n",
        "\n",
        "    print(f\"\\nüìä Statistics:\")\n",
        "    print(f\"  Original: {len(test_doc.split())} words\")\n",
        "    print(f\"  mT5: {len(mt5_summary.split())} words\")\n",
        "    print(f\"  ViT5: {len(vit5_summary.split())} words\")\n",
        "    print(f\"  Reference: {len(test_ref.split())} words\")\n",
        "\n",
        "print(\"\\n‚úÖ Abstractive summarization demo complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.6 Generation Strategy Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different generation strategies\n",
        "print(\"=\"*60)\n",
        "print(\"GENERATION STRATEGY COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_text = dataset['test'][0]['document']\n",
        "\n",
        "print(f\"\\nTest Document ({len(test_text.split())} words):\")\n",
        "print(test_text[:200] + \"...\\n\")\n",
        "\n",
        "strategies = [\"beam_search\", \"sampling\", \"top_k\", \"top_p\"]\n",
        "\n",
        "print(\"\\nComparing generation strategies with mT5-small:\\n\")\n",
        "\n",
        "for strategy in strategies:\n",
        "    summary = generate_summary_mt5(test_text, strategy=strategy)\n",
        "    print(f\"{'‚îÄ'*60}\")\n",
        "    print(f\"Strategy: {strategy.upper()}\")\n",
        "    print(f\"{'‚îÄ'*60}\")\n",
        "    print(summary)\n",
        "    print(f\"Length: {len(summary.split())} words\\n\")\n",
        "\n",
        "print(\"‚úÖ Strategy comparison complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 5. Evaluation & Comparison\n",
        "\n",
        "## 5.1 ROUGE Metrics Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_rouge_scores(predictions, references):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
        "\n",
        "    scores = {\n",
        "        'rouge1': {'precision': [], 'recall': [], 'fmeasure': []},\n",
        "        'rouge2': {'precision': [], 'recall': [], 'fmeasure': []},\n",
        "        'rougeL': {'precision': [], 'recall': [], 'fmeasure': []}\n",
        "    }\n",
        "\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        result = scorer.score(ref, pred)\n",
        "        for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
        "            scores[metric]['precision'].append(result[metric].precision)\n",
        "            scores[metric]['recall'].append(result[metric].recall)\n",
        "            scores[metric]['fmeasure'].append(result[metric].fmeasure)\n",
        "\n",
        "    return scores\n",
        "\n",
        "print(\"‚úÖ ROUGE computation function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2 Generate Predictions on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"GENERATING PREDICTIONS ON TEST SET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Use subset for faster execution (adjust as needed)\n",
        "sample_size = 500\n",
        "print(f\"\\nUsing {sample_size} samples from test set\")\n",
        "print(\"This will take approximately 10-15 minutes...\")\n",
        "\n",
        "test_docs_sample = dataset['test']['document'][:sample_size]\n",
        "test_refs_sample = dataset['test']['summary'][:sample_size]\n",
        "\n",
        "# Initialize lists\n",
        "mt5_predictions = []\n",
        "vit5_predictions = []\n",
        "extractive_predictions = []\n",
        "\n",
        "# Generate predictions with progress bar\n",
        "print(\"\\nGenerating predictions...\")\n",
        "\n",
        "for i, doc in enumerate(tqdm(test_docs_sample, desc=\"Processing\")):\n",
        "    # mT5 predictions\n",
        "    mt5_pred = generate_summary_mt5(doc)\n",
        "    mt5_predictions.append(mt5_pred)\n",
        "\n",
        "    # ViT5 predictions\n",
        "    vit5_pred = generate_summary_vit5(doc)\n",
        "    vit5_predictions.append(vit5_pred)\n",
        "\n",
        "    # Extractive predictions\n",
        "    extractive_pred = textrank.summarize(doc, num_sentences=3)\n",
        "    extractive_predictions.append(extractive_pred)\n",
        "\n",
        "    if (i + 1) % 50 == 0:\n",
        "        print(f\"  Processed {i + 1}/{sample_size} samples...\")\n",
        "\n",
        "print(f\"\\n‚úÖ All {sample_size} predictions generated!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3 Compute ROUGE Scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"COMPUTING ROUGE SCORES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Compute ROUGE scores for all models\n",
        "mt5_scores = compute_rouge_scores(mt5_predictions, test_refs_sample)\n",
        "vit5_scores = compute_rouge_scores(vit5_predictions, test_refs_sample)\n",
        "extractive_scores = compute_rouge_scores(extractive_predictions, test_refs_sample)\n",
        "\n",
        "# Create models dictionary\n",
        "models = {\n",
        "    'mT5-small': mt5_scores,\n",
        "    'ViT5': vit5_scores,\n",
        "    'TextRank (Extractive)': extractive_scores\n",
        "}\n",
        "\n",
        "# Print results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for model_name, scores in models.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  ROUGE-1 F1: {np.mean(scores['rouge1']['fmeasure']):.4f}\")\n",
        "    print(f\"  ROUGE-2 F1: {np.mean(scores['rouge2']['fmeasure']):.4f}\")\n",
        "    print(f\"  ROUGE-L F1: {np.mean(scores['rougeL']['fmeasure']):.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ ROUGE evaluation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.4 Detailed Comparison Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create detailed comparison table\n",
        "comparison_data = []\n",
        "\n",
        "for model_name, scores in models.items():\n",
        "    comparison_data.append({\n",
        "        'Model': model_name,\n",
        "        'ROUGE-1': f\"{np.mean(scores['rouge1']['fmeasure']):.4f} ¬± {np.std(scores['rouge1']['fmeasure']):.4f}\",\n",
        "        'ROUGE-2': f\"{np.mean(scores['rouge2']['fmeasure']):.4f} ¬± {np.std(scores['rouge2']['fmeasure']):.4f}\",\n",
        "        'ROUGE-L': f\"{np.mean(scores['rougeL']['fmeasure']):.4f} ¬± {np.std(scores['rougeL']['fmeasure']):.4f}\",\n",
        "        'Avg': np.mean([\n",
        "            np.mean(scores['rouge1']['fmeasure']),\n",
        "            np.mean(scores['rouge2']['fmeasure']),\n",
        "            np.mean(scores['rougeL']['fmeasure'])\n",
        "        ])\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.sort_values('Avg', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DETAILED MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"\\n‚úÖ Comparison table created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.5 Side-by-Side Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show side-by-side examples\n",
        "print(\"=\"*60)\n",
        "print(\"SIDE-BY-SIDE COMPARISON EXAMPLES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "num_examples = 5\n",
        "\n",
        "for i in range(num_examples):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"EXAMPLE {i+1}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    print(f\"\\nüìÑ Original Document ({len(test_docs_sample[i].split())} words):\")\n",
        "    print(test_docs_sample[i][:200] + \"...\")\n",
        "\n",
        "    print(f\"\\nü§ñ mT5-small:\")\n",
        "    print(mt5_predictions[i])\n",
        "\n",
        "    print(f\"\\nü§ñ ViT5:\")\n",
        "    print(vit5_predictions[i])\n",
        "\n",
        "    print(f\"\\nü§ñ TextRank (Extractive):\")\n",
        "    print(extractive_predictions[i][:200] if len(extractive_predictions[i]) > 200 else extractive_predictions[i])\n",
        "\n",
        "    print(f\"\\nüìù Reference:\")\n",
        "    print(test_refs_sample[i])\n",
        "\n",
        "print(\"\\n‚úÖ Side-by-side comparison complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 6. Visualizations\n",
        "\n",
        "## 6.1 ROUGE-1 Score Distribution (Chart 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for i, (model_name, scores) in enumerate(models.items()):\n",
        "    r1_scores = scores['rouge1']['fmeasure']\n",
        "    axes[i].hist(r1_scores, bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "    axes[i].axvline(np.mean(r1_scores), color='red', linestyle='--',\n",
        "                   linewidth=2, label=f'Mean: {np.mean(r1_scores):.3f}')\n",
        "    axes[i].set_xlabel('ROUGE-1 F1 Score', fontsize=12)\n",
        "    axes[i].set_ylabel('Frequency', fontsize=12)\n",
        "    axes[i].set_title(f'{model_name} - ROUGE-1 Distribution', fontsize=13, fontweight='bold')\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Chart 1: ROUGE-1 Distribution created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.2 ROUGE-2 Score Distribution (Chart 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for i, (model_name, scores) in enumerate(models.items()):\n",
        "    r2_scores = scores['rouge2']['fmeasure']\n",
        "    axes[i].hist(r2_scores, bins=30, alpha=0.7, color='coral', edgecolor='black')\n",
        "    axes[i].axvline(np.mean(r2_scores), color='red', linestyle='--',\n",
        "                   linewidth=2, label=f'Mean: {np.mean(r2_scores):.3f}')\n",
        "    axes[i].set_xlabel('ROUGE-2 F1 Score', fontsize=12)\n",
        "    axes[i].set_ylabel('Frequency', fontsize=12)\n",
        "    axes[i].set_title(f'{model_name} - ROUGE-2 Distribution', fontsize=13, fontweight='bold')\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Chart 2: ROUGE-2 Distribution created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.3 ROUGE-L Score Distribution (Chart 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for i, (model_name, scores) in enumerate(models.items()):\n",
        "    rL_scores = scores['rougeL']['fmeasure']\n",
        "    axes[i].hist(rL_scores, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "    axes[i].axvline(np.mean(rL_scores), color='red', linestyle='--',\n",
        "                   linewidth=2, label=f'Mean: {np.mean(rL_scores):.3f}')\n",
        "    axes[i].set_xlabel('ROUGE-L F1 Score', fontsize=12)\n",
        "    axes[i].set_ylabel('Frequency', fontsize=12)\n",
        "    axes[i].set_title(f'{model_name} - ROUGE-L Distribution', fontsize=13, fontweight='bold')\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Chart 3: ROUGE-L Distribution created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.4 Box Plots Comparing Models (Chart 4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "metrics = ['rouge1', 'rouge2', 'rougeL']\n",
        "metric_names = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
        "\n",
        "for i, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
        "    data_to_plot = [scores[metric]['fmeasure'] for scores in models.values()]\n",
        "    bp = axes[i].boxplot(data_to_plot, labels=models.keys(), patch_artist=True)\n",
        "\n",
        "    for patch in bp['boxes']:\n",
        "        patch.set_facecolor('lightblue')\n",
        "\n",
        "    axes[i].set_ylabel('F1 Score', fontsize=12)\n",
        "    axes[i].set_title(f'{name} Score Comparison', fontsize=13, fontweight='bold')\n",
        "    axes[i].grid(True, alpha=0.3, axis='y')\n",
        "    axes[i].tick_params(axis='x', rotation=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Chart 4: Box Plots created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.5 Document Length vs ROUGE-L (Chart 5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_lengths = [len(doc.split()) for doc in test_docs_sample]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for i, (model_name, scores) in enumerate(models.items()):\n",
        "    rL_scores = scores['rougeL']['fmeasure']\n",
        "    axes[i].scatter(doc_lengths, rL_scores, alpha=0.4, s=20)\n",
        "\n",
        "    # Add correlation coefficient\n",
        "    corr = np.corrcoef(doc_lengths, rL_scores)[0, 1]\n",
        "    axes[i].text(0.05, 0.95, f'Correlation: {corr:.3f}',\n",
        "                transform=axes[i].transAxes, fontsize=11,\n",
        "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat'))\n",
        "\n",
        "    axes[i].set_xlabel('Document Length (words)', fontsize=12)\n",
        "    axes[i].set_ylabel('ROUGE-L F1 Score', fontsize=12)\n",
        "    axes[i].set_title(f'{model_name}', fontsize=13, fontweight='bold')\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Chart 5: Document Length vs ROUGE-L created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.6 Prediction vs Reference Length (Chart 6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "ref_lengths = [len(ref.split()) for ref in test_refs_sample]\n",
        "pred_lengths_dict = {\n",
        "    'mT5-small': [len(p.split()) for p in mt5_predictions],\n",
        "    'ViT5': [len(p.split()) for p in vit5_predictions],\n",
        "    'TextRank (Extractive)': [len(p.split()) for p in extractive_predictions]\n",
        "}\n",
        "\n",
        "for i, (model_name, pred_lengths) in enumerate(pred_lengths_dict.items()):\n",
        "    axes[i].scatter(ref_lengths, pred_lengths, alpha=0.4, s=20, color='coral')\n",
        "\n",
        "    # Add diagonal line (perfect match)\n",
        "    max_len = max(max(ref_lengths), max(pred_lengths))\n",
        "    axes[i].plot([0, max_len], [0, max_len], 'k--', alpha=0.5, label='Perfect match')\n",
        "\n",
        "    # Add correlation\n",
        "    corr = np.corrcoef(ref_lengths, pred_lengths)[0, 1]\n",
        "    axes[i].text(0.05, 0.95, f'Correlation: {corr:.3f}',\n",
        "                transform=axes[i].transAxes, fontsize=11,\n",
        "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat'))\n",
        "\n",
        "    axes[i].set_xlabel('Reference Length (words)', fontsize=12)\n",
        "    axes[i].set_ylabel('Prediction Length (words)', fontsize=12)\n",
        "    axes[i].set_title(f'{model_name}', fontsize=13, fontweight='bold')\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Chart 6: Prediction vs Reference Length created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.7 Performance by Document Length Category (Chart 7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorize documents by length\n",
        "def categorize_length(length):\n",
        "    if length < 100:\n",
        "        return 'Short (<100)'\n",
        "    elif length < 300:\n",
        "        return 'Medium (100-300)'\n",
        "    else:\n",
        "        return 'Long (>300)'\n",
        "\n",
        "doc_categories = [categorize_length(l) for l in doc_lengths]\n",
        "\n",
        "# Calculate average ROUGE by category\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "categories = ['Short (<100)', 'Medium (100-300)', 'Long (>300)']\n",
        "\n",
        "for i, (model_name, scores) in enumerate(models.items()):\n",
        "    rouge_scores_by_cat = {cat: [] for cat in categories}\n",
        "\n",
        "    for cat, score in zip(doc_categories, scores['rouge1']['fmeasure']):\n",
        "        rouge_scores_by_cat[cat].append(score)\n",
        "\n",
        "    means = [np.mean(rouge_scores_by_cat[cat]) if rouge_scores_by_cat[cat] else 0\n",
        "             for cat in categories]\n",
        "    stds = [np.std(rouge_scores_by_cat[cat]) if rouge_scores_by_cat[cat] else 0\n",
        "            for cat in categories]\n",
        "\n",
        "    x_pos = np.arange(len(categories))\n",
        "    axes[i].bar(x_pos, means, yerr=stds, alpha=0.7, capsize=5,\n",
        "               color=['skyblue', 'lightgreen', 'lightcoral'])\n",
        "    axes[i].set_xticks(x_pos)\n",
        "    axes[i].set_xticklabels(categories, rotation=15)\n",
        "    axes[i].set_ylabel('ROUGE-1 F1 Score', fontsize=12)\n",
        "    axes[i].set_title(f'{model_name}', fontsize=13, fontweight='bold')\n",
        "    axes[i].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Chart 7: Performance by Document Category created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.8 Correlation Heatmap + Summary Statistics (Chart 8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(16, 10))\n",
        "gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# Correlation heatmap (top)\n",
        "ax1 = fig.add_subplot(gs[0, :])\n",
        "\n",
        "correlation_data = []\n",
        "for model_name, scores in models.items():\n",
        "    correlation_data.append([\n",
        "        np.mean(scores['rouge1']['fmeasure']),\n",
        "        np.mean(scores['rouge2']['fmeasure']),\n",
        "        np.mean(scores['rougeL']['fmeasure'])\n",
        "    ])\n",
        "\n",
        "corr_df = pd.DataFrame(\n",
        "    correlation_data,\n",
        "    columns=['ROUGE-1', 'ROUGE-2', 'ROUGE-L'],\n",
        "    index=models.keys()\n",
        ")\n",
        "\n",
        "sns.heatmap(corr_df, annot=True, fmt='.4f', cmap='YlGnBu',\n",
        "           cbar_kws={'label': 'F1 Score'}, ax=ax1, linewidths=1)\n",
        "ax1.set_title('Model Performance Heatmap', fontsize=15, fontweight='bold')\n",
        "\n",
        "# Summary statistics table (bottom left)\n",
        "ax2 = fig.add_subplot(gs[1, 0])\n",
        "ax2.axis('off')\n",
        "\n",
        "summary_stats = []\n",
        "for model_name, scores in models.items():\n",
        "    r1_mean = np.mean(scores['rouge1']['fmeasure'])\n",
        "    r1_std = np.std(scores['rouge1']['fmeasure'])\n",
        "    r2_mean = np.mean(scores['rouge2']['fmeasure'])\n",
        "    r2_std = np.std(scores['rouge2']['fmeasure'])\n",
        "    rL_mean = np.mean(scores['rougeL']['fmeasure'])\n",
        "    rL_std = np.std(scores['rougeL']['fmeasure'])\n",
        "\n",
        "    summary_stats.append([\n",
        "        model_name,\n",
        "        f\"{r1_mean:.4f} ¬± {r1_std:.4f}\",\n",
        "        f\"{r2_mean:.4f} ¬± {r2_std:.4f}\",\n",
        "        f\"{rL_mean:.4f} ¬± {rL_std:.4f}\"\n",
        "    ])\n",
        "\n",
        "table = ax2.table(cellText=summary_stats,\n",
        "                 colLabels=['Model', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L'],\n",
        "                 cellLoc='center', loc='center',\n",
        "                 colWidths=[0.3, 0.23, 0.23, 0.23])\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1, 2)\n",
        "\n",
        "for i in range(4):\n",
        "    table[(0, i)].set_facecolor('#40466e')\n",
        "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "ax2.set_title('Summary Statistics (Mean ¬± Std)', fontsize=13, fontweight='bold', pad=20)\n",
        "\n",
        "# Model ranking (bottom right)\n",
        "ax3 = fig.add_subplot(gs[1, 1])\n",
        "\n",
        "model_names = list(models.keys())\n",
        "avg_rouge = [np.mean([\n",
        "    np.mean(scores['rouge1']['fmeasure']),\n",
        "    np.mean(scores['rouge2']['fmeasure']),\n",
        "    np.mean(scores['rougeL']['fmeasure'])\n",
        "]) for scores in models.values()]\n",
        "\n",
        "sorted_indices = np.argsort(avg_rouge)[::-1]\n",
        "sorted_names = [model_names[i] for i in sorted_indices]\n",
        "sorted_scores = [avg_rouge[i] for i in sorted_indices]\n",
        "\n",
        "colors = ['gold', 'silver', 'chocolate']\n",
        "bars = ax3.barh(sorted_names, sorted_scores, color=colors)\n",
        "ax3.set_xlabel('Average ROUGE Score', fontsize=12)\n",
        "ax3.set_title('Model Ranking', fontsize=13, fontweight='bold')\n",
        "ax3.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "for i, (bar, score) in enumerate(zip(bars, sorted_scores)):\n",
        "    ax3.text(score + 0.005, i, f'{score:.4f}',\n",
        "            va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Chart 8: Correlation Heatmap + Summary Statistics created!\")\n",
        "print(\"\\n‚úÖ All 8 visualizations complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 7. Applications\n",
        "\n",
        "## 7.1 Real-World Use Cases\n",
        "\n",
        "This section demonstrates how Vietnamese text summarization can be applied in various real-world scenarios:\n",
        "\n",
        "### 1. News Summarization\n",
        "- Automatically generate headlines and summaries for news articles\n",
        "- Help readers quickly understand main points\n",
        "- Enable news aggregation services\n",
        "\n",
        "### 2. Document Summarization\n",
        "- Summarize research papers and technical reports\n",
        "- Create executive summaries for business documents\n",
        "- Generate abstracts for academic papers\n",
        "\n",
        "### 3. Meeting Notes\n",
        "- Automatically summarize meeting transcripts\n",
        "- Extract action items and key decisions\n",
        "- Create concise meeting reports\n",
        "\n",
        "### 4. Legal Document Summaries\n",
        "- Summarize contracts and legal agreements\n",
        "- Extract key terms and conditions\n",
        "- Help legal professionals quickly review documents\n",
        "\n",
        "### 5. Medical Record Summarization\n",
        "- Summarize patient medical histories\n",
        "- Extract key symptoms and diagnoses\n",
        "- Create concise clinical summaries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2 Application 1: News Article Summarization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: News Article\n",
        "news_article = dataset['test'][10]['document']\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"APPLICATION 1: NEWS ARTICLE SUMMARIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüì∞ Original News Article ({len(news_article.split())} words):\")\n",
        "print(news_article)\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(\"ü§ñ mT5-small Summary:\")\n",
        "print(generate_summary_mt5(news_article))\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(\"ü§ñ ViT5 Summary:\")\n",
        "print(generate_summary_vit5(news_article))\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(\"ü§ñ TextRank (Extractive) Summary:\")\n",
        "print(textrank.summarize(news_article, num_sentences=3))\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(\"üìù Reference Summary:\")\n",
        "print(dataset['test'][10]['summary'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.3 Application 2: Long Document Summarization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Long Document\n",
        "long_doc = dataset['test'][50]['document']\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"APPLICATION 2: LONG DOCUMENT SUMMARIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìÑ Original Document ({len(long_doc.split())} words):\")\n",
        "print(long_doc[:500] + \"...\")\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(\"ü§ñ mT5-small Summary:\")\n",
        "print(generate_summary_mt5(long_doc, max_length=150))\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(\"ü§ñ ViT5 Summary:\")\n",
        "print(generate_summary_vit5(long_doc, max_length=200))\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(\"üìù Reference Summary:\")\n",
        "print(dataset['test'][50]['summary'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.4 Application 3: Multiple Summary Lengths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate different summary lengths\n",
        "test_doc = dataset['test'][100]['document']\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"APPLICATION 3: MULTIPLE SUMMARY LENGTHS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìÑ Original Document ({len(test_doc.split())} words):\")\n",
        "print(test_doc[:300] + \"...\")\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(\"SHORT Summary (max 50 words):\")\n",
        "print(generate_summary_mt5(test_doc, max_length=50, min_length=20))\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(\"MEDIUM Summary (max 100 words):\")\n",
        "print(generate_summary_mt5(test_doc, max_length=100, min_length=40))\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(\"LONG Summary (max 150 words):\")\n",
        "print(generate_summary_mt5(test_doc, max_length=150, min_length=60))\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*60}\")\n",
        "print(\"üìù Reference Summary:\")\n",
        "print(dataset['test'][100]['summary'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.5 Application 4: Batch Summarization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate batch summarization\n",
        "print(\"=\"*60)\n",
        "print(\"APPLICATION 4: BATCH SUMMARIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "batch_docs = dataset['test']['document'][200:205]\n",
        "\n",
        "print(f\"\\nSummarizing {len(batch_docs)} documents...\\n\")\n",
        "\n",
        "for i, doc in enumerate(batch_docs):\n",
        "    print(f\"{'‚îÄ'*60}\")\n",
        "    print(f\"Document {i+1} ({len(doc.split())} words)\")\n",
        "    print(f\"{'‚îÄ'*60}\")\n",
        "    print(f\"Summary: {generate_summary_mt5(doc)}\")\n",
        "    print()\n",
        "\n",
        "print(\"‚úÖ Batch summarization complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.6 Application 5: Quality Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare quality across different approaches\n",
        "comparison_doc = dataset['test'][150]['document']\n",
        "comparison_ref = dataset['test'][150]['summary']\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"APPLICATION 5: QUALITY COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìÑ Original Document:\")\n",
        "print(comparison_doc[:300] + \"...\\n\")\n",
        "\n",
        "# Generate summaries\n",
        "summaries = {\n",
        "    'mT5-small (beam=4)': generate_summary_mt5(comparison_doc, num_beams=4),\n",
        "    'mT5-small (beam=8)': generate_summary_mt5(comparison_doc, num_beams=8),\n",
        "    'ViT5': generate_summary_vit5(comparison_doc),\n",
        "    'TextRank': textrank.summarize(comparison_doc, num_sentences=3),\n",
        "    'Reference': comparison_ref\n",
        "}\n",
        "\n",
        "# Compute ROUGE for each\n",
        "print(\"ROUGE Scores:\\n\")\n",
        "for name, summary in summaries.items():\n",
        "    if name != 'Reference':\n",
        "        score = compute_rouge_scores([summary], [comparison_ref])\n",
        "        r1 = np.mean(score['rouge1']['fmeasure'])\n",
        "        r2 = np.mean(score['rouge2']['fmeasure'])\n",
        "        rL = np.mean(score['rougeL']['fmeasure'])\n",
        "        print(f\"{name}:\")\n",
        "        print(f\"  ROUGE-1: {r1:.4f}, ROUGE-2: {r2:.4f}, ROUGE-L: {rL:.4f}\")\n",
        "        print(f\"  Summary: {summary}\")\n",
        "        print()\n",
        "\n",
        "print(\"\\n‚úÖ Quality comparison complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.7 Conclusion\n",
        "\n",
        "### Summary of Findings:\n",
        "\n",
        "1. **Best Overall Performance**: The abstractive models (mT5-small and ViT5) generally outperform the extractive approach in ROUGE scores\n",
        "\n",
        "2. **Model Comparison**:\n",
        "   - **ViT5**: Best for Vietnamese-specific content, more natural summaries\n",
        "   - **mT5-small**: Good multilingual performance, fast inference\n",
        "   - **TextRank**: Fast, reliable, but less fluent summaries\n",
        "\n",
        "3. **Use Case Recommendations**:\n",
        "   - **News**: Use ViT5 or mT5 for natural, concise summaries\n",
        "   - **Technical Documents**: TextRank for factual accuracy\n",
        "   - **Long Documents**: mT5/ViT5 with adjusted length parameters\n",
        "   - **Real-time Applications**: TextRank for speed\n",
        "\n",
        "4. **Key Insights**:\n",
        "   - Beam search (4-8 beams) produces best quality\n",
        "   - Document length impacts performance\n",
        "   - Vietnamese-specific models (ViT5) better capture language nuances\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Fine-tune mT5/ViT5 on your specific domain data\n",
        "- Experiment with different generation parameters\n",
        "- Combine extractive and abstractive approaches\n",
        "- Deploy models with appropriate hardware for production\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Notebook Complete!\n",
        "\n",
        "This comprehensive notebook covered:\n",
        "1. ‚úÖ Theory of text summarization\n",
        "2. ‚úÖ Data loading and exploration\n",
        "3. ‚úÖ Extractive summarization (TextRank)\n",
        "4. ‚úÖ Abstractive summarization (mT5 + ViT5)\n",
        "5. ‚úÖ ROUGE evaluation and comparison\n",
        "6. ‚úÖ 8 comprehensive visualizations\n",
        "7. ‚úÖ Real-world applications\n",
        "\n",
        "Thank you for using this notebook! üéâ\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}